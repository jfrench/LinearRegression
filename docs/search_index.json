[["index.html", "Joshua French Preliminaries", " Joshua French Joshua French 2022-06-13 Preliminaries I recommend you execute the following commands install packages we may use in this course. # packages related to books books = c(&quot;faraway&quot;, &quot;alr4&quot;, &quot;car&quot;, &quot;rms&quot;) install.packages(books) # packages related to tidy/tidying data tidy = c(&quot;broom&quot;, &quot;tidyr&quot;, &quot;dplyr&quot;) install.packages(tidy) # packages related to plotting moreplots = c(&quot;ggplot2&quot;, &quot;ggthemes&quot;, &quot;lattice&quot;, &quot;HH&quot;) install.packages(moreplots) # packages related to model diagnostics diag = c(&quot;leaps&quot;, &quot;lmtest&quot;, &quot;gvlma&quot;, &quot;caret&quot;) install.packages(diag) # packages related to workflow workflow = c(&quot;remotes&quot;) install.packages(workflow) Lastly, we need to install the perturb package, which is currently not available through the install.packages function. To install this from the package developers GitHub repository, we run the command below in the Console. remotes::install_github(repo = &quot;JohnHendrickx/Perturb&quot;) Acknowledgments The bookdown package (Xie 2022) was used to generate this book. References "],["r-foundations.html", "Chapter 1 R Foundations 1.1 What is R? 1.2 Where to get R (and R Studio Desktop) 1.3 R Studio Layout 1.4 Running code, scripts, and comments 1.5 Packages 1.6 Getting help 1.7 Data types and structures 1.8 Assignment 1.9 Vectors 1.10 Helpful functions 1.11 Data Frames 1.12 Logical statements 1.13 Subsetting with logical statements 1.14 Ecosystem debate", " Chapter 1 R Foundations Meaningful data analysis requires the use of computer software. R statistical software is one of the most popular tools for data analysis both in academia and the broader workforce. In what follows, I will attempt to lay a foundation of basic knowledge and skills with R that you will need for data analysis. I make no attempt to be exhaustive, and many other important aspects of using R (like plotting) will be discussed later, as needed. 1.1 What is R? R is programming language and environment designed for statistical computing. It was introduced by Robert Gentleman and Robert Ihaka in 1993. It is modeled after the S programming language. R is free, open source, and runs on Windows, Macs, Linux, and other types of computers. R is an interactive programming language You type and execute a command in the Console for immediate feedback in contrast to a compiled programming language, which compiles a program that is then executed. R is highly extendable. Many user-created packages are available to extend the functionality beyond what is installed by default. Users can write their own functions and easily add software libraries to R. 1.2 Where to get R (and R Studio Desktop) R may be downloaded from the R Projects website. This link should bring you to the relevant page for downloading the software. R Studio Desktop is a free front end for R provided by R Studio. R Studio Desktop makes doing data analysis with R much easier by adding an Integrated Development Environment (IDE) and providing many other features. Currently, you may download R Studio at this link. You may need to navigate the R Studio website directly if this link no longer functions. Install R and R Studio Desktop before continuing. Then open R Studio Desktop as you continue to learn about R. 1.3 R Studio Layout R Studio Desktop has four panes: Console: the pane where the code is executed. Source: the pane where you prepare commands to be executed. Environment/History: the pane where you can see all the objects in your workspace, your command history, and other things. The Files/Plot/Packages/Help: the pane where you navigate between directories, where plots can be viewed, where you can see the packages available to be loaded, and where you can get help. To see all R Studio panes, press the keys Shift + Ctrl + Alt + 0 1.4 Running code, scripts, and comments Code is executed in R by typing it in the Console and hitting enter. Instead of typing all of your code in the Console and hitting enter, its better to write your code in a Script and execute the code separately. A new script can be obtained by executing File -&gt; New File -&gt; R Script or pressing Ctrl + Shift + n (on a PC) or Cmd + Shift + n on a Mac. There are various ways to run code from a Script file. The most common ones are: Highlight the code you want to run and hit the Run button at the top of the Script pane. Highlight the code you want to run and press Ctrl + Enter on your keyboard. If you dont highlight anything, by default, R Studio runs the command the cursor currently lies on. To save a script, click File -&gt; Save or press Ctrl + s (on a PC) or Cmd + s (on a Mac). A comment is a set of text ignored by R when submitted to the Console. A comment is indicated by the # symbol. Nothing to the right of the # is executed in the Console. To comment (or uncomment) multiple lines in R, highlight the code you want to comment and press Ctrl + Shift + c on a PC or Cmd + Shift + c on a Mac. 1.4.1 Example Perform the following tasks: Type 1+1 in the Console and hit enter. Open a new Script in R Studio. mean(1:3) in your Script file. Type # mean(1:3) in your Script file. Run the commands from the Script using an approach mentioned above. 1.5 Packages Packages are collections of functions, data, and other objects that extend the functionality installed by default in R. R packages can be installed using the install.packages function and loaded using the library function. 1.5.1 Example Practice installing and loading a package by doing the following: Install the set of faraway package by executing the command install.packages(\"faraway\"). Load the faraway package by executing the command library(faraway). 1.6 Getting help There are a number of helps to get help in R. If you know the command for which you want help, then exectue ?command in the Console. * e.g., ?lm * This also may work with data sets, package names, object classes, etc. If you want to search the documentation for a certain topic, then execute ??topic in the Console. * If you need help deciphering an error, identifying packages to perform a certain analysis, how to do something better, then a web search is likely to help. 1.6.1 Example Do the following: Execute ?lm in the Console to get help on the lm function, which is one of the main functions used for fitting linear models. Execute ??logarithms in the Console to search the R documentation for information about logarithms. Do a web search for something along the lines of How do I change the size of the axis labels in an R plot? 1.7 Data types and structures 1.7.1 Basic data types R has 6 basic (atomic) vector types: character - collections of characters. E.g., \"a\", \"hello world!\" double - decimal numbers. e.g., 1.2, 1.0 integer - whole numbers. In R, you must add L to the end of a number to specify it as an integer. E.g., 1L is an integer but 1 is a double. logical - boolean values, TRUE and FALSE complex - complex numbers. E.g., 1+3i raw - a type to hold raw bytes. The typeof function returns the R internal type or storage mode of any object. Consider the following commands and output: # determine basic data type typeof(1) #&gt; [1] &quot;double&quot; typeof(1L) #&gt; [1] &quot;integer&quot; typeof(&quot;hello world!&quot;) #&gt; [1] &quot;character&quot; 1.7.2 Other important object types There are other important types of objects in R that are not basic. We will discuss a few. The R Project manual provides additional information about available types. 1.7.2.1 Numeric An object is numeric if it is of type integer or double. In that case, its mode is said to be numeric. The is.numeric function tests whether an object can be interpreted as numbers. We can use it to determine whether an object is numeric. Some examples: # is the object numeric? is.numeric(&quot;hello world!&quot;) #&gt; [1] FALSE is.numeric(1) #&gt; [1] TRUE is.numeric(1L) #&gt; [1] TRUE 1.7.2.2 NULL NULL is a special object to indicate an object is absent. An object having a length of zero is not the same thing as an object being absent. 1.7.2.3 NA A missing value occurs when the value of something isnt known. R uses the special object NA to represent missing value. If you have a missing value, you should represent that value as NA. Note: \"NA\" is not the same thing as NA. 1.7.2.4 Functions A function is an object the performs a certain action or set of actions based on objects it receives from its arguments. 1.7.3 Data structures R operates on data structures. A data structure is simply some sort of container that holds certain kinds of information R has 5 basic data structures: vector matrix array data frame list Vectors, matrices, and arrays are homogeneous objects that can only store a single data type at a time. Data frames and lists can store multiple data types. Vectors and lists are considered one-dimensional objects. A list is technically a vector. Vectors of a single type are atomic vectors. (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#List-objects) Matrices and data frames are considered two-dimensional objects. Arrays can be n-dimensional objects. This is summarized in the table below, which is based on a table in the first edition of Hadley Wickhams Advanced R. dimensionality homogeneous heterogeneous 1d vector list 2d matrix data frame nd array 1.8 Assignment To store a data structure in the computers memory we must assign it a name. Data structures can be stored using the assignment operator &lt;- or =. Some comments: In general, both &lt;- and = can be used for assignment. Pressing the Alt and - keys simultaneously on a PC or Linux machine (Option and - on a Mac) will insert &lt;- into the R console and script files. If you are creating an R Markdown file, then this command will only insert &lt;- if you are in an R computing environment. &lt;- and = are NOT synonyms, but can be used identically most of the time. Its safest to use &lt;- for assignment. Once an object has been assigned a name, it can be printed by executing the name of the object or using the print function. 1.8.1 Example In the following code, we compute the mean of a vector and print the result. # compute the mean of 1, 2, ..., 10 and assign the name m m &lt;- mean(1:10) m # print m #&gt; [1] 5.5 print(m) # print m a different way #&gt; [1] 5.5 1.9 Vectors A vector is a single-dimensional set of data of the same type. 1.9.1 Creation The most basic way to create a vector is the c function. The c function combines values into a vector or list. e.g., the following commands create vectors of type numeric, character, and logical, respectively. c(1, 2, 5.3, 6, -2, 4) c(\"one\", \"two\", \"three\") c(TRUE, TRUE, FALSE, TRUE) 1.9.2 Creating patterned vectors R provides a number of functions for creating vectors following certain consistent patterns. The seq (sequence) function is used to create an equidistant series of numeric values. Some examples: seq(1, 10): A sequence of numbers from 1 to 10 in increments of 1. 1:10: A sequence of numbers from 1 to 10 in increments of 1. seq(1, 20, by = 2): A sequence of numbers from 1 to 20 in increments of 2. seq(10, 20, len = 100): A sequence of numbers from 10 to 20 of length 100. The rep (replicate) function can be used to create a vector by replicating values. Some examples: rep(1:3, times = 3): Repeat the sequence 1, 2, 3 three times in a row. rep(c(\"trt1\", \"trt2\", \"trt3\"), times = 1:3): Repeat trt1 once, trt2 twice, and trt3 three times. rep(1:3, each = 3): Repeat each element of the sequence 1, 2, 3 three times. 1.9.3 Example Execute the following commands in the Console to see what you get. # vector creation c(1, 2, 5.3, 6, -2, 4) c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;) c(TRUE, TRUE, FALSE, TRUE) # sequences of values seq(1, 10) 1:10 seq(1, 20, by = 2) seq(10, 20, len = 100) # replicated values rep(1:3, times = 3) rep(c(&quot;trt1&quot;, &quot;trt2&quot;, &quot;trt3&quot;), times = 1:3) rep(1:3, each = 3) Vectors can be combined into a new object using the c function. 1.9.4 Example Execute the following commands in the Console v1 &lt;- 1:5 # create a vector v1 # print the vector #&gt; [1] 1 2 3 4 5 print(v1) #&gt; [1] 1 2 3 4 5 v2 &lt;- c(1, 10, 11) # create a new vector new &lt;- c(v1, v2) # combine and assign the combined vectors new # print the combined vector #&gt; [1] 1 2 3 4 5 1 10 11 1.9.5 Categorical vectors Categorical data should be stored as a factor in R. The factor function takes values that can be coerced to a character and converts them to an object of class factor. Some examples: # create some factor variables f1 &lt;- factor(rep(1:6, times = 3)) f1 #&gt; [1] 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 #&gt; Levels: 1 2 3 4 5 6 f2 &lt;- factor(c(&quot;a&quot;, 7, &quot;blue&quot;, &quot;blue&quot;, FALSE)) f2 #&gt; [1] a 7 blue blue FALSE #&gt; Levels: 7 a blue FALSE 1.9.6 Example Create a vector named grp that has two levels: a and b, where the first 7 values are a and the second 4 values are b. 1.9.7 Extracting parts of a vector Subsets of the elements of a vector can be extracted by appending an index vector in square brackets [] to the name of the vector . Lets create the numeric vector 2, 4, 6, 8, 10, 12, 14, 16. # define a sequence 2, 4, ..., 16 a &lt;- seq(2, 16, by = 2) a #&gt; [1] 2 4 6 8 10 12 14 16 Lets access the 2nd, 4th, and 6th elements of a. # extract subset of vector a[c(2, 4, 6)] #&gt; [1] 4 8 12 Lets access all elements in a EXCEPT the 2nd, 4th, and 6th using the minus (-) sign in front of the index vector. # extract subset of vector using minus a[-c(2, 4, 6)] #&gt; [1] 2 6 10 14 16 Lets access all elements in a except elements 3 through 6. a[-(3:6)] #&gt; [1] 2 4 14 16 1.10 Helpful functions 1.10.1 General functions Some general functions commonly used to describe data objects: length(x): length of x sum(x): sum elements in x mean(x): sample mean of elements in x var(x): sample variance of elements in x sd(x): sample standard deviation of elements in x range(x): range (minimum and maximum) of elements in x log(x): (natural) logarithm of elements in x summary(x): a summary of x. Output changes depending on the class of x. str(x): provides information about the structure of x. Usually, the class of the object and some information about its size. 1.10.2 Example Run the following commands in the Console: # common functions x &lt;- rexp(100) # sample 100 iid values from an Exponential(1) distribution length(x) # length of x sum(x) # sum of x mean(x) # sample mean of x var(x) # sample variance of x sd(x) # sample standard deviation of x range(x) # range of x log(x) # logarithm of x summary(x) # summary of x str(x) # structure of x 1.10.3 Functions related to statistical distributions Suppose that a random variable \\(X\\) has the dist distribution: p[dist](q, ...): returns the cdf of \\(X\\) evaluated at q, i.e., \\(p=P(X\\leq q)\\). q[dist](p, ...): returns the inverse cdf (or quantile function) of \\(X\\) evaluated at \\(p\\), i.e., \\(q = \\inf\\{x: P(X\\leq x) \\geq p\\}\\). d[dist](x, ...): returns the mass or density of \\(X\\) evaluated at \\(x\\) (depending on whether its discrete or continuous). r[dist](n, ...): returns an i.i.d. random sample of size n having the same distribution as \\(X\\). The ... indicates that additional arguments describing the parameters of the distribution may be required. Execute ?Distributions to get information about the distributions available in R by default. 1.10.4 Example Execute the following commands in R to see the output. What is each command doing? # statistical calculations pnorm(1.96, mean = 0, sd = 1) qunif(0.6, min = 0, max = 1) dbinom(2, size = 20, prob = .2) dexp(1, rate = 2) rchisq(100, df = 5) pnorm(1.96, mean = 0, sd = 1) returns the probability that a standard normal random variable is less than or equal to 1.96, i.e., \\(P(X \\leq 1.96)\\). qunif(0.6, min = 0, max = 1) returns the value \\(x\\) such that \\(P(X\\leq x) = 0.6\\) for a uniform random variable on the interval \\([0, 1]\\). dbinom(2, size = 20, prob = .2) returns the probability that \\(P(X=2)\\) for \\(X~\\textrm{Binom}(n=20,\\pi=0.2)\\). dexp(1, rate = 2) evaluates the density of an exponential random variable with mean = 1/2 at \\(x=1\\). rchisq(100, df = 5) returns a sample of 100 observations from a chi-squared random variable with 5 degrees of freedom. 1.11 Data Frames Data frames are two-dimensional data objects. Each column of a data frame is a vector (or variable) of possibly different data types. This is a fundamental data structure used by most of Rs modeling software. In general, I recommend tidy data, which means that each variable forms a column of the data frame, and each observation forms a row. 1.11.1 Creation Data frames are created by passing vectors into the data.frame function. The names of the columns in the data frame are the names of the vectors you give the data.frame function. Consider the following simple example. # create basic data frame d &lt;- c(1, 2, 3, 4) e &lt;- c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;, NA) f &lt;- c(TRUE, TRUE, TRUE, FALSE) df &lt;- data.frame(d,e,f) df #&gt; d e f #&gt; 1 1 red TRUE #&gt; 2 2 white TRUE #&gt; 3 3 blue TRUE #&gt; 4 4 &lt;NA&gt; FALSE The columns of a data frame can be renamed using the names function on the data frame. # name columns of data frame names(df) &lt;- c(&quot;ID&quot;, &quot;Color&quot;, &quot;Passed&quot;) df #&gt; ID Color Passed #&gt; 1 1 red TRUE #&gt; 2 2 white TRUE #&gt; 3 3 blue TRUE #&gt; 4 4 &lt;NA&gt; FALSE The columns of a data frame can be named when you are first creating the data frame by using name = for each vector of data. # create data frame with better column names df2 &lt;- data.frame(ID = d, Color = e, Passed = f) df2 #&gt; ID Color Passed #&gt; 1 1 red TRUE #&gt; 2 2 white TRUE #&gt; 3 3 blue TRUE #&gt; 4 4 &lt;NA&gt; FALSE 1.11.2 Extracting parts of a data frame The column vectors of a data frame may be extracted using $ and specifying the name of the desired vector. df$Color would access the Color column of data frame df. Part of a data frame can also be extracted by thinking of at as a general matrix and specifying the desired rows or columns in square brackets after the object name. For example, if we had a data frame named df: df[1,] would access the first row of df. df[1:2,] would access the first two rows of df. df[,2] would access the second column of df. df[1:2, 2:3] would access the information in rows 1 and 2 of columns 2 and 3 of df. If you need to select multiple columns of a data frame by name, you can pass a character vector with column names in the column position of []. df[, c(\"Color\", \"Passed\")] would extract the Color and Passed columns of df. 1.11.3 Example Execute the following commands in the Console: # Extract parts of a data frame df3 &lt;- data.frame(numbers = 1:5, characters = letters[1:5], logicals = c(TRUE, TRUE, FALSE, TRUE, FALSE)) df3 # print df df3$logicals # access the logicals vector of df3 df3[1, ] # access the first column of df3 df3[, 3] # access the third column of df3 df3[, 2:3] # access the column 2 and 3 of df3 df3[, c(&quot;numbers&quot;, &quot;logicals&quot;)] # access the numbers and logical columns of df3 Students often can work more conveniently with vectors, so it is sometimes desirable to access a part of a data frame and assign it a new name for later use. For example, to access the ID column of df2 and assign it the name newID, we could execute newID &lt;- df2$ID. 1.11.4 Importing Data The read.table function imports data from file into R as a data frame. Usage: read.table(file, header = TRUE, sep = \",\") file is the file path and name of the file you want to import into R. If you dont know the file path, set file = file.choose() will bring up a dialog box asking you to locate the file you want to import. header specifies whether the data file has a header (variable labels for each column of data in the first row of the data file). If you dont specify this option in R or use header = FALSE, then R will assume the file doesnt have any headings. header = TRUE tells R to read in the data as a data frame with column names taken from the first row of the data file. sep specifies the delimiter separating elements in the file. If each column of data in the file is separated by a space, then use sep = \" \" If each column of data in the file is separated by a comma, then use sep = \",\" If each column of data in the file is separated by a tab, then use sep = \"\\t\". Here is an example reading a csv (comma separated file) with a header: # import data as data frame dtf &lt;- read.table(file = &quot;https://raw.githubusercontent.com/jfrench/DataWrangleViz/master/data/covid_dec4.csv&quot;, header = TRUE, sep = &quot;,&quot;) str(dtf) #&gt; &#39;data.frame&#39;: 50 obs. of 7 variables: #&gt; $ state_name: chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... #&gt; $ state_abb : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... #&gt; $ deaths : int 3831 142 6885 2586 19582 2724 5146 782 19236 9725 ... #&gt; $ population: num 387000 96500 498000 238000 2815000 ... #&gt; $ income : int 25734 35455 29348 25359 31086 35053 37299 32928 27107 28838 ... #&gt; $ hs : num 82.1 91 85.6 82.9 80.7 89.7 88.6 87.7 85.5 84.3 ... #&gt; $ bs : num 21.9 27.9 25.9 19.5 30.1 36.4 35.5 27.8 25.8 27.3 ... Note that the read_table function in the readr package and the fread function in the data.table package are perhaps better ways of reading in tabular data and use similar syntax. 1.12 Logical statements 1.12.1 Basic comparisons Sometimes we need to know if the elements of an object satisfy certain conditions. This can be determined using the logical operators &lt;, &lt;=, &gt;, &gt;=, ==, !=. == means equal to. != means NOT equal to. 1.12.2 Example Execute the following commands in R and see what you get. What is each statement performing? # logical statements # a &lt;- seq(2, 16, by = 2) # creating the vector a a a &gt; 10 a &lt;= 4 a == 10 a != 10 1.12.3 And and Or statements More complicated logical statements can be made using &amp; and |. &amp; means and Only TRUE &amp; TRUE returns TRUE. Otherwise the &amp; operator returns FALSE. | means or Only a single value in an | statement needs to be true for TRUE to be returned. Note that: TRUE &amp; TRUE returns TRUE FALSE &amp; TRUE returns FALSE FALSE &amp; FALSE returns FALSE TRUE | TRUE returns TRUE FALSE | TRUE returns TRUE FALSE | FALSE returns FALSE # relationship between logicals &amp; (and), | (or) TRUE &amp; TRUE #&gt; [1] TRUE FALSE &amp; TRUE #&gt; [1] FALSE FALSE &amp; FALSE #&gt; [1] FALSE TRUE | TRUE #&gt; [1] TRUE FALSE | TRUE #&gt; [1] TRUE FALSE | FALSE #&gt; [1] FALSE 1.12.4 Example Execute the following commands in R and see what you get. # complex logical statements (a &gt; 6) &amp; (a &lt;= 10) (a &lt;= 4) | (a &gt;= 12) 1.13 Subsetting with logical statements Logical statements can be used to return parts of an object satisfying the appropriate criteria. Specifically, we pass logical statements within the square brackets used to access part of a data structure. 1.13.1 Example Execute the following code: # accessing parts of a vector using logicals a a &lt; 6 a[a &lt; 6] a == 10 a[a == 10] (a &lt; 6) | ( a == 10) a[(a &lt; 6) | ( a == 10)] # accessing parts of a data frame # create a logical vector based on whether # a state_abb in dtf is &quot;CA&quot; or &quot;CO&quot; ca_or_co &lt;- is.element(dtf$state_abb, c(&quot;CA&quot;, &quot;CO&quot;)) ca_or_co # access the CA and CA rows of dtf dtf[ca_or_co,] 1.14 Ecosystem debate We will typically work with base R, which are commands and functions R offers by default. The tidyverse (https://www.tidyverse.org) is a collection of R packages that provides a unified framework for data manipulation and visualization. Since this course focuses more on modeling than data manipulation, I will typically focus on approaches in base R. I will use functions from the tidyverse when it greatly simplifies analysis, data manipulation, or visualization. "],["data-exploration.html", "Chapter 2 Data exploration 2.1 Data analysis process 2.2 Data exploration 2.3 Kidney Example 2.4 Visualizing data with base graphics 2.5 Visualizing data with ggplot2 2.6 Summary of data exploration", " Chapter 2 Data exploration Based on Chapter 1 of LMWR2, Chapter 1 of ALR4 2.1 Data analysis process Define a statistical question of interest. Collect relevant data. Analyze the data. Interpret your analysis. Make a decision. The formulation of a problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill - Albert Einstein 2.1.1 Problem Formulation Understand the physical background. Statisticians often work in collaboration with others and need to understand something about the subject area. Understand the objective. What are your goals? Make sure you know what the client wants. Put the problem into statistical terms. This is often the most challenging step and where irreparable errors are sometimes made. That a statistical method can read in and process the data is not enough. The results of an inept analysis may be meaningless. 2.1.2 Data collection Data collection: How the data were collected has a crucial impact on what conclusions can be made. Are the data observational or experimental? Are the data a sample of convenience or were they obtained via a designed sample survey? Is there nonresponse bias? The data you do not see may be just as important as the data you do see. Are there missing values? This is a common problem that is troublesome and time consuming to handle. How are the data coded? How are the qualitative variables represented? What are the units of measurement? Beware of data entry errors and other corruption of the data. Perform some data sanity checks. 2.2 Data exploration An initial exploration of the data should be performed prior to any formal analysis or modeling. Initial data analysis should consist of numerical summaries and appropriate plots. 2.2.1 Numerical summaries of data Statistics can be used to numerically summarize aspects of the data: mean standard deviation (SD) maximum and minimum correlation other measures, as appropriate 2.2.2 Visual summaries of data Plots can provide a useful visual summary of the data. For one numerical variable: boxplots, histograms, density plots, etc. For one categorial variable: bar charts. For two numerical variables: scatter plots. For one numerical and one categorical variables: parallel boxplots or density plots that distinguish between category level. For two categorical variables: panels of bar charts. For three or more variables: one or two variable plots with distinguishing colors or line types, interactive and dynamic graphics. Good graphics are essential in data analysis. They help us to understand our data structure so that we can avoid mistakes. They help us decide on a model. They help communicate the results of our analysis. Graphics can be more convincing than text at times. 2.2.3 What to look for When summarizing the data, look for: outliers data-entry errors skewness unusual distributions patterns or structure 2.3 Kidney Example The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Native Americans living near Phoenix. The following variables were recorded: pregnant - number of times pregnant glucose - plasma glucose concentration at 2 hours in an oral glucose tolerance test diastolic - diastolic blood pressure (mm Hg) triceps - triceps skin fold thickness (mm) insulin - 2-hour serum insulin (mu U/ml) bmi - body mass index (weight in kg/(height in m2)) diabetes - diabetes pedigree function age - age (years) test - test whether the patient showed signs of diabetes (coded zero if negative, one if positive). The data may be obtained from the UCI Repository of machine learning databases at https://archive.ics.uci.edu/ml. Lets load and examine the structure of the data data(pima, package = &quot;faraway&quot;) str(pima) # structure #&gt; &#39;data.frame&#39;: 768 obs. of 9 variables: #&gt; $ pregnant : int 6 1 8 1 0 5 3 10 2 8 ... #&gt; $ glucose : int 148 85 183 89 137 116 78 115 197 125 ... #&gt; $ diastolic: int 72 66 64 66 40 74 50 0 70 96 ... #&gt; $ triceps : int 35 29 0 23 35 0 32 0 45 0 ... #&gt; $ insulin : int 0 0 0 94 168 0 88 0 543 0 ... #&gt; $ bmi : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... #&gt; $ diabetes : num 0.627 0.351 0.672 0.167 2.288 ... #&gt; $ age : int 50 31 32 21 33 30 26 29 53 54 ... #&gt; $ test : int 1 0 1 0 1 0 1 0 1 1 ... head(pima) # first six rows #&gt; pregnant glucose diastolic triceps insulin bmi diabetes age test #&gt; 1 6 148 72 35 0 33.6 0.627 50 1 #&gt; 2 1 85 66 29 0 26.6 0.351 31 0 #&gt; 3 8 183 64 0 0 23.3 0.672 32 1 #&gt; 4 1 89 66 23 94 28.1 0.167 21 0 #&gt; 5 0 137 40 35 168 43.1 2.288 33 1 #&gt; 6 5 116 74 0 0 25.6 0.201 30 0 tail(pima) # last six rows #&gt; pregnant glucose diastolic triceps insulin bmi diabetes age test #&gt; 763 9 89 62 0 0 22.5 0.142 33 0 #&gt; 764 10 101 76 48 180 32.9 0.171 63 0 #&gt; 765 2 122 70 27 0 36.8 0.340 27 0 #&gt; 766 5 121 72 23 112 26.2 0.245 30 0 #&gt; 767 1 126 60 0 0 30.1 0.349 47 1 #&gt; 768 1 93 70 31 0 30.4 0.315 23 0 2.3.1 Numerically summarizing the data The summary command is a useful way to numerically summarize a data frame. The summary function will compute the minimum, 0.25 quantile, mean, median, 0.75 quantile, and maximum of a numeric variable. The summary function will count the number of values having each level for a factor variable. Lets summarize the pima data frame. summary(pima) #&gt; pregnant glucose diastolic triceps #&gt; Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 #&gt; 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 #&gt; Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 #&gt; Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 #&gt; 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 #&gt; Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 #&gt; insulin bmi diabetes age #&gt; Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 #&gt; 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 #&gt; Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 #&gt; Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 #&gt; 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 #&gt; Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 #&gt; test #&gt; Min. :0.000 #&gt; 1st Qu.:0.000 #&gt; Median :0.000 #&gt; Mean :0.349 #&gt; 3rd Qu.:1.000 #&gt; Max. :1.000 2.3.2 Cleaning the data Cleaning data involves finding and correcting data quality issues. The pima data set has some odd characterics: The minimum diastolic blood pressure is zero. Thats generally an indication of a health problem. The test variable appears to be numeric but should be a factor (categorical) variable. Many other variables have unusual zeros. Look for anything unusual or unexpected, perhaps indicating a data-entry error. Lets look at the first 40 sorted diastolic values. sort(pima$diastolic)[1:40] #&gt; [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [26] 0 0 0 0 0 0 0 0 0 0 24 30 30 38 40 The first 35 values of diastolic are zero. Thats a problem. It seems that 0 was used in place of a missing value. This is very bad since 0 is a real number and this problem may be overlooked, which can lead to faulty analysis! This is why we must check our data carefully for things that dont make sense. The value for missing data in R is NA. Several variables share this problem. Lets set the 0s that should be missing values to NA. pima$diastolic[pima$diastolic == 0] &lt;- NA pima$glucose[pima$glucose == 0] &lt;- NA pima$triceps[pima$triceps == 0] &lt;- NA pima$insulin[pima$insulin == 0] &lt;- NA pima$bmi[pima$bmi == 0] &lt;- NA The test variable is a categorical variable, not numerical. R thinks the test variable is numeric. In R, a categorical variable is a factor. We need to convert the test variable to a factor. Lets convert test to a factor. pima$test &lt;- factor(pima$test) summary(pima$test) #&gt; 0 1 #&gt; 500 268 500 of the cases were negative and 268 were positive. We can provide more descriptive labels using the levels function. We change the 0 and 1 levels to negative and positive to make the data more descriptive. A summary of the updates test variable shows why this is useful. levels(pima$test) &lt;- c(&quot;negative&quot;, &quot;positive&quot;) summary(pima$test) #&gt; negative positive #&gt; 500 268 2.4 Visualizing data with base graphics 2.4.1 Histograms The hist function can be used create a histogram of a numerical vector. The labels of the plot can be customized using the xlab and ylab arguments. The main title of the plot can be customized using the main argument. Here is a slightly customized histogram of diastolic blood pressure. hist(pima$diastolic, xlab = &quot;diastolic blood pressure&quot;, main = &quot;&quot;) The histogram is approximately bell-shaped and centered around 70. We can change the number of breaks in the histogram by specifying the breaks argument of the hist function. Consider how the plot changes below. hist(pima$diastolic, xlab = &quot;diastolic blood pressure&quot;, main = &quot;&quot;, breaks = 20) 2.4.2 Density plots Many people prefer the density plot over the histogram because the histogram is more sensitive to its options. A density plot is essentially a smoothed version of a histogram. It isnt as blocky. It sometimes has weird things happen at the boundaries. The plot and density function can be combined to construct a density plot. plot(density(pima$diastolic, na.rm = TRUE), main = &quot;&quot;) In the example above, we have to specify na.rm = TRUE so that the density is only computed using the non-missing values. 2.4.3 Index plots An index plot is a scatter plot of a numeric variable versus the index of each value (i.e., the position of the value in the vector). This is most useful for sorted vectors. A scatter plot of sorted numeric values versus their index can be used to identify outliers and see whether the data has many repeated values. plot(sort(pima$diastolic), ylab = &quot;sorted diastolic bp&quot;) The flat spots in the plot above show that the diastolic variable has many repeated values. 2.4.4 Bivariate scatter plots Bivariate scatter plots can be used to identify the relationship between two numeric variables. A scatter plot of diabetes vs diastolic blood pressure is shown below. plot(diabetes ~ diastolic, data = pima) There is no clear pattern in the points, so its difficult to claim a relationship between the two variables. 2.4.5 Bivariate boxplots A parallel boxplot of diabetes score versus test result is shown below. plot(diabetes ~ test, data = pima) The median diabetes score seems to be a bit higher for positive tests in comparison to the negative tests. 2.4.6 Multiple plots in one figure The par function can be used to construct multiple plots in one figure. The mfrow argument can be used to specify the number of rows and columns of plots you need. A 1 by 2 set of plots is shown below. par(mfrow = c(1, 2)) plot(diabetes ~ diastolic, data = pima) plot(diabetes ~ test, data = pima) par(mfrow = c(1, 1)) # reset to a single plot 2.5 Visualizing data with ggplot2 The previous plots were created using Rs base graphics system. base graphics are fast and simple to produce while looking professional. A fancier alternative is to construct plots using the ggplot2 package. In its simplest form, to construct a (useful) plot in ggplot2, you need to provide: A ggplot object. This is usually the object that holds your data frame. The data frame is passed to ggplot via the data argument. A geometry object Roughly speaking, this is the kind of plot you want. e.g., geom_hist for a histogram, geom_point for a scatter plot, geom_density for a density plot. An aesthetic mapping Aesthetic mappings describe how variables in the data are mapped to visual properties of a geometry. This is where you specify which variable with be the x variable, the y variable, which variable will control color in the plots, etc. 2.5.1 A ggplot2 histogram library(ggplot2) ggpima &lt;- ggplot(pima) ggpima + geom_histogram(aes(x=diastolic)) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 35 rows containing non-finite values (stat_bin). 2.5.2 A ggplot2 density plot ggpima + geom_density(aes(x = diastolic)) #&gt; Warning: Removed 35 rows containing non-finite values (stat_density). 2.5.3 A ggplot2 scatter plot ggpima + geom_point(aes(x = diastolic, y = diabetes)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). 2.5.4 Scaling ggplot2 plots In general, scaling is the process by which ggplot2 maps variables to unique values. When this is done for discrete variables, ggplot2 will often scale the variable to distinct colors, symbols, or sizes, depending on the aesthetic mapped. In the example below, we map the test variable to the shape aesthetic, which is then scaled to different shapes for the different test levels. ggpima + geom_point(aes(x = diastolic, y = diabetes, shape = test)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). Alternatively, we can map the test variable to the color aesthetic, which creates a plot with different colors for each observation based on the test level. ggpima + geom_point(aes(x = diastolic, y = diabetes, color = test)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). We can even combine these two aesthetic mappings in a single plot to get different colors and symbols for each level of test. ggpima + geom_point(aes(x = diastolic, y = diabetes, shape = test, color = test)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). 2.5.5 Facetting in ggplot2 Facetting creates separate panels (facets) of a data frame based on one or more facetting variables. Below, we facet the data by the test result. ggpima + geom_point(aes(x = diastolic, y = diabetes)) + facet_grid(~ test) #&gt; Warning: Removed 35 rows containing missing values (geom_point). 2.5.6 Summary of ggplot2 To create a ggplot2 plot: Create aggplot object using the ggplot function. Specify the data frame the data is contained in (e.g., the data frame is pima). Specify the geometry for the plot (the kind of plot you want to produce) Specify the aesthetics using aes. The aesthetic specifies what you see, such as position in the \\(x\\) or \\(y\\) direction or aspects such as shape or color. The aesthetic can be specified in the geometry, or if you have consistent aesthetics across multiple geometries, in the ggplot statement. The advantage of ggplot2 is more apparent in producing complex plots involving more than two variables. ggplot2 makes it easy to plot the data for each group with different colors, symbols, line types, etc. ggplot2 will automatically provide a legend mapping the attributes to the different groups. ggplot2 makes it easy to create separate panels with plots for the observations having a certain characteristic. 2.6 Summary of data exploration You should use both numerical and graphical summaries of data prior to modeling data. Data exploration helps us to: Gain understanding about our data Identify problems or unusual features of our data Identify patterns in our data Decide on a modeling approach for the data etc. "],["review-of-probability-random-variables-and-random-vectors.html", "Chapter 3 Review of probability, random variables, and random vectors 3.1 Probability Basics 3.2 Random Variables 3.3 Multivariate distributions 3.4 Random vectors 3.5 Properties of transformations of random vectors 3.6 Multivariate normal (Gaussian) distribution 3.7 Example 1 3.8 Example 2", " Chapter 3 Review of probability, random variables, and random vectors 3.1 Probability Basics Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. A set is a (possibly empty) collection of elements. Sets are denoted as a set of elements between curly braces, i.e., \\(\\{\\omega_1, \\omega_2, \\ldots\\}\\), where the \\(\\omega_i\\) are elements of \\(\\Omega\\). Set \\(A\\) is a subset of set \\(B\\) if every element of \\(A\\) is an element of \\(B\\). This is denoted as \\(A \\subseteq B\\), meaning that \\(A\\) is a subset of \\(B\\). Subsets of \\(\\Omega\\) are events. The null set or empty set, \\(\\emptyset\\), is the set with no elements, i.e., \\(\\{\\}\\). The empty set is a subset of any other set. A function \\(P\\) that assigns a real number \\(P(A)\\) to every event \\(A\\) is a probability distribution if it satisfies three properties: \\(P(A)\\geq 0\\) for all \\(A\\in \\Omega\\) \\(P(\\Omega)=P(\\omega \\in \\Omega) = 1\\) If \\(A_1, A_2, \\ldots\\) are disjoint, then \\(P\\left(\\bigcup_{i=1}^\\infty A_i \\right)=\\sum_{i=1}^\\infty P(A_i)\\). A set of events \\(\\{A_i:i\\in I\\}\\) are independent if \\[P\\left(\\cap_{i\\in J} A_i \\right)=\\prod_{i\\in J} P(A_i ) \\] for every finite subset \\(J\\subseteq I\\). 3.2 Random Variables A random variable \\(Y\\) is a mapping/function \\[Y:\\Omega\\to\\mathbb{R}\\] that assigns a real number \\(Y(\\omega)\\) to each outcome \\(\\omega\\). We typically drop the \\((\\omega)\\) part for simplicity. The cumulative distribution function (CDF) of \\(Y\\), \\(F_Y\\), is a function \\(F_Y:\\mathbb{R}\\to [0,1]\\) defined by \\[F_Y (y)=P(Y \\leq y).\\] The subscript of \\(F\\) indicates the random variable the CDF describes. E.g., \\(F_X\\) denotes the CDF of the random variable \\(X\\) and \\(F_Y\\) denotes the CDF of the random variable \\(Y\\). The subscript can be dropped when the context makes it clear what random variable the CDF describes. The support of \\(Y\\), \\(\\mathcal{S}\\), is the smallest set such that \\(P(Y\\in \\mathcal{S})=1\\). 3.2.1 Discrete random variables \\(Y\\) is a discrete random variable if it takes countably many values \\(\\{y_1, y_2, \\dots \\} = \\mathcal{S}\\). The probability mass function (pmf) for \\(Y\\) is \\(f_Y (y)=P(Y=y)\\), where \\(y\\in \\mathbb{R}\\), and must have the following properties: \\(0 \\leq f_Y(y) \\leq 1\\). \\(\\sum_{y\\in \\mathcal{S}} f_Y(y) = 1\\). Additionally, the following statements are true: \\(F_Y(c) = P(Y \\leq c) = \\sum_{y\\in \\mathcal{S}:y \\leq c} f_Y(y)\\). \\(P(Y \\in A) = \\sum_{y \\in A} f_Y(y)\\) for some event \\(A\\). \\(P(a \\leq Y \\leq b) = \\sum_{y\\in\\mathcal{S}:a\\leq y\\leq b} f_Y(y)\\). The expected value, mean, or first moment of \\(Y\\) is defined as \\[E(Y) = \\sum_{y\\in \\mathcal{S}} y f_Y(y),\\] assuming the sum is well-defined. The variance of \\(Y\\) is defined as \\[\\mathrm{var}(Y)=E(Y-E(Y))^2== \\sum_{y\\in \\mathcal{S}} (y - E(Y))^2 f_Y(y).\\] The standard deviation of Y is \\[SD(Y)=\\sqrt{\\mathrm{var}(Y) }.\\] 3.2.1.1 Example (Bernoulli) A random variable \\(Y\\sim \\mathsf{Bernoulli}(\\pi)\\) if \\(\\mathcal{S} = {0, 1}\\) and \\(P(Y = 1) = \\pi\\), where \\(\\pi\\in (0,1)\\). The pmf of a Bernoulli random variable is \\[f_Y(y) = \\pi^y (1-\\pi)^{(1-y)}.\\] Determine \\(E(Y)\\) and \\(\\mathrm{var}(Y)\\). 3.2.2 Continuous random variables \\(Y\\) is a continuous random variable if there exists a function \\(f_Y (y)\\) such that: \\(f_Y (y)\\geq 0\\) for all \\(y\\), \\(\\int_{-\\infty}^\\infty f_Y (y) dy = 1\\), \\(a\\leq b\\), \\(P(a&lt;Y&lt;b)=\\int_a^b f_Y (y) dy\\). The function \\(f_Y\\) is called the probability density function (pdf). Additionally, \\(F_Y (y)=\\int_{-\\infty}^y f_Y (y) dy\\) and \\(f_Y (y)=F&#39;_Y(y)\\) for any point \\(y\\) at which \\(F_Y\\) is differentiable. The expected value of a continuous random variables \\(Y\\) is defined as \\[E(Y)= \\int_{-\\infty}^{\\infty} y f_Y(y) dy = \\int_{y\\in\\mathcal{S}} y f_Y(y).\\] assuming the integral is well-defined. The variance of a continuous random variable \\(Y\\) is defined by \\[\\mathrm{var}(Y)=E(Y-E(Y))^2=\\int_{-\\infty}^{\\infty} (y - E(Y))^2 f_Y(y) dy = \\int_{y\\in\\mathcal{S}} (y - E(Y))^2 f_Y(y) dy.\\] 3.2.3 Useful facts for transformation of random variables Let \\(Y\\) be a random variable and \\(c\\in\\mathbb{R}\\) be a constant. Then: \\(E(cY) = c E(Y)\\) \\(E(c + Y) = c + E(Y)\\) \\(\\mathrm{var}(cY) = c^2 \\mathrm{var}(Y)\\) \\(\\mathrm{var}(c + Y) = \\mathrm{var}(Y)\\) 3.3 Multivariate distributions 3.3.1 Basic properties Let \\(Y_1,Y_2,\\ldots,Y_n\\) denote \\(n\\) random variables with supports \\(\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_n\\), respectively. If the random variables are jointly (all) discrete, then the joint pmf \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,\\ldots,Y_n=y_n)\\) satisfies the following properties: \\(0\\leq f(y_1,\\ldots,y_n )\\leq 1\\), \\(\\sum_{y_1\\in\\mathcal{S}_1}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) = 1\\), \\(P((Y_1,\\ldots,Y_n)\\in A)=\\sum_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n)\\). In this context, \\[E(Y_1 \\cdots Y_n)=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n}y_1 \\cdots y_n f(y_1,\\ldots,y_n).\\] In general, \\[E(g(Y_1,\\ldots,Y_n))=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n),\\] where \\(g\\) is a function of the random variables. If the random variables are jointly continuous, then \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,\\ldots,Y_n=y_n)\\) is the joint pdf if it satisfies the following properties: \\(f(y_1,\\ldots,y_n ) \\geq 0\\), \\(\\int_{y_1\\in\\mathcal{S}_1}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) dy_n \\cdots dy_1 = 1\\), \\(P((Y_1,\\ldots,Y_n)\\in A)=\\int \\cdots \\int_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n) dy_n\\ldots dy_1\\). In this context, \\[E(Y_1 \\cdots Y_n)=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} y_1 \\cdots y_n f(y_1,\\ldots,y_n) dy_n \\ldots dy_1.\\] In general, \\[E(g(Y_1,\\ldots,Y_n))=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n) dy_n \\cdots dy_1,\\] where \\(g\\) is a function of the random variables. 3.3.2 Marginal distributions If the random variables are jointly discrete, then the marginal pmf of \\(Y_1\\) is \\[f_{Y_1}(y_1)=\\sum_{y_2\\in\\mathcal{S}_2}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n).\\] Similarly, if the random variables are jointly continuous, then the marginal pdf of \\(Y_1\\) is \\[f_{Y_1}(y_1)=\\int_{y_2\\in\\mathcal{S}_2}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n) dy_n \\cdots dy_2.\\] 3.3.3 Independence of random variables Random variables \\(X\\) and \\(Y\\) are independent if \\[F(x, y) = F_X(x) F_Y(y).\\] Alternatively, \\(X\\) and \\(Y\\) are independent if \\[f(x, y) = f_X(x)f_Y(y).\\] 3.3.4 Conditional distributions Let \\(X\\) and \\(Y\\) be random variables. The conditional distribution of \\(X\\) given \\(Y = y\\), denoted \\(X|Y=y\\) is \\[ f(x|y) = f(x, y)/f_{Y}(y).\\] 3.3.5 Covariance The covariance between random variables \\(X\\) and \\(Y\\) is \\[\\mathrm{cov}(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).\\] 3.3.6 Useful facts for transformations of multiple random variables Let \\(a\\) and \\(b\\) be scalar constants. Then: \\(E(aY)=aE(Y)\\) \\(E(a+Y)=a+E(Y)\\) \\(E(aY+bZ)=aE(Y)+bE(Z)\\) \\(\\mathrm{var}(aY)=a^2 \\mathrm{var}(Y)\\) \\(\\mathrm{var}(a+Y)=\\mathrm{var}(Y)\\) \\(\\mathrm{var}(Y+Z)=\\mathrm{var}(Y)+\\mathrm{var}(Z)+2\\mathrm{cov}(Y,Z)\\) \\(\\mathrm{cov}(a,Y)=0\\) \\(\\mathrm{cov}(Y,Y)=\\mathrm{var}(Y)\\) \\(\\mathrm{cov}(aY,bZ)=ab\\mathrm{cov}(Y,Z)\\) \\(\\mathrm{cov}(a + Y,b + Z)=\\mathrm{cov}(Y,Z)\\) 3.4 Random vectors 3.4.1 Definition Let \\(\\mathbf{y}=(Y_1,Y_2,\\dots,Y_n )^T\\) be an \\(n\\times1\\) vector of random variables. \\(\\mathbf{y}\\) is a random vector. A vector is always defined to be a column vector, even if the notation is ambiguous. 3.4.2 Mean, variance, and covariance The mean of a random vector is \\[E(\\mathbf{y})=\\begin{pmatrix}E(Y_1)\\\\E(Y_2)\\\\\\vdots\\\\E(Y_n)\\end{pmatrix}.\\] The variance (covariance) of a random vector is \\[\\begin{aligned} \\mathrm{var}(\\mathbf{y}) &amp;= E(\\mathbf{y}\\mathbf{y}^T )-E(\\mathbf{y})E(\\mathbf{y})^T\\\\ &amp;= \\begin{pmatrix}\\mathrm{var}(Y_1) &amp; \\mathrm{cov}(Y_1,Y_2) &amp;\\dots &amp;\\mathrm{cov}(Y_1,Y_n)\\\\\\mathrm{cov}(Y_2,Y_1 )&amp;\\mathrm{var}(Y_2)&amp;\\dots&amp;\\mathrm{cov}(Y_2,Y_n)\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ \\mathrm{cov}(Y_n,Y_1)&amp;\\mathrm{cov}(Y_n,Y_2)&amp;\\dots&amp;\\mathrm{var}(Y_n)\\end{pmatrix}\\end{aligned}.\\] Let \\(\\mathbf{x} = (X_1, X_2, \\ldots, X_n)^T\\) and \\(\\mathbf{y} = (Y_1, Y_2, \\ldots, Y_n)^T\\) be \\(n\\times 1\\) random vectors. The covariance between two random vectors is \\[\\mathrm{cov}(\\mathbf{x}, \\mathbf{y}) = E(\\mathbf{x}, \\mathbf{y}^T) - E(\\mathbf{x}) E(\\mathbf{y})^T.\\] 3.5 Properties of transformations of random vectors Define: \\(\\mathbf{a}\\) to be an \\(n\\times 1\\) vector of constants \\(A\\) to be an \\(m\\times n\\) matrix of constants \\(\\mathbf{x}=(X_1,X_2,\\ldots,X_n)^T\\) to be an \\(n\\times 1\\) random vector \\(\\mathbf{y}=(Y_1,Y_2,\\ldots,Y_n)^T\\) to be an \\(n\\times 1\\) random vector \\(\\mathbf{z}=(Z_1,Z_2,\\ldots,Z_n)^T\\) to be an \\(n\\times 1\\) random vector \\(0_{n\\times n}\\) to be an \\(n\\times n\\) matrix of zeros. Then: \\(E(A\\mathbf{y})=AE(\\mathbf{y}), E(\\mathbf{y}A^T )=E(\\mathbf{y}) A^T.\\) \\(E(\\mathbf{x}+\\mathbf{y})=E(\\mathbf{x})+E(\\mathbf{y})\\) \\(\\mathrm{var}(A\\mathbf{y})=A\\mathrm{var}(\\mathbf{y}) A^T\\) \\(\\mathrm{cov}(\\mathbf{x}+\\mathbf{y},\\mathbf{z})=\\mathrm{cov}(\\mathbf{x},\\mathbf{z})+\\mathrm{cov}(\\mathbf{y},\\mathbf{z})\\) \\(\\mathrm{cov}(\\mathbf{x},\\mathbf{y}+\\mathbf{z})=\\mathrm{cov}(\\mathbf{x},\\mathbf{y})+\\mathrm{cov}(\\mathbf{x},\\mathbf{z})\\) \\(\\mathrm{cov}(A\\mathbf{x},\\mathbf{y})=A\\ cov(\\mathbf{x},\\mathbf{y})\\) \\(\\mathrm{cov}(\\mathbf{x},A\\mathbf{y})=\\mathrm{cov}(\\mathbf{x},\\mathbf{y}) A^T\\) \\(\\mathrm{var}(a)= 0_{n\\times n}\\) \\(\\mathrm{cov}(\\mathbf{a},\\mathbf{y})=0_{n\\times n}\\) \\(\\mathrm{var}(\\mathbf{a}+\\mathbf{y})=var(\\mathbf{y})\\) 3.6 Multivariate normal (Gaussian) distribution 3.6.1 Definition \\(\\mathbf{y}=(Y_1,\\dots,Y_n )^T\\) has a multivariate normal distribution with mean \\(E(\\mathbf{y})=\\boldsymbol{\\mu}\\) (an \\(n\\times 1\\) vector) and covariance \\(\\mathrm{var}(\\mathbf{y})=\\boldsymbol{\\Sigma}\\) (an \\(n\\times n\\) matrix) if the joint pdf is \\[f(\\mathbf{y})=\\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2} } \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{y}-\\boldsymbol{\\mu})\\right),\\] where \\(|\\boldsymbol{\\Sigma}|\\) is the determinant of \\(\\boldsymbol{\\Sigma}\\). Note that \\(\\boldsymbol{\\Sigma}\\) must be symmetric and positive definite. We would denote this as \\(\\mathbf{y}\\sim N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\). 3.6.2 Useful facts Important fact: A linear function of a multivariate normal random vector (i.e., \\(\\mathbf{a}+A\\mathbf{y}\\)) is also multivariate normal (though it could collapse to a single random variable if \\(A\\) is a \\(1\\times n\\) vector). Application: Suppose that \\(\\mathbf{y}\\sim N(\\mu,\\Sigma)\\). For an \\(m\\times n\\) matrix of constants \\(A\\), \\(A\\mathbf{y}\\sim N(A\\mu,A\\Sigma A^T)\\). 3.7 Example 1 3.7.1 Bernoulli distribution A random variable \\(Y\\sim \\mathrm{Bernoulli}(\\theta)\\) when \\(\\mathcal{S} = \\{0, 1\\}\\) and the pmf of a Bernoulli random variable is \\[f(y\\mid\\theta)=\\theta^y (1-\\theta)^{(1-y)}.\\] Determine \\(E(Y)\\) \\[\\\\[4in]\\] Determine \\(\\mathrm{var}(Y)\\) \\[\\\\[4in]\\] 3.7.2 Binomial distribution A random variable \\(Y\\sim \\mathrm{Bin}(n,\\theta)\\) when \\(\\mathcal{S}=\\{0,1,2,\\ldots,n\\}\\) and the pmf is \\[f(y\\mid\\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{(n-y)}.\\] Alternatively, let \\(Y_1,Y_2,\\ldots,Y_n\\stackrel{i.i.d.}{\\sim} \\mathrm{Bernoulli}(\\theta)\\). Then \\(Y=\\sum_{i=1}^n Y_i \\sim \\mathrm{Bin}(n,\\theta)\\). Determine \\(E(Y)\\) \\[\\\\[4in]\\] Determine \\(\\mathrm{var}(Y)\\) \\[\\\\[4in]\\] Assume \\(Y\\sim \\mathrm{Bin}(20,0.4)\\). Determine \\(F(8)\\). \\[\\\\[4in]\\] Determine \\(P(8\\leq Y \\leq 10)\\). \\[\\\\[4in]\\] 3.7.3 Poisson Distribution \\(Y \\sim \\mathrm{Poisson}(\\theta)\\) when \\(\\Omega=\\{0,1,2,\\ldots\\}\\) and \\[f(y\\mid\\theta)=\\frac{1}{y!} \\theta^y e^{-\\theta}.\\] Determine \\(E(Y)\\) \\[\\\\[4in]\\] Determine \\(\\mathrm{var}(Y)\\) \\[\\\\[4in]\\] Assume \\(Y\\sim \\mathrm{Poisson}(4)\\). Determine \\(F(12)\\) \\[\\\\[4in]\\] Determine \\(P(15\\leq Y \\leq 20)\\). \\[\\\\[4in]\\] 3.8 Example 2 Gasoline is to be stocked in a bulk tank once at the beginning of each week and then sold to individual customers. Let \\(Y_1\\) denote the proportion of the capacity of the bulk tank that is available after the tank is stocked at the beginning of the week. Because of the limited supplies, \\(Y_1\\) varies from week to week. Let \\(Y_2\\) denote the proportion of the capacity of the bulk tank that is sold during the week. Because \\(Y_1\\) and \\(Y_2\\) are both proportions, both variables are between 0 and 1. Further, the amount sold, \\(y_2\\), cannot exceed the amount available, \\(y_1\\). Suppose the joint density function for \\(Y_1\\) and \\(Y_2\\) is given by \\[f(y_1,y_2 )=3y_1;\\ 0 \\leq y_2\\leq y_1\\leq 1.\\] 3.8.1 Problem 1 Determine \\(P(0\\leq Y_1\\leq 0.5;\\ 0.25\\leq Y_2)\\) \\[\\\\[4in]\\] 3.8.2 Problem 2 Determine \\(f_{Y_1 }\\) and \\(f_{Y_2 }\\) \\[\\\\[4in]\\] 3.8.3 Problem 3 Determine \\(E(Y_1)\\) and \\(E(Y_2)\\) \\[\\\\[4in]\\] 3.8.4 Problem 4 Determine \\(\\mathrm{var}(Y_1)\\) and \\(\\mathrm{var}(Y_2)\\) \\[\\\\[4in]\\] 3.8.5 Problem 5 Determine \\(E(Y_1 Y_2)\\) \\[\\\\[4in]\\] 3.8.6 Problem 6 Determine \\(\\mathrm{cov}(Y_1,Y_2)\\) \\[\\\\[4in]\\] 3.8.7 Problem 7 Determine the mean and variance of \\(\\mathbf{a}^T \\mathbf{y}\\), where \\(\\mathbf{a}=(1,-1)^T\\) and \\(\\mathbf{y}=(Y_1,Y_2 )^T\\). This is the expectation and variance of the difference between the amount of gas available and the amount of gas sold. \\[\\\\[4in]\\] "],["useful-matrix-facts.html", "Chapter 4 Useful matrix facts 4.1 Notation 4.2 Basic mathematical properties 4.3 Transpose and related properties 4.4 Special matrices 4.5 Matrix inverse 4.6 Matrix derivatives", " Chapter 4 Useful matrix facts A matrix is a two-dimensional array of values, symbols, or other objects (depending on the context). We will assume that our matrices contain numbers or random variables. Context will make it clear which is being represented. Matrices are commonly denoted by bold capital letters like \\(\\mathbf{A}\\) or \\(\\mathbf{B}\\), but this will sometimes be simplified to capital letters like \\(A\\) or \\(B\\). 4.1 Notation A matrix \\(\\mathbf{A}\\) with \\(m\\) rows and \\(n\\) columns (an \\(m\\times n\\) matrix) will be denoted as \\[\\mathbf{A} = \\begin{bmatrix} \\mathbf{A}_{1,1} &amp; \\mathbf{A}_{2,1} &amp; \\cdots &amp; \\mathbf{A}_{1,n} \\\\ \\mathbf{A}_{2,1} &amp; \\mathbf{A}_{2,1} &amp; \\cdots &amp; \\mathbf{A}_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{m,1} &amp; \\mathbf{A}_{m,2} &amp; \\cdots &amp; \\mathbf{A}_{m,n} \\\\ \\end{bmatrix}, \\] where \\(\\mathbf{A}_{i,j}\\) denotes the element in row \\(i\\) and column \\(j\\) of matrix \\(\\mathbf{A}\\). A column vector is a matrix with a single column. A row vector is a matrix with a single row. Vectors are commonly denoted with bold lowercase letters such as \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\), but this may be simplified to lowercase letters such as \\(a\\) or \\(b\\). A \\(p\\times 1\\) column vector \\(\\mathbf{a}\\) may constructed as \\[\\mathbf{a} = [a_1, a_2, \\ldots, a_p]^T = \\begin{bmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_p \\end{bmatrix}^T = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{bmatrix}.\\] 4.2 Basic mathematical properties 4.2.1 Addition and subtraction Consider matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with identical sizes \\(m\\times n\\). We add \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) by adding the element in position \\(i,j\\) of \\(\\mathbf{B}\\) with the element in position \\(i,j\\) of \\(A\\), i.e., \\[(\\mathbf{A} + \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} + \\mathbf{B}_{i,j}.\\] Similarly, if we subtract \\(\\mathbf{B}\\) from matrix \\(\\mathbf{A}\\), then we subtract the element in position \\(i,j\\) of \\(\\mathbf{B}\\) from the element in position \\(i,j\\) of \\(\\mathbf{A}\\), i.e., \\[(\\mathbf{A} - \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} - \\mathbf{B}_{i,j}.\\] Example: \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix} + \\begin{bmatrix} 2 &amp; 9 &amp; 1 \\\\ 1 &amp; 3 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 11 &amp; 4 \\\\ 5 &amp; 8 &amp; 7 \\\\ \\end{bmatrix}.\\] \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix} - \\begin{bmatrix} 2 &amp; 9 &amp; 1 \\\\ 1 &amp; 3 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} -1 &amp; -7 &amp; 2 \\\\ 3 &amp; 2 &amp; 5 \\\\ \\end{bmatrix}.\\] 4.2.2 Scalar multiplication A matrix multiplied by a scalar value \\(c\\in\\mathbb{R}\\) is the matrix obtained by multiplying each element of the matrix by \\(c\\). If \\(\\mathbf{A}\\) is a matrix and \\(c\\in \\mathbb{R}\\), then \\[(c\\mathbf{A})_{i,j} = c\\mathbf{A}_{i,j}.\\] Example: \\[3\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix}= \\begin{bmatrix} 3\\cdot 1 &amp; 3\\cdot 2 &amp; 3\\cdot 3 \\\\ 3\\cdot 4 &amp; 3\\cdot 5 &amp; 3\\cdot 6 \\\\ \\end{bmatrix}= \\begin{bmatrix} 3 &amp; 6 &amp; 9 \\\\ 12 &amp; 15 &amp; 18 \\\\ \\end{bmatrix}.\\] 4.2.3 Matrix multiplication Consider two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). The matrix product \\(\\mathbf{AB}\\) is only defined if the number of columns in \\(\\mathbf{A}\\) matches the number of rows in \\(\\mathbf{B}\\). Assume \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix and \\(\\mathbf{B}\\) is an \\(n\\times p\\) matrix. \\(\\mathbf{AB}\\) will be an \\(m\\times p\\) matrix and \\[(\\mathbf{AB})_{i,j} = \\sum_{k=1}^{n} \\mathbf{A}_{i,k}\\mathbf{B}_{k,j}.\\] Example: \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 4\\\\ 2 &amp; 5\\\\ 3 &amp; 6 \\end{bmatrix}= \\begin{bmatrix} 1\\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3 &amp; 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6\\\\ 4\\cdot 1 + 5 \\cdot 2 + 6 \\cdot 3 &amp; 4 \\cdot 4 + 5 \\cdot 5 + 6 \\cdot 6\\\\ \\end{bmatrix}= \\begin{bmatrix} 14 &amp; 32\\\\ 32 &amp; 77\\\\ \\end{bmatrix}.\\] 4.2.4 Associative property Addition and multiplication satisfy the associative property for matrices. Assuming that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to do the operations below, then \\[(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\] and \\[(\\mathbf{AB})\\mathbf{C}=\\mathbf{A}(\\mathbf{BC}).\\] 4.2.5 Distributive property Matrix operations satisfy the distributive property. Assuming that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to do the operations below, then \\[\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} + \\mathbf{AC}\\quad\\mathrm{and}\\quad (\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC} + \\mathbf{BC}.\\] 4.2.6 No commutative property In general, matrix multiplication does not satisfy the commutative property, i.e., \\[AB \\neq BA,\\] even when the matrix sizes allow the operation to be performed. Example: \\[\\begin{bmatrix} 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5 \\end{bmatrix}\\] \\[ \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 2\\\\ 2 &amp; 4 \\end{bmatrix}\\] 4.3 Transpose and related properties 4.3.1 Definition The transpose of a matrix, denoted \\(T\\) as a superscript, exchanges the rows and columns of the matrix. More formally, the \\(i,j\\) element of \\(\\mathbf{A}^T\\) is the \\(j,i\\) element of \\(\\mathbf{A}\\), i.e., \\((\\mathbf{A}^T)_{i,j} = \\mathbf{A}_{j,i}\\). Example: \\[\\begin{bmatrix} 2 &amp; 9 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix}^T = \\begin{bmatrix} 2 &amp; 4\\\\ 9 &amp; 5\\\\ 3 &amp; 6 \\end{bmatrix}.\\] 4.3.2 Transpose and mathematical operations Assume that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to perform the operations below. Additionally, assume that \\(c\\in \\mathbb{R}\\) is a scalar constant. The following properties are true: \\(c^T = c\\) \\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\) \\((\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T\\), which can be extended to \\((\\mathbf{ABC})^T=\\mathbf{C}^T \\mathbf{B}^T \\mathbf{A}^T\\), etc. \\((\\mathbf{A}^T)^T=\\mathbf{A}\\) 4.4 Special matrices 4.4.1 Square matrices A matrix is square if the number of rows equals the number of columns. The diagonal elements of an \\(n\\times n\\) square matrix \\(\\mathbf{A}\\) are the elements \\(\\mathbf{A}_{i,i}\\) for \\(i = 1, 2, \\ldots, n\\). Any non-diagonal elements of \\(\\mathbf{A}\\) are called off-diagonal elements. 4.4.2 Identity matrix The \\(n\\times n\\) identity matrix \\(\\mathbf{I}_{n\\times n}\\) is 1 for its diagonal elements and 0 for its off-diagonal elements. Context often makes it clear what the dimensions of an identity matrix are, so \\(\\mathbf{I}_{n\\times n}\\) is often simplified to \\(\\mathbf{I}\\) or \\(I\\). Example: \\[\\mathbf{I}_{3\\times 3} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] 4.4.3 Symmetric A matrix \\(\\mathbf{A}\\) is symmetric if \\(\\mathbf{A} = \\mathbf{A}^T\\), i.e., \\(\\mathbf{A}_{i,j} = \\mathbf{A}_{j,i}\\) for all potential \\(i,j\\). A symmetric matrix must be square. 4.4.4 Idempotent A matrix is idempotent if \\(\\mathbf{AA} = \\mathbf{A}\\) An idempotent matrix must be square. 4.5 Matrix inverse An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is invertible if there exists a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{AB}=\\mathbf{BA}=\\mathbf{I}_{n\\times n}\\). The inverse of \\(\\mathbf{A}\\) is denoted \\(\\mathbf{A}^{-1}\\). Inverse matrices only exist for square matrices. Some other properties related to the inverse operator: If \\(n\\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are invertible then \\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A} ^{-1}\\). If \\(\\mathbf{A}\\) is invertible then \\((\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}\\). 4.6 Matrix derivatives We start with some basic calculus results. Let \\(f(b)\\) be a function of a scalar value \\(b\\) and \\(\\frac{df(b)}{db}\\) denote the derivative of the function with respect to \\(b\\). Assume \\(x\\) is a fixed value. Then the following is true: \\(f(b)\\) \\(\\frac{df(b)}{db}\\) \\(bx\\) \\(x\\) \\(b^2\\) \\(2b\\) \\(x b^2\\) \\(2bx\\) Now lets look at the deriviate of a scalar function with respect to a vector. Let \\(f(\\mathbf{b})\\) be a function of a \\(p\\times 1\\) column vector \\(\\mathbb{b}=[b_1, b_2, \\ldots, b_p]^T\\). The derivative of \\(f(\\mathbf{b})\\) with respect to \\(\\mathbf{b}\\) is denoted \\(\\frac{\\partial f(\\mathbf{b})}{\\partial \\mathbf{b}}\\) and \\[\\frac{\\partial f(\\mathbf{b})}{\\partial \\mathbf{b}} = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{b})}{\\partial b_1}\\\\ \\frac{\\partial f(\\mathbf{b})}{\\partial b_2}\\\\ \\vdots \\\\ \\frac{\\partial f(\\mathbf{b})}{\\partial b_p} \\end{bmatrix}.\\] Assume \\(\\mathbf{X}\\) is a fixed matrix. The following is true: \\(f(\\mathbf{b})\\) \\(\\frac{\\partial f(\\mathbf{b})}{\\partial \\mathbf{b}}\\) \\(\\mathbf{b}^T \\mathbf{X}\\) \\(\\mathbf{X}\\) \\(\\mathbf{b}^T \\mathbf{b}\\) \\(2\\mathbf{b}\\) \\(\\mathbf{b}^T \\mathbf{X} \\mathbf{b}\\) \\(2\\mathbf{X}\\mathbf{b}\\) "],["linear-model-estimation.html", "Chapter 5 Linear model estimation 5.1 A simple motivating example 5.2 Defining a linear model 5.3 Estimation of the simple linear regression model 5.4 Penguins simple linear regression example 5.5 Estimation of the multiple linear regression coefficients) 5.6 Penguins multiple linear regression example 5.7 Categorical predictors 5.8 Penguins multiple linear regression example with categorical predictor 5.9 Evaluating model fit 5.10 Summary of notation 5.11 Summary of functions used in this chapter 5.12 Summarizing the components of a linear model 5.13 Going Deeper", " Chapter 5 Linear model estimation 5.1 A simple motivating example Suppose you observe data related to the heights of 5 mothers and their adult daughters. The observed heights (measured in inches) are provided in Table 5.1. Table 5.1: Heights of mothers and their adult daughters (in). observation mothers height (in) daughters height (in) 1 57.5 61.5 2 60.5 63.5 3 63.5 63.5 4 66.5 66.5 5 69.5 66.5 The 5 pairs of observed data are denoted \\[(x_1, Y_1), (x_2, Y_2), \\ldots, (x_5, Y_5),\\] with \\((x_i, Y_i)\\) denoting the data for observation \\(i\\). \\(x_i\\) denotes the mothers height for observation \\(i\\) and \\(Y_i\\) denotes the daughters height for observation \\(i\\). In this data set, e.g., \\(x_3 = 63.5\\) and \\(Y_5= 66.5\\). Figure 5.1 displays a scatter plot of height data provided in Table 5.1. The relationship between the points is approximately a straight line. Thus, we will model the typical (mean) relationship between the height of mothers and their adult daughters as a straight line. Figure 5.1: A scatter plot displaying pairs of heights for a mother and her adult daughter. The \\(x_1,x_2,\\ldots,x_5\\) are observed values of a random variable \\(X\\), while \\(Y_1, Y_2, \\ldots, Y_5\\) are observed values of a random variable \\(Y\\). Thus, \\(X\\) denotes the height a mother and \\(Y\\) denotes the height of (one of) their adult daughter(s). We want to model variable \\(Y\\) using variable \\(X\\). The variable we are trying to model is known as the response variable. The variables we use to model the response are known as regressor variables. Response variables are also known as outcome, output, or dependent variables. Regressor variables are also known as explanatory, predictor, input, dependent, or feature variables. A regression model describes the typical relationship between the response variable \\(Y\\) as a function of the regressor variable \\(X\\). More formally, the regression model for \\(Y\\) as a function of \\(X\\), denoted \\(E(Y|X)\\) is the expected value of \\(Y\\) conditional on the regressor \\(X\\). The regression model specifically refers to the expected relationship between the response and regressors. A simple linear regression model assumes the regression model between \\(Y\\) and \\(X\\) is a straight line using the equation \\[E(Y\\mid X)=\\beta_0 + \\beta_1 X.\\] \\(\\beta_0\\) and \\(\\beta_1\\) are the intercept and slope of our regression functions. In general, \\(\\beta_0\\) and \\(\\beta_1\\) are known as regression coefficients and are model parameters that we estimate from our data. The estimated regression model is denoted by \\[\\hat{E}(Y|X)=\\hat{\\beta}_0 + \\hat{\\beta}_1 X,\\] where \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are values of \\(\\beta_0\\) and \\(\\beta_1\\) that we estimate from the data. A \\(\\hat{}\\) above a term indicates it is an estimate. We will refer to \\(\\hat{E}(Y|X)\\) as the fitted model or estimated regression model. How do we determine the best fitting model? Consider Figure 5.2, in which 2 potential best fitting models are drawn on the scatter plot of the height data. Which one is best? Figure 5.2: Comparison of three potential fitted models to some observed data. The fitted models are shown in grey. The rest of this chapter focuses on defining and estimating the parameters of a linear regression model. 5.2 Defining a linear model 5.2.1 Necessary components and notation We begin by defining notation for the components of a linear model and some of their important properties. We repeat some of the previous discussion for clarity. \\(Y\\) denotes the response variable. The response variable is treated as a random variable. We will observe realizations of this random variable for each observation in our data set. \\(X\\) denotes a single regressor variable. \\(X_1\\), \\(X_2\\), , \\(X_{p-1}\\) denote distinct regressor variables if we are performing multiple regression. The regressor variables are treated as non-random variables. The observed values of the regressor variables are treated as fixed, known values. \\(\\mathbb{X}=\\{X_1,\\ldots,X_{p-1}\\}\\) denotes the collection of all regressors under consideration, though this notation is really only needed in the context of multiple regression. \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_{p-1}\\) denote regression coefficients. Regression coefficients are statistical parameters that we will estimate from our data. The regression coefficients are treated as fixed (non-random) but unknown values. Regression coefficients are not observable. \\(\\epsilon\\) denotes model error. The model error is more accurately described as random variation of each observation from the regression model \\(E(\\epsilon\\mid\\mathbb{X})\\). The error is treated as a random variable. The error is assumed to have mean 0 for all values of the regressors, i.e., \\(E(\\epsilon\\mid\\mathbb{X}) = 0\\). The variance of the errors is assumed to be a constant value for all values of the regressors, i.e., \\(\\mathrm{var}(\\epsilon\\mid\\mathbb{X})=\\sigma^2\\). The error is never observable (except in the context of a simulation study where the experimenter literally defines the true model). 5.2.2 Standard definition of linear model A linear model for \\(Y\\) is defined by the equation \\[\\begin{align} Y &amp;= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon \\\\ &amp;= E(Y \\mid \\mathbb{X}) + \\epsilon \\tag{5.1} \\end{align}\\] We write the model using the form in Equation (5.1) to emphasize the fact the response value equals the expected response for that combination of regressor values plus some error. It should be clear from comparing Equation (5.1) with the previous line that \\[E(Y \\mid \\mathbb{X}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1},\\] which we will prove later. More generally, one can say that a regression model is linear if the mean function can be written as a linear combination of the regression coefficients and known values created from our regressor variables, i.e., \\[\\begin{equation} E(Y \\mid X_1, X_2, \\ldots, X_{p-1}) = \\sum_{j=0}^{p-1} c_j \\beta_j, \\tag{5.2} \\end{equation}\\] where \\(c_0, c_1, \\ldots, c_{p-1}\\) are known functions of the regressor variables, e.g., \\(c_1 = X_1 X_2 X_3\\), \\(c_3 = X_2^2\\), \\(c_8 = \\ln(X_1)/X_2^2\\), etc. Thus, if \\(g_0,\\ldots,g_{p-1}\\) are functions of \\(\\mathbb{X}\\), then we can say that the regression model is linear if it can be written as \\[E(Y\\mid \\mathbb{X}) = \\sum_{j=0}^{p-1} g_j(\\mathbb{X})\\beta_j.\\] Some examples of linear regression models: \\(E(Y|X) = \\beta_0\\). \\(E(Y|X) = \\beta_0 + +\\beta_1 X + \\beta_2 X^2\\). \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2\\). \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 \\ln(X_1) + \\beta_2 X_2^{-1}\\). \\(E(\\ln(Y)|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). \\(E(Y^{-1}|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). Some examples of non-linear regression models: \\(E(Y|X) = \\beta_0 + e^{\\beta_1 X}\\). \\(E(Y|X) = \\beta_0 + \\beta_1 X/(\\beta_2 + X)\\). The latter regression models are non-linear models because there is no way to express them using the expression in Equation (5.2). There are many different methods of parameter estimation in statistics: method-of-moments, maximum likelihood, Bayesian, etc. The most common parameter estimation method for linear models is the least squares method, which is commonly called Ordinary Least Squares (OLS) estimation. OLS estimation estimates the regression coefficients with the values that minimize the residual sum of squares (RSS), which we will define shortly. 5.3 Estimation of the simple linear regression model 5.3.1 Fitted values, residuals, and RSS Recall that a simple linear regression model is defined by the equation \\[Y = \\beta_0 + \\beta_1 X + \\epsilon = E(Y|X) + \\epsilon\\] where \\[E(Y|X) = \\beta_0 + \\beta_1 X.\\] In a simple linear regression context, we have \\(n\\) observed responses \\(Y_1,Y_2,\\ldots,Y_n\\) and \\(n\\) regressor values \\(x_1,x_2,\\ldots,x_n\\). Let \\(\\hat{\\beta}_j\\) denote the estimated value of \\(\\beta_j\\) and \\(\\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\) denote the estimated regression model. The \\(i\\)th fitted value is defined as \\[\\begin{equation} \\hat{Y}_i = \\hat{E}(Y|X = x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i. \\tag{5.3} \\end{equation}\\] Thus, the \\(i\\)th fitted value is the estimated mean of \\(Y\\) when the regressor \\(X=x_i\\). More specifically, the \\(i\\)th fitted value is the estimated mean response for the combination of regressor values observed for the \\(i\\)th observation. The \\(i\\)th residual is defined as \\[\\begin{equation} \\hat{\\epsilon}_i = Y_i - \\hat{Y}_i. \\tag{5.4} \\end{equation}\\] The \\(i\\)th residual is the difference between the response and estimated mean response of observation \\(i\\). The RSS of a regression model is the sum of its squared residuals. The RSS for a simple linear regression model, as a function of the estimated regression coefficients \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), is defined as \\[\\begin{equation} RSS(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\sum_{i=1}^n \\hat{\\epsilon}_i^2. \\tag{5.5} \\end{equation}\\] Using several of objects defined above, there are many equivalent expressions for the RSS. Notably, Equation (5.5) can be rewritten using Equations (5.4) and (5.3) as \\[\\begin{align*} RSS(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp;= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 &amp; \\\\ &amp;= \\sum_{i=1}^n (Y_i - \\hat{E}(Y|X=x_i))^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2. \\end{align*}\\] The fitted model is the estimated model that minimizes the RSS, i.e., the fitted model (in the context of simple linear regression) is defined as \\[\\begin{equation} \\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X. \\tag{5.6} \\end{equation}\\] In a simple linear regression context, the fitted model is known as the line of best fit. In Figure 5.3, we visualize the response values, fitted values, residuals, and fitted model in a simple linear regression context. Note that: The fitted model is shown as the dashed grey line and minimizes the RSS. The fitted values, shown as blue xs, are the values returned by evaluating the fitted model at the observed regressor values. The residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative. The RSS is the sum of the squared vertical distances between the response and fitted values. Figure 5.3: Visualization of the response values, fitted values, residuals, and fitted model. 5.3.2 OLS estimators of the simple linear regression parameters The estimates of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the RSS for a simple linear regression model can be obtained analytically using basic calculus under minimal assumptions. Specifically, the optimal analytical solutions for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are valid as long as the regressor values are not a constant value, i.e, \\(x_i \\neq x_j\\) for at least some \\(i,j\\in \\{1,2,\\ldots,n\\}\\). Define \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\) and \\(\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\). The OLS estimators of the simple linear regression coefficients are \\[\\begin{align} \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i=1}^n x_i Y_i - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)\\biggl(\\sum_{i=1}^n Y_i\\biggr)}{\\sum_{i=1}^n x_i^2 - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)^2} \\notag \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\notag \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})Y_i}{\\sum_{i=1}^n (x_i - \\bar{x})x_i} \\tag{5.7} \\end{align}\\] and \\[\\begin{equation} \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}. \\tag{5.8} \\end{equation}\\] We emphasize once again that the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\) are the estimators that minimize the RSS. In edition to the regression coefficients, the other parameter we discussed (in Section 5.2.1) is the error variance, \\(\\sigma^2\\). The most common estimator of the error variance is \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{RSS}{n-p}, \\tag{5.9} \\end{equation}\\] where \\(p\\) is the number of regression coefficients. In general, \\(n-p\\) is the degrees of freedom of the RSS. In a simple linear regression context, the denominator of Equation (5.9) is \\(n-2\\). 5.4 Penguins simple linear regression example We will use the penguins data set in the palmerpenguins package (Horst, Hill, and Gorman 2020) to illustrate a very basic simple linear regression analysis. The penguins data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman, Williams, and Fraser (2014). We start by loading the data into memory. data(penguins, package = &quot;palmerpenguins&quot;) The data set includes 344 observations of 8 variables. The variables are: species: a factor indicating the penguin species island: a factor indicating the island the penguin was observed bill_length_mm: a numeric variable indicating the bill length in millimeters bill_depth_mm: a numeric variable indicating the bill depth in millimeters flipper_length_mm: an integer variable indicating the flipper length in millimeters body_mass_g: an integer variable indicating the body mass in grams sex: a factor indicating the penguin sex (female, male) year: an integer denoting the study year the penguin was observed (2007, 2008, or 2009) We begin by creating a scatter plot of bill_length_mm versus body_mass_g (y-axis versus x-axis) in Figure 5.4. We see a clear positive association between body mass and bill length: as the body mass increases, the bill length tends to increase. The pattern is linear, i.e., roughly a straight line. plot(bill_length_mm ~ body_mass_g, data = penguins, ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;, main = &quot;Penguin size measurements&quot;) Figure 5.4: A scatter plot of penguin bill length (mm) versus body mass (g) We first perform a single linear regression analysis manually using the equations previously provided by regressing bill_length_mm on body_mass_g. Using the summary function on the penguins data frame, we see that both bill_length_mm and body_mass_g have NA values. summary(penguins) #&gt; species island bill_length_mm bill_depth_mm #&gt; Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10 #&gt; Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60 #&gt; Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30 #&gt; Mean :43.92 Mean :17.15 #&gt; 3rd Qu.:48.50 3rd Qu.:18.70 #&gt; Max. :59.60 Max. :21.50 #&gt; NA&#39;s :2 NA&#39;s :2 #&gt; flipper_length_mm body_mass_g sex year #&gt; Min. :172.0 Min. :2700 female:165 Min. :2007 #&gt; 1st Qu.:190.0 1st Qu.:3550 male :168 1st Qu.:2007 #&gt; Median :197.0 Median :4050 NA&#39;s : 11 Median :2008 #&gt; Mean :200.9 Mean :4202 Mean :2008 #&gt; 3rd Qu.:213.0 3rd Qu.:4750 3rd Qu.:2009 #&gt; Max. :231.0 Max. :6300 Max. :2009 #&gt; NA&#39;s :2 NA&#39;s :2 We want to remove the rows of penguins where either body_mass_g or bill_length_mm have NA values. We do that below using the na.omit function (selecting only the relevant variables) and assign the cleaned object the name penguins_clean. # remove rows of penguins where bill_length_mm or body_mass_g have NA values penguins_clean &lt;- na.omit(penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)]) We extract the bill_length_mm variable from the penguins data frame and assign it the name y since it will be the response variable. We extract the body_mass_g variable from the penguins data frame and assign it the name y since it will be the predictor variable. We also determine the number of observations and assign that value the name n. # extract response and predictor from penguins_clean y &lt;- penguins_clean$bill_length_mm x &lt;- penguins_clean$body_mass_g # determine number of observations n &lt;- length(y) We now compute \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\). Note that placing () around the assignment operations will both perform the assign and print the results. # compute OLS estimates of beta1 and beta0 (b1 &lt;- (sum(x * y) - sum(x) * sum(y) / n)/(sum(x^2) - sum(x)^2/n)) #&gt; [1] 0.004051417 (b0 &lt;- mean(y) - b1 * mean(x)) #&gt; [1] 26.89887 The estimated value of \\(\\beta_0\\) is \\(\\hat{\\beta}_0=26.90\\) and the estimated value of \\(\\beta_1\\) is \\(\\hat{\\beta}_1=0.004\\). The basic mathematical interpretation of our results is that: (\\(\\hat{\\beta}_1\\)): If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguins bill length to be 0.004 millimeters longer. (\\(\\hat{\\beta}_0\\)):A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters. The latter interpretation is clearly non-sensical and is caused by the fact that we are extrapolating far outside the observed body mass values. The relationship between body mass and bill length is different for penguin chicks versus adults. We can use the abline function to overlay the fitted model on the observed data. Note that in simple linear regression, \\(\\hat{\\beta}_1\\) corresponds to the slope of the fitted line and \\(\\hat{\\beta}_0\\) will be the intercept. plot(bill_length_mm ~ body_mass_g, data = penguins, ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;, main = &quot;Penguin size measurements&quot;) # a is the intercept and b is the slope abline(a = b0, b = b1) The fit of the model to our observed data seems reasonable. We can also compute the residuals, \\(\\hat{\\epsilon}_1,\\ldots,\\hat{\\epsilon}_n\\), the fitted values \\(\\hat{y}_1,\\ldots,\\hat{y}_n\\), and the associated RSS, \\(RSS=\\sum_{i=1}^n \\hat{\\epsilon}_i^2\\). yhat &lt;- b0 + b1 * x # compute fitted values ehat &lt;- y - yhat # compute residuals (rss &lt;- sum(ehat^2)) # sum of the squared residuals #&gt; [1] 6564.494 (sigmasqhat &lt;- rss/(n-2)) # estimated error variance #&gt; [1] 19.30734 5.5 Estimation of the multiple linear regression coefficients) We now consider the context where we want to estimate the parameters of a linear model with 1 or more regressors, i.e., when \\[Y=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon.\\] The multiple linear regression model relating the responses, the regressors, and the errors for all \\(n\\) observations is defined by the system of equations \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n. \\tag{5.10} \\end{equation}\\] 5.5.1 Using matrix notation to represent a linear model To simplify estimation of the regression coefficients in a linear regression model, we must use matrix notation to describe the system of equation defining our linear model. We define the following notation: \\(\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]\\) denotes the column vector containing the \\(n\\) responses. \\(\\mathbf{X}\\) denotes the matrix containing a column of 1s and the observed regressor values, specifically, \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,p-1} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,p-1} \\end{bmatrix}.\\] \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]\\) denotes the column vector containing the \\(p\\) regression coefficients. \\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]\\) denotes the column vector contained the \\(n\\) errors. Then the system of equations defining the linear model in (5.10) can be written as \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\] Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model. 5.5.2 Residuals, fitted values, and RSS for multiple linear regression We proceed with discussion of residuals, fitted values, and RSS for the multiple linear regression context using matrix notation. The vector of estimated values for the coefficients contained in \\(\\boldsymbol{\\beta}\\) is denoted by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}=[\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_{p-1}]. \\tag{5.11} \\end{equation}\\] The vector of regressor values for the \\(i\\)th observation is denoted by \\[\\begin{equation} \\mathbf{x}_i=[1,x_{i,1},\\ldots,x_{i,p-1}], \\tag{5.12} \\end{equation}\\] where the 1 is needed to account for the intercept in our model. Extending the original definition of a fitted value in Equation (5.3), the \\(i\\)th fitted value in the context of multiple linear regression is defined as \\[\\begin{align} \\hat{Y}_i &amp;= \\hat{E}(Y|\\mathbb{X} = \\mathbf{x}_i) \\notag \\\\ &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i,1} + \\cdots + \\hat{\\beta}_{p-1} x_{i,p-1} \\notag \\\\ &amp;= \\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}}. \\tag{5.13} \\end{align}\\] The (column) vector of fitted values is defined as \\[\\begin{equation} \\hat{\\mathbf{y}} = [\\hat{Y}_1,\\ldots,\\hat{Y}_n]. \\tag{5.14} \\end{equation}\\] Extending the original definition of a residual in Equation (5.4), the \\(i\\)th residual in the context of multiple linear regression can be written as \\[\\begin{align} \\hat{\\epsilon}_i = Y_i - \\hat{Y}_i=Y_i-\\mathbf{x}_i^T\\hat{\\boldsymbol{\\beta}}, \\end{align}\\] using Equation (5.13). The RSS for a simple linear regression model, as a function of the estimated regression coefficients, is \\[\\begin{align*} RSS(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp;= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - \\hat{E}(Y|X=x_i))^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2. \\end{align*}\\] 5.5.3 OLS estimator of the linear model parameters The OLS estimator of the regression coefficient vector, \\(\\boldsymbol{\\beta}\\), is \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}, \\tag{5.15} \\end{equation}\\] 5.6 Penguins multiple linear regression example The data set includes 344 observations of 8 variables. The variables are: species: a factor indicating the penguin species island: a factor indicating the island the penguin was observed bill_length_mm: a numeric variable indicating the bill length in millimeters bill_depth_mm: a numeric variable indicating the bill depth in millimeters flipper_length_mm: an integer variable indicating the flipper length in millimeters body_mass_g: an integer variable indicating the body mass in grams sex: a factor indicating the penguin sex (female, male) year: an integer denoting the study year the penguin was observed (2007, 2008, or 2009) mlmod &lt;- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, data = penguins) summary(mlmod) #&gt; #&gt; Call: #&gt; lm(formula = bill_length_mm ~ body_mass_g + flipper_length_mm, #&gt; data = penguins) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.8064 -2.5898 -0.7053 1.9911 18.8288 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -3.4366939 4.5805532 -0.750 0.454 #&gt; body_mass_g 0.0006622 0.0005672 1.168 0.244 #&gt; flipper_length_mm 0.2218655 0.0323484 6.859 3.31e-11 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.124 on 339 degrees of freedom #&gt; (2 observations deleted due to missingness) #&gt; Multiple R-squared: 0.4329, Adjusted R-squared: 0.4295 #&gt; F-statistic: 129.4 on 2 and 339 DF, p-value: &lt; 2.2e-16 5.7 Categorical predictors 5.8 Penguins multiple linear regression example with categorical predictor library(ggplot2) ggplot(data = penguins) + geom_point(aes(x = body_mass_g, y = bill_length_mm, col = species)) #&gt; Warning: Removed 2 rows containing missing values (geom_point). lmodb &lt;- lm(bill_length_mm ~ body_mass_g + species + body_mass_g:species, data = penguins) summary(lmodb) #&gt; #&gt; Call: #&gt; lm(formula = bill_length_mm ~ body_mass_g + species + body_mass_g:species, #&gt; data = penguins) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -6.4208 -1.6461 0.0919 1.4718 9.3138 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 26.9941391 1.5926015 16.950 &lt; 2e-16 *** #&gt; body_mass_g 0.0031879 0.0004271 7.464 7.27e-13 *** #&gt; speciesChinstrap 5.1800537 3.2746719 1.582 0.115 #&gt; speciesGentoo -0.2545907 2.7138655 -0.094 0.925 #&gt; body_mass_g:speciesChinstrap 0.0012748 0.0008740 1.459 0.146 #&gt; body_mass_g:speciesGentoo 0.0009030 0.0006066 1.489 0.138 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 2.399 on 336 degrees of freedom #&gt; (2 observations deleted due to missingness) #&gt; Multiple R-squared: 0.8098, Adjusted R-squared: 0.807 #&gt; F-statistic: 286.1 on 5 and 336 DF, p-value: &lt; 2.2e-16 lmodc &lt;- lm(bill_length_mm ~ body_mass_g*species, data = penguins) coefficients(lmodb) #&gt; (Intercept) body_mass_g #&gt; 26.9941391367 0.0031878758 #&gt; speciesChinstrap speciesGentoo #&gt; 5.1800537287 -0.2545906615 #&gt; body_mass_g:speciesChinstrap body_mass_g:speciesGentoo #&gt; 0.0012748183 0.0009029956 coefficients(lmodc) #&gt; (Intercept) body_mass_g #&gt; 26.9941391367 0.0031878758 #&gt; speciesChinstrap speciesGentoo #&gt; 5.1800537287 -0.2545906615 #&gt; body_mass_g:speciesChinstrap body_mass_g:speciesGentoo #&gt; 0.0012748183 0.0009029956 5.9 Evaluating model fit 5.10 Summary of notation 5.11 Summary of functions used in this chapter 5.12 Summarizing the components of a linear model We have already introduced a lot of objects. To aid in making sense of their notation, their purpose in the model, whether they can be observed, and whether they are modeled as a random variable (vector) or fixed, non-random values, we summarize things below. Weve already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable. On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation (5.10) is for the errors to be random. We summarize this information in the table below for the objects previously discussed using the various notations introduced. Notation Description Observable Random \\(Y\\) response variable Yes Yes \\(Y_i\\) response value for the \\(i\\)th observation Yes Yes \\(\\mathbf{y}\\) the \\(n\\times 1\\) column vector of response values Yes Yes \\(X\\) regressor variable Yes No \\(X_j\\) the \\(j\\)th regressor variable Yes No \\(x_{i,j}\\) the value of the \\(j\\)th regressor variable for the \\(i\\)th observation Yes No \\(\\mathbf{X}\\) the \\(n\\times p\\) matrix of regressor values Yes No \\(\\beta_j\\) the regression coefficient associated with the \\(j\\)th regressor variable No No $\\boldsymbol{ a}$| the $p column vector of regression coefficients 1$ | No | No \\(\\epsilon\\) the error No Yes \\(\\epsilon_i\\) the error associated with observation \\(i\\) No Yes 5.13 Going Deeper 5.13.1 Manually estimating the simple linear regression coefficients In this section we will manually perform the Penguins simple linear regression analysis provided in Section @ref{s:penguins-slr}. Recall that the penguins data frame in the **palmerpenguins* package contained the variables: bill_length_mm: a numeric variable indicating the bill length in millimeters body_mass_g: an integer variable indicating the body mass in grams We will use formulas for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) in Equations @ref\\tag{5.7} and @ref\\tag{5.8} to manually compute the estimated regression coefficients for the simple linear regression model when regressing bill_length_mm on body_mass_g. We first load the penguins data frame. We then select the bill_length_mm and body_mass_g variables of the data frame and use the pipe operator to pass the simplified data frame to the summary function. data(penguins, package = &quot;palmerpenguins&quot;) # load data penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)] |&gt; summary() # simplify and summarize #&gt; bill_length_mm body_mass_g #&gt; Min. :32.10 Min. :2700 #&gt; 1st Qu.:39.23 1st Qu.:3550 #&gt; Median :44.45 Median :4050 #&gt; Mean :43.92 Mean :4202 #&gt; 3rd Qu.:48.50 3rd Qu.:4750 #&gt; Max. :59.60 Max. :6300 #&gt; NA&#39;s :2 NA&#39;s :2 Both bill_length_mm and body_mass_g have NA values that will poison our calculations if we naively use those variables in our calculations, so we must remove them prior to calculation. The na.omit function attempts to handle missing values in R objects. The penguins data frame has class data.frame. For a data.frame, na.omit will remove any rows that have NA values. Note that if you are only concerned with the NA values for certain variables of a data frame then you should apply the na.omit to the data frame containing only those variables, otherwise na.omit may remove rows unnecessarily because a different variable has an NA value. Compare the dimensions of the results when we apply na.omit to penguins versus penguins with only bill_length_mm and body_mass_g. penguins |&gt; na.omit() |&gt; dim() # dimensions penguins after filtering rows with NA #&gt; [1] 333 8 penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)] |&gt; na.omit() |&gt; dim() #&gt; [1] 342 2 The data frame obtained after applying na.omit to the penguins data frame has only 333 rows, while the data frame obtained by applying na.omit to only the bill_length_mm and body_mass_g columns of penguins has 342 rows. This is because some of the other variables in penguins have NA values in rows that the bill_length_mm and body_mass_g columns do not, so the na.omit function must remove additional rows. Continuing our analysis, we create a new data frame, penguins_clean, that is obtained by selecting the bill_length_mm and body_mass_g variables of penguins and then using the na.omit function to retain only the rows without NA values. # remove rows of penguins where bill_length_mm or body_mass_g have NA values penguins_clean &lt;- na.omit(penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)]) We extract the bill_length_mm variable from the penguins_clean data frame and assign it the name y since it will be the response variable. We extract the body_mass_g variable from the penguins data frame and assign it the name x since it will be the regressor variable. We also determine the number of observations and assign that value the name n. # extract response and regressor from penguins_clean y &lt;- penguins_clean$bill_length_mm x &lt;- penguins_clean$body_mass_g # determine number of observations n &lt;- length(y) We now compute \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) using the formulas in Equations @ref\\tag{5.7} and @ref\\tag{5.8}, respectively. Note that placing () around the assignment operations will both perform the assignment and print the results. # compute OLS estimates of beta1 and beta0 (b1 &lt;- (sum(x * y) - sum(x) * sum(y) / n)/(sum(x^2) - sum(x)^2/n)) #&gt; [1] 0.004051417 (b0 &lt;- mean(y) - b1 * mean(x)) #&gt; [1] 26.89887 The estimated value of \\(\\beta_0\\) is $_0=$0 and the estimated value of \\(\\beta_1\\) is \\(\\hat{\\beta}_1=0.004\\). The basic mathematical interpretation of our results is that: \\(\\hat{\\beta}_1\\): If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguins bill length to be 0.004 millimeters longer. \\(\\hat{\\beta}_0\\):A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters. The latter interpretation is clearly non-sensical and is caused by the fact that we are extrapolating far outside the observed body mass values. The relationship between body mass and bill length is different for penguin chicks versus adults. We can use the abline function to overlay the fitted model on the observed data. Note that in simple linear regression, \\(\\hat{\\beta}_1\\) corresponds to the slope of the fitted line and \\(\\hat{\\beta}_0\\) will be the intercept. plot(bill_length_mm ~ body_mass_g, data = penguins, ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;, main = &quot;Penguin size measurements&quot;) # a is the intercept and b is the slope abline(a = b0, b = b1) The fit of the model to our observed data seems reasonable. We can also compute the residuals, \\(\\hat{\\epsilon}_1,\\ldots,\\hat{\\epsilon}_n\\), the fitted values \\(\\hat{y}_1,\\ldots,\\hat{y}_n\\), and the associated RSS, \\(RSS=\\sum_{i=1}^n \\hat{\\epsilon}_i^2\\). yhat &lt;- b0 + b1 * x # compute fitted values ehat &lt;- y - yhat # compute residuals (rss &lt;- sum(ehat^2)) # sum of the squared residuals #&gt; [1] 6564.494 (sigmasqhat &lt;- rss/(n-2)) # estimated error variance #&gt; [1] 19.30734 5.13.2 Manually estimating the multiple linear regression coefficients 5.13.3 Parameter estimation and matrix decompositions 5.13.4 Updating a model 5.13.5 More discussion of formula for model-building The models fit by, e.g., the lm and glm functions are specified in a compact symbolic form. The ~ operator is basic in the formation of such models. An expression of the form y ~ model is interpreted as a specification that the response y is modelled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. The terms themselves consist of variable and factor names separated by : operators. Such a term is interpreted as the interaction of all the variables and factors appearing in the term. In addition to + and :, a number of other operators are useful in model formulae. The * operator denotes factor crossing: ab interpreted as a+b+a:b. The ^ operator indicates crossing to the specified degree. For example (a+b+c)^2 is identical to (a+b+c)(a+b+c) which in turn expands to a formula containing the main effects for a, b and c together with their second-order interactions. The %in% operator indicates that the terms on its left are nested within those on the right. For example a + b %in% a expands to the formula a + a:b. The - operator removes the specified terms, so that (a+b+c)^2 - a:b is identical to a + b + c + b:c + a:c.It can also used to remove the intercept term: when fitting a linear model y ~ x - 1 specifies a line through the origin. A model with no intercept can be also specified as y ~ x + 0 or y ~ 0 + x. While formulae usually involve just variable and factor names, they can also involve arithmetic expressions. The formula log(y) ~ a + log(x) is quite legal. When such arithmetic expressions involve operators which are also used symbolically in model formulae, there can be confusion between arithmetic and symbolic operator use. To avoid this confusion, the function I() can be used to bracket those portions of a model formula where the operators are used in their arithmetic sense. For example, in the formula y ~ a + I(b+c), the term b+c is to be interpreted as the sum of b and c. Variable names can be quoted by backticks like this in formulae, although there is no guarantee that all code using formulae will accept such non-syntactic names. Most model-fitting functions accept formulae with right-hand-side including the function offset to indicate terms with a fixed coefficient of one. Some functions accept other specials such as strata or cluster (see the specials argument of terms.formula). There are two special interpretations of . in a formula. The usual one is in the context of a data argument of model fitting functions and means all columns not otherwise in the formula: see terms.formula. In the context of update.formula, only, it means what was previously in this part of the formula. When formula is called on a fitted model object, either a specific method is used (such as that for class nls) or the default method. The default first looks for a formula component of the object (and evaluates it), then a terms component, then a formula parameter of the call (and evaluates its value) and finally a formula attribute. There is a formula method for data frames. When theres terms attribute with a formula, e.g., for a model.frame(), that formula is returned. If youd like the previous (R &lt;= 3.5.x) behavior, use the auxiliary DF2formula() which does not consider a terms attribute. Otherwise, if there is only one column this forms the RHS with an empty LHS. For more columns, the first column is the LHS of the formula and the remaining columns separated by + form the RHS. References "],["interpreting-a-fitted-linear-model.html", "Chapter 6 Interpreting a fitted linear model 6.1 Orthogonality 6.2 Example: Fuel Consumption Data 6.3 Setting Up a Linear Model 6.4 Interpreting the Coefficients 6.5 Example: Berkeley Guidance Study 6.6 Dictionary of Data 6.7 Analysing Relations Between Regressors 6.8 Adjusting the Regressors 6.9 Comparing Different Models 6.10 Effect Plots 6.11 Regressors on Logarithmic Scale 6.12 Below we create an effects plot on a natural log scale. 6.13 Below we create an effects plot on a log10 scale. 6.14 Below we create an effects plot on a regular scale. 6.15 Interpreting Coefficients with Log Scale on Regressor 6.16 Log-Level Interpretation 6.17 Log-log Interpretation 6.18 Summary of Interpretations (Simple Linear Regression) 6.19 More Practice", " Chapter 6 Interpreting a fitted linear model 6.1 Orthogonality Let \\[\\mathbf{x}_j=(x_{1,j},\\ldots,x_{n,j})^T\\] denote the \\(n\\times 1\\) column vector of observed values for regressor \\(X_j\\). Regressors, \\(\\mathbf{x}_j\\) and \\(\\mathbf{x}_k\\) are orthogonal if \\(\\mathbf{x}_j^T \\mathbf{x}_k=0\\). Let \\(\\boldsymbol{1}_{n\\times1}\\) denote an \\(n\\times 1\\) column vector of 1s. The definition of orthogonal vectors above implies that \\(\\mathbf{x}_j\\) is orthogonal to \\(\\boldsymbol{1}_{n\\times1}\\) if \\[ \\mathbf{x}_j^T \\boldsymbol{1}_{n\\times1} = \\sum_{i=1}^n x_{i,j} = 0,\\] i.e., if the values in \\(\\mathbf{x}_j\\) sum to zero. Let \\(\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{i,j}\\) denote the sample mean of \\(\\mathbf{x}_j\\) and \\(\\bar{\\mathbf{x}}_j = \\bar{x}_j \\boldsymbol{1}_{n\\times 1}\\) denote the column vector that repeats \\(\\bar{x}_j\\) \\(n\\) times. Centering \\(\\mathbf{x}_j\\) involves subtracting the sample mean of \\(\\mathbf{x}_j\\) from \\(\\mathbf{x}_j\\), i.e., \\(\\mathbf{x}_j - \\bar{\\mathbf{x}}_j\\). Regressors \\(\\mathbf{x}_j\\) and \\(\\mathbf{x}_k\\) are uncorrelated if they are orthogonal after being centered, i.e., if \\[(\\mathbf{x}_j - \\bar{\\mathbf{x}}_j)^T (\\mathbf{x}_k - \\bar{\\mathbf{x}}_k).\\] Note that the sample covariance between vectors \\(\\mathbf{x}_j\\) and \\(\\mathbf{x}_k\\) is \\[\\begin{align*} \\widehat{\\mathrm{cov}}(\\mathbf{x}_j, \\mathbf{x}_k) &amp;= \\frac{1}{n}\\sum_{i=1}^n (x_i,j - \\bar{x}_j)(x_i,k - \\bar{x}_k) \\\\ &amp;= \\frac{1}{n-1}(\\mathbf{x}_j - \\bar{\\mathbf{x}}_j)^T (\\mathbf{x}_k - \\bar{\\mathbf{x}}_k). \\end{align*}\\] Thus, two centered regressors are orthogonal if their covariance is zero. It is a desirable to have orthogonal regressors in your fitted model because they simplify estimating the relationship between the regressors and the response. Specifically: If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from your model will not impact the estimated regression coefficients of the other regressors. Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s. We consider a simple example to demonstrate how orthogonality of regressors impacts the estimated regression coefficients. In the code below, we define vectors y, x1, and x2. y &lt;- c(1, 4, 6, 8, 9) # create an arbitrary response vector x1 &lt;- c(7, 5, 5, 7, 7) # create regressor 1 x2 &lt;- c(-1, 2, -3, 1, 5/7) # create regressor 2 to be orthogonal to x1 Regressors x1 and x2 are orthogonal since their crossproduct \\(\\mathbf{x}_1^T \\mathbf{x}_2\\) (in R, crossprod(x1, x2)) equals zero. crossprod(x1, x2) # crossproduct is zero so x1 and x2 are orthogonal #&gt; [,1] #&gt; [1,] 0 In the code below, we regress y on x1 without an intercept (lmod1). The estimated coefficient for x1 is \\(\\hat{\\beta}_1=0.893\\). Next, we then regress y on x1 and x2 without an intercept (lmod2). The estimated coefficients for x1 and x2 are \\(\\hat{\\beta}_1=0.893\\) and \\(\\hat{\\beta}_2=0.221\\), respectively. Because x1 and x2 are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for x1 stays the same in both models. lmod1 &lt;- lm(y ~ x1 - 1) coef(lmod1) #&gt; x1 #&gt; 0.893401 lmod2 &lt;- lm(y ~ x1 + x2 - 1) coef(lmod2) #&gt; x1 x2 #&gt; 0.8934010 0.2210526 The previous models (lmod1 and lmod2) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our \\(\\mathbf{X}\\) matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s. However, neither x1 nor x2 is orthogonal with the column of ones. We define the vector ones below, which is a column of 1s, and compute the crossproduct between ones and the two regressors. Since the crossproducts are not zero, x1 and x2 are not orthogonal to the column of ones. ones &lt;- rep(1, 5) crossprod(ones, x1) # not zero #&gt; [,1] #&gt; [1,] 31 crossprod(ones, x2) # not zero #&gt; [,1] #&gt; [1,] -0.2857143 If we add the column of ones to lmod2 (i.e., if we include the intercept in the model), then the coefficients for both x1 and x2 change because these regressors are not orthogonal to the column of 1s, as verified by the R output below. Comparing lmod2 and lmod3, \\(\\hat{\\beta}_1\\) changes from \\(0.893\\) to \\(0.397\\) and \\(\\hat{\\beta}_2\\) changes from \\(0.221\\) to \\(0.279\\). lmod3 &lt;- lm(y ~ x1 + x2) coef(lmod3) #&gt; (Intercept) x1 x2 #&gt; 3.1547101 0.3969746 0.2791657 For orthogonality of our regressors to be most impactful, the models regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesnt impact the estimated coefficients of the other regressors. In the code below, we define centered regressors x3 and x4 to be uncorrelated, i.e., x3 and x4 have sample mean zero and are orthogonal to each other. x3 &lt;- c(0, -1, 1, 0, 0) # sample mean is zero x4 &lt;- c(0, 0, 0, 1, -1) # sample mean is zero cov(x3, x4) # 0, so x3 and x4 are uncorrelated #&gt; [1] 0 If we fit a linear regression model with any combination of ones, x3, or x4 as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below. coef(lm(y ~ 1)) # only column of 1s coef(lm(y ~ x3 - 1)) # only x3 coef(lm(y ~ x4 - 1)) # only x4 coef(lm(y ~ x3)) # 1s and x3 coef(lm(y ~ x4)) # 1s and x4 coef(lm(y ~ x3 + x4 - 1)) # x3 and x4 coef(lm(y ~ x3 + x4)) # 1s, x3, and x4 We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesnt impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were \\(\\hat{\\beta}_{int}=5.6\\), \\(\\hat{\\beta}_{3}=1.0\\), \\(\\hat{\\beta}_{4}=-0.5\\) when the relevant regressor was included in the model. The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the crossproduct of the \\(\\mathbf{X}\\) matrix for the largest set of regressors you are considering. Consider the matrix of crossproducts for the columns of 1s, x1, x2, x3, and x4. crossprod(model.matrix(~ x1 + x2 + x3 + x4)) #&gt; (Intercept) x1 x2 x3 x4 #&gt; (Intercept) 5.0000000 31 -0.2857143 0 0.0000000 #&gt; x1 31.0000000 197 0.0000000 0 0.0000000 #&gt; x2 -0.2857143 0 15.5102041 -5 0.2857143 #&gt; x3 0.0000000 0 -5.0000000 2 0.0000000 #&gt; x4 0.0000000 0 0.2857143 0 2.0000000 Consider the sequence of models below. coef(lm(y ~ 1)) #&gt; (Intercept) #&gt; 5.6 The model with only an intercept has an estimated coefficient of \\(\\hat{\\beta}_{int}=5.6\\). If we add the x1 to the model with an intercept, then both coefficients change because they are not orthogonal to each other. lmod4 &lt;- lm(y ~ x1) # model with 1s and x1 coef(lmod4) #&gt; (Intercept) x1 #&gt; 2.5 0.5 If we add x2 to lmod4, we might think that only \\(\\hat{\\beta}_{int}\\) will change because x1 and x2 are orthogonal to each other. However, because x2 is not orthogonal to all of the other regressors in the model (x1 and the column of 1s), both \\(\\hat{\\beta}_{int}\\) and \\(\\hat{\\beta}_1\\) will change. The easiest way to realize this is to look at lmod2 above with only x1 and x2. When we add the column of 1s to lmod2, both \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term. coef(lm(y ~ x1 + x2)) #&gt; (Intercept) x1 x2 #&gt; 3.1547101 0.3969746 0.2791657 However, note that x3 is orthogonal to the column of 1s and x1. Thus, if we add x3 to lmod4, which includes both a column of 1s and x1, x3 will not change the estimated coefficients for the intercept or x1. coef(lm(y ~ x1 + x3)) #&gt; (Intercept) x1 x3 #&gt; 2.5 0.5 1.0 Additionally, since x4 is orthogonal to the column of 1s, x1, and x3, adding x4 to the previous model will not change the estimated coefficients for any of the other variables already in the model. coef(lm(y ~ x1 + x3 + x4)) #&gt; (Intercept) x1 x3 x4 #&gt; 2.5 0.5 1.0 -0.5 Lastly, if we can partition our \\(\\mathbf{X}\\) matrix usch that \\(\\mathbf{X}^T \\mathbf{X}\\) is a block diagonal matrix, then the none of the blocks of variables will affect the estimated coefficients of the other variables. Define a new regressor x5 below. x5 is orthogonal to the column of 1s and x1, but not x4. x5 &lt;- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4 crossprod(cbind(1, x1, x4, x5)) #&gt; x1 x4 x5 #&gt; 5 31 0 0 #&gt; x1 31 197 0 0 #&gt; x4 0 0 2 -1 #&gt; x5 0 0 -1 2 This means that if we fit the model with only the column of 1s and the intercept, the model only with x4 and x5, and then fit the model with the column of 1s, x1, x4, and x5, then the coefficients \\(\\hat{\\beta}_{int}\\) and \\(\\hat{\\beta}_{1}\\) are not impacted when x4 and x5 are added to the model. Similarly, \\(\\hat{\\beta}_{4}\\) and \\(\\hat{\\beta}_{5}\\) are not impacted when the column of 1s and x1 are added to the model with x4 and x5. See the output below. lm(y ~ x1) # model with 1s and x1 #&gt; #&gt; Call: #&gt; lm(formula = y ~ x1) #&gt; #&gt; Coefficients: #&gt; (Intercept) x1 #&gt; 2.5 0.5 lm(y ~ x4 + x5 - 1) # model with x4 and x5 only #&gt; #&gt; Call: #&gt; lm(formula = y ~ x4 + x5 - 1) #&gt; #&gt; Coefficients: #&gt; x4 x5 #&gt; -3 -5 lm(y ~ x1 + x4 + x5) # model with 1s, x1, x4, x5 #&gt; #&gt; Call: #&gt; lm(formula = y ~ x1 + x4 + x5) #&gt; #&gt; Coefficients: #&gt; (Intercept) x1 x4 x5 #&gt; 2.5 0.5 -3.0 -5.0 6.2 Example: Fuel Consumption Data data(fuel2001, package = &quot;alr4&quot;) The variables (for the year 2001 unless otherwise noted) are: Drivers: Number of Licensed drivers in the state FuelC: Gasoline sold for road use (1000s of gal.) Miles: Miles of Federal-aid highway miles in the state Pop: 2001 population age 16 and over Tax: Gasoline state tax rate (cents/gallon) 6.3 Setting Up a Linear Model What is the relationship between fuel consumption and various regressors for the 50 United States and the District of Columbia? 6.3.1 Adjusting Units Some of the variables are adjusted for population. Others are are not. Some dollar values are given in thousands of dollars. Others are given in dollars. Some units of fuel are given in gallons. Others are given in 1000s of gallons. Our model should have regressor variables with compatible units with the response variable. Create a new variable called Fuel that converts units of FuelC from 1000s of gallons to gallons per person. Create a new variable called Income1kthat converts the units of Income from dollars per capita to 1000s of dollars per capita. Convert the units of Drivers from number of drivers to number of drivers per capita. Fuel: Average amount of gasoline sold for road use per person (Gallons/person) Income1K: Average personal income (in thousands) for the year 2000 per person ($1K/person) Dlic: Number of licensed drivers per 1000 persons (licensed drivers/1K persons) # create new regressors/transformed responses to fuel2001 data frame fuel2001$Fuel &lt;- 1000*fuel2001$FuelC/fuel2001$Pop fuel2001$Dlic &lt;- 1000*fuel2001$Drivers/fuel2001$Pop fuel2001$Income1K &lt;- fuel2001$Income/1000 summary(fuel2001) #&gt; Drivers FuelC Income Miles #&gt; Min. : 328094 Min. : 148769 Min. :20993 Min. : 1534 #&gt; 1st Qu.: 1087128 1st Qu.: 737361 1st Qu.:25323 1st Qu.: 36586 #&gt; Median : 2718209 Median : 2048664 Median :27871 Median : 78914 #&gt; Mean : 3750504 Mean : 2542786 Mean :28404 Mean : 77419 #&gt; 3rd Qu.: 4424256 3rd Qu.: 3039932 3rd Qu.:31209 3rd Qu.:112828 #&gt; Max. :21623793 Max. :14691753 Max. :40640 Max. :300767 #&gt; MPC Pop Tax Fuel #&gt; Min. : 6556 Min. : 381882 Min. : 7.50 Min. :317.5 #&gt; 1st Qu.: 9391 1st Qu.: 1162624 1st Qu.:18.00 1st Qu.:575.0 #&gt; Median :10458 Median : 3115130 Median :20.00 Median :626.0 #&gt; Mean :10448 Mean : 4257046 Mean :20.15 Mean :613.1 #&gt; 3rd Qu.:11311 3rd Qu.: 4845200 3rd Qu.:23.25 3rd Qu.:666.6 #&gt; Max. :17495 Max. :25599275 Max. :29.00 Max. :842.8 #&gt; Dlic Income1K #&gt; Min. : 700.2 Min. :20.99 #&gt; 1st Qu.: 864.1 1st Qu.:25.32 #&gt; Median : 909.1 Median :27.87 #&gt; Mean : 903.7 Mean :28.40 #&gt; 3rd Qu.: 943.0 3rd Qu.:31.21 #&gt; Max. :1075.3 Max. :40.64 6.3.2 Fitting a Model We will set up a regression model to determine how Fuel (gallons per person) is related to Tax (cents per gallon), Dlic (drivers per capita), Income1k (thousands of dollars of income per capita), and Miles (federal highway miles). \\[E( \\mbox{Fuel} \\ | \\ \\mbox{Tax, Dlic, Income1K, Miles})=\\beta_0+\\beta_1 (\\mbox{Tax}) +\\beta_2 (\\mbox{Dlic}) + \\beta_3 (\\mbox{Income1K}) + \\beta_4 \\log{(\\mbox{Miles})}\\] # fit model lmod &lt;- lm(Fuel ~ Tax + Dlic + Income1K + log(Miles), data = fuel2001) # summarize model faraway::sumary(lmod) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 154.19284 194.90616 0.7911 0.4329381 #&gt; Tax -4.22798 2.03012 -2.0826 0.0428733 #&gt; Dlic 0.47187 0.12851 3.6718 0.0006256 #&gt; Income1K -6.13533 2.19363 -2.7969 0.0075078 #&gt; log(Miles) 26.75518 9.33737 2.8654 0.0062592 #&gt; #&gt; n = 51, p = 5, Residual SE = 64.89122, R-Squared = 0.51 We see the fitted model is: \\[\\widehat{\\mbox{E}}(\\mbox{Fuel} \\ | \\ \\mbox{Tax, Dlic, Income1k, Miles}) = 154.19 - 4.24 (\\mbox{Tax}) + 0.47 (\\mbox{Dlic}) - 6.14(\\mbox{Income1K}) + 26.76 \\log{(\\mbox{26.76})}\\] This equation represents the estimated conditional mean of Fuel given fixed values of the regressors Tax, Dlic, Income1K, and Miles. 6.4 Interpreting the Coefficients Estimated coefficients are usually interpreted as a rate of change. If we increase a regressor by 1 unit (and hold all others constant), what is the predicted change in the response variable? Interpret the practical meaning of \\(\\beta_1 = -4.24\\). Pay attention to units when giving your interpretation. The sign of a parameter estimate indicates the direction of the relationship between the regressor and the response (when all other regressors are constant). The sign of the effect of a regressor is often more important than its magnitude. If regressors are highly correlated with other regressors, both the magnitude and sign of an estimated coefficient may change depending on the values of the other regressors are in the model. 6.5 Example: Berkeley Guidance Study Data from the Berkeley guidance study of children born in 1928-29 in Berkeley, CA. BGSgirls contains data from just the girls in the study. data(BGSgirls, package = &quot;alr4&quot;) head(BGSgirls) #&gt; WT2 HT2 WT9 HT9 LG9 ST9 WT18 HT18 LG18 ST18 BMI18 Soma #&gt; 67 13.6 87.7 32.5 133.4 28.4 74 56.9 158.9 34.6 143 22.5 5.0 #&gt; 68 11.3 90.0 27.8 134.8 26.9 65 49.9 166.0 33.8 117 18.1 4.0 #&gt; 69 17.0 89.6 44.4 141.5 31.9 104 55.3 162.2 35.1 143 21.0 5.5 #&gt; 70 13.2 90.3 40.5 137.1 31.8 79 65.9 167.8 39.3 148 23.4 5.5 #&gt; 71 13.3 89.4 29.9 136.1 27.7 83 62.3 170.9 36.3 152 21.3 4.5 #&gt; 72 11.3 85.5 22.8 130.6 23.4 60 47.4 164.9 31.8 126 17.4 3.0 6.6 Dictionary of Data BMI18: the body mass index at age 18 WT2, WT9, and WT18: the weights at ages 2, 9, and 18 (in kg) for the \\(n=70\\) girls in the study. 6.7 Analysing Relations Between Regressors # basic scatterplot matrix pairs(~ BMI18 + WT2 + WT9 + WT18, data = BGSgirls) Based on the scatter plot matrix above, does there seem to be any relations among the regressors? Explain why these relations make practical sense. How can we adjust our model to account for the relations between the regressors? 6.8 Adjusting the Regressors Create a new regressor called DW9 that is the weight gain from age 2 to 9. Create a new regressor called DW18 that is the weight gain from age 9 to 18. BGSgirls$DW9 &lt;- BGSgirls$WT9-BGSgirls$WT2 BGSgirls$DW18 &lt;- BGSgirls$WT18-BGSgirls$WT9 BGSgirls$DW218 &lt;- BGSgirls$WT18-BGSgirls$WT2 # basic scatterplot matrix pairs(~ BMI18 + DW9 + DW18, data = BGSgirls) Based on the scatter plot matrix above, how can you tell that the new regressors seem to be more independent from each other? 6.9 Comparing Different Models 6.9.1 BMI relation to WT2, WT9 and WT18 m1 &lt;- lm(BMI18 ~ WT2 + WT9 + WT18, BGSgirls) faraway::sumary(m1) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.309780 1.655175 5.0205 4.156e-06 #&gt; WT2 -0.386633 0.151451 -2.5528 0.0130 #&gt; WT9 0.031410 0.049370 0.6362 0.5268 #&gt; WT18 0.287447 0.026026 11.0444 &lt; 2.2e-16 #&gt; #&gt; n = 70, p = 4, Residual SE = 1.33291, R-Squared = 0.78 6.9.2 BMI relation to WT2, DW9 and DW18 m2 &lt;- lm(BMI18 ~ WT2 + DW9 + DW18, BGSgirls) faraway::sumary(m2) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.309780 1.655175 5.0205 4.156e-06 #&gt; WT2 -0.067776 0.127509 -0.5315 0.5968 #&gt; DW9 0.318857 0.038553 8.2706 8.683e-12 #&gt; DW18 0.287447 0.026026 11.0444 &lt; 2.2e-16 #&gt; #&gt; n = 70, p = 4, Residual SE = 1.33291, R-Squared = 0.78 6.9.3 BMI relation to WT2, WT9, WT18, DW9 and DW18 m3 &lt;- lm(BMI18 ~ WT2 + WT9 + WT18 + DW9 + DW18, BGSgirls) faraway::sumary(m3) #&gt; #&gt; Coefficients: (2 not defined because of singularities) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 8.309780 1.655175 5.0205 4.156e-06 #&gt; WT2 -0.386633 0.151451 -2.5528 0.0130 #&gt; WT9 0.031410 0.049370 0.6362 0.5268 #&gt; WT18 0.287447 0.026026 11.0444 &lt; 2.2e-16 #&gt; #&gt; n = 70, p = 4, Residual SE = 1.33291, R-Squared = 0.78 coef(m1) #&gt; (Intercept) WT2 WT9 WT18 #&gt; 8.30977995 -0.38663273 0.03140967 0.28744733 coef(m2) #&gt; (Intercept) WT2 DW9 DW18 #&gt; 8.30977995 -0.06777573 0.31885700 0.28744733 coef(m3) #&gt; (Intercept) WT2 WT9 WT18 DW9 DW18 #&gt; 8.30977995 -0.38663273 0.03140967 0.28744733 NA NA Regressor Model 1 Model 2 Model 3 Intercept 8.298 8.298 8.298 CI (5.00,11.62) (5.00,11.62) (5.00,11.62) WT2 -0.383 -0.065 -0.383 CI (-0.69,-0.08) (-0.32,0.19) (-0.69,-0.08) WT9 0.032  0.032 CI (-0.06,0.13)  (-0.06,0.13) WT18 0.287   CI (0.23,0.34)   DW9  0.318 NA CI  (0.24,0.40) NA DW18  0.287 NA CI  (0.23,0.34) NA Comment on how the WT2 coefficient is the same/different in the different models. When regressors are correlated, interpretation of the effect of a regressor depends not only on the other regressors in the model, but also upon the linear transformation of the variables used. Why are their NAs in Model 3? 6.10 Effect Plots An effect plot displays effect of a regressor on the mean response while holding the other regressors at their mean values. \\[ \\hat{y} = \\beta_0 + \\beta_1 (\\bar{X}_1) + \\beta_2 (\\bar{X}_2) + \\ldots + \\beta_{i-1} (\\bar{X}_{i-1}) + \\beta_i (X_i) + \\beta_{i+1} (\\bar{X}_{i+1}) + \\ldots + + \\beta_{p-1} (\\bar{X}_{p-1})\\] summary(lmod)$coefficients #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 154.1928446 194.9061606 0.7911132 0.4329381433 #&gt; Tax -4.2279832 2.0301211 -2.0826261 0.0428733310 #&gt; Dlic 0.4718712 0.1285134 3.6717660 0.0006255639 #&gt; Income1K -6.1353310 2.1936336 -2.7968805 0.0075077902 #&gt; log(Miles) 26.7551756 9.3373740 2.8653854 0.0062591801 Complete the code below to extract each of the coefficients in the Fuel Consumption model lmod from the coefficient array above. b0 &lt;- summary(lmod)$coefficients[1] #beta_0 b1 &lt;- summary(lmod)$coefficients[2] #beta_1 b2 &lt;- summary(lmod)$coefficients[3] #beta_2 b3 &lt;- summary(lmod)$coefficients[4] #beta_3 b4 &lt;- summary(lmod)$coefficients[5] #beta_4 Complete the R code below to compute the sample means for each of the regressors. xbar.Tax &lt;- mean(fuel2001$Tax) xbar.Dlic &lt;- mean(fuel2001$Dlic) xbar.Income1K &lt;- mean(fuel2001$Income1K) xbar.Miles &lt;- mean(fuel2001$Miles) What is the effect of Tax on expected Fuel consumption when the other regressors are fixed at the sample mean values? Write a formula to express this relation. Thus we have the model \\[\\mbox{E}(\\mbox{Fuel} \\ | \\ \\mbox{Tax, Dlic=??, Income1K=??, log(Miles) = ??}) = ?? - ??(\\mbox{Tax})\\] library(effects) # for Effect function #&gt; Loading required package: carData #&gt; lattice theme set by effectsTheme() #&gt; See ?effectsTheme for details. # effect plot for Tax regressor plot(Effect(&quot;Tax&quot;, lmod)) If instead of fixing the values of the regressors at their mean, we choose other values such as the minimum value of each of regressors. What effect (if any) would this have on the graph above? 6.11 Regressors on Logarithmic Scale Logarithms are commonly used both for the response and for regressors. summary(fuel2001) #&gt; Drivers FuelC Income Miles #&gt; Min. : 328094 Min. : 148769 Min. :20993 Min. : 1534 #&gt; 1st Qu.: 1087128 1st Qu.: 737361 1st Qu.:25323 1st Qu.: 36586 #&gt; Median : 2718209 Median : 2048664 Median :27871 Median : 78914 #&gt; Mean : 3750504 Mean : 2542786 Mean :28404 Mean : 77419 #&gt; 3rd Qu.: 4424256 3rd Qu.: 3039932 3rd Qu.:31209 3rd Qu.:112828 #&gt; Max. :21623793 Max. :14691753 Max. :40640 Max. :300767 #&gt; MPC Pop Tax Fuel #&gt; Min. : 6556 Min. : 381882 Min. : 7.50 Min. :317.5 #&gt; 1st Qu.: 9391 1st Qu.: 1162624 1st Qu.:18.00 1st Qu.:575.0 #&gt; Median :10458 Median : 3115130 Median :20.00 Median :626.0 #&gt; Mean :10448 Mean : 4257046 Mean :20.15 Mean :613.1 #&gt; 3rd Qu.:11311 3rd Qu.: 4845200 3rd Qu.:23.25 3rd Qu.:666.6 #&gt; Max. :17495 Max. :25599275 Max. :29.00 Max. :842.8 #&gt; Dlic Income1K #&gt; Min. : 700.2 Min. :20.99 #&gt; 1st Qu.: 864.1 1st Qu.:25.32 #&gt; Median : 909.1 Median :27.87 #&gt; Mean : 903.7 Mean :28.40 #&gt; 3rd Qu.: 943.0 3rd Qu.:31.21 #&gt; Max. :1075.3 Max. :40.64 Based on the summary output above, why do you think we used a log scale on Miles? In the code block below, we create new variables that are the natural log and log base 10 of Miles and recreate the linear model using each of these new variables. fuel2001$LnMiles &lt;- log(fuel2001$Miles) fuel2001$LogMiles &lt;- log10(fuel2001$Miles) lmod.ln &lt;- lm(Fuel ~ Tax + Dlic + Income1K + LnMiles, data = fuel2001) lmod.log &lt;- lm(Fuel ~ Tax + Dlic + Income1K + LogMiles, data = fuel2001) faraway::sumary(lmod.ln) #Check that model is the same #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 154.19284 194.90616 0.7911 0.4329381 #&gt; Tax -4.22798 2.03012 -2.0826 0.0428733 #&gt; Dlic 0.47187 0.12851 3.6718 0.0006256 #&gt; Income1K -6.13533 2.19363 -2.7969 0.0075078 #&gt; LnMiles 26.75518 9.33737 2.8654 0.0062592 #&gt; #&gt; n = 51, p = 5, Residual SE = 64.89122, R-Squared = 0.51 faraway::sumary(lmod.log) #Check that model is the same #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 154.19284 194.90616 0.7911 0.4329381 #&gt; Tax -4.22798 2.03012 -2.0826 0.0428733 #&gt; Dlic 0.47187 0.12851 3.6718 0.0006256 #&gt; Income1K -6.13533 2.19363 -2.7969 0.0075078 #&gt; LogMiles 61.60607 21.50010 2.8654 0.0062592 #&gt; #&gt; n = 51, p = 5, Residual SE = 64.89122, R-Squared = 0.51 6.12 Below we create an effects plot on a natural log scale. plot(Effect(&quot;LnMiles&quot;, lmod.ln), main = &quot;ln(Miles) effect plot&quot;) 6.13 Below we create an effects plot on a log10 scale. plot(Effect(&quot;LogMiles&quot;, lmod.log), main = &quot;log(Miles) effect plot&quot;) 6.14 Below we create an effects plot on a regular scale. plot(Effect(&quot;Miles&quot;, lmod, xlevels = list(Miles = seq(1, 3e5, len = 301)))) The effect of increasing Miles is greater in states with fewer miles of roadway, with relatively little change in states with the most roadway. This is the usual effect of logarithms: the fitted effect changes most rapidly when the regressor is small and less rapidly when the predictor is large. 6.15 Interpreting Coefficients with Log Scale on Regressor 6.15.1 Natural Log Scale Regressor \\(X_j\\) increasing by 1% while the other regressors remain constant is associated with a \\(\\beta_j/100\\) increase in the response variable, on average. Interpret the meaning of the coefficient associated to the natural log of Miles which you can find below. summary(lmod.ln)$coefficients[5, 1] #ln coeff #&gt; [1] 26.75518 6.15.2 Common Log (base 10) Scale Regressor \\(X_j\\) increasing by a factor of 10 (an increase of 900%) while the other regressors remain constant is associated with a \\(\\beta_j\\) increase in the response variable, on average. Interpret the meaning of the coefficient associated to the log base 10 of Miles which you can find below. summary(lmod.log)$coefficients[5, 1] #log coeff #&gt; [1] 61.60607 6.16 Log-Level Interpretation It is common for responses to be transformed to a logarithmic scale for theoretical or practical considerations. \\[\\mbox{E}( \\log{Y} \\ | \\ X)= \\beta_0 + \\beta_1 X.\\] This is sometimes called a log-level model. A unit increase in \\(X\\) is associated with a change in the mean \\(Y\\) by the multiplicative effect \\(\\exp^{\\beta_1}\\). Thus \\(beta_1\\) is the continuous exponential growth/decay rate. It is often acceptable to approximate the expected value of a log by the log of the expected value: \\[\\log{( \\mbox{E}(Y \\ | \\ X=x) )} \\approx\\mbox{E}(\\log{Y} \\ | \\ X=x)\\] Thus, we have \\[\\mbox{E}(Y \\ | \\ X=x) \\approx e^{\\rm{E}(\\log{Y} \\ | \\ X=x)} = e^{\\beta_0 + \\beta_1 X}=e^{\\beta_0}e^{\\beta_1X}.\\] 6.17 Log-log Interpretation Consider the log-log simple linear regression model \\[\\mbox{E}( \\log{Y} \\ | \\ X) = \\beta_0 + \\beta_1 \\log{X}.\\] When we scale \\(X\\) by a factor of \\(c\\), the response is predicted to grow by a factor of \\(c^{\\beta_1}\\), on average. 6.18 Summary of Interpretations (Simple Linear Regression) Level-level: \\(\\mathbf{\\mbox{E}(Y \\ | \\ X=x)} = \\boldsymbol\\beta_0 + \\boldsymbol\\beta_1 X\\): The predicted change in the response is \\(\\beta_1\\) when we increase \\(X\\) by 1 unit, on average. Level-log: \\(\\mathbf{\\mbox{E}(Y \\ | \\ X=x)} = \\boldsymbol\\beta_0 + \\boldsymbol\\beta_1 \\log{X}\\): When we increase \\(X\\) by 1%, the response is predicted to increase by \\(\\beta_j/100\\), on average. Log-level: \\(\\mathbf{\\mbox{E}(\\log{Y} \\ | \\ X=x) = \\boldsymbol\\beta_0 + \\boldsymbol\\beta_1 X}\\): A unit increase in \\(X\\) is predicted to change the response by a factor of \\(e^{\\beta_1}\\), on average. The continuous growth rate is \\(\\beta_1\\). Log-log: \\(\\mathbf{\\mbox{E}(\\log{Y} \\ | \\ X=x) = \\boldsymbol\\beta_0 + \\boldsymbol\\beta_1 \\log{X}}\\): When we scale \\(X\\) by a factor of \\(c\\), the response is predicted to grow by a factor of \\(c^{\\beta_1}\\), on average. 6.19 More Practice For a log-level model, interpret the relationship between \\(X\\) and the mean of \\(Y\\) when \\(X\\) increases by 1 unit and \\(\\beta_j=0.3\\) and the other predictors do not change. For a log-log model, what is the expected change in \\(Y\\) if we multiply \\(X\\) by a factor of \\(c\\). For a log-log model, interpret the relationship between \\(X\\) and the expected value of \\(Y\\) when \\(X\\) increases by \\(10\\%\\) and \\(\\beta_1=0.3\\). "],["linear-model-inference.html", "Chapter 7 Linear model inference", " Chapter 7 Linear model inference With our regression model, we also hope to be able to: Generalize our results from the sample to the a larger population of interest. E.g., we want to extend our results from a small set of college students to all college students. Infer causality between our regressors and the response. E.g., if a person receives the measles vaccine, then this causes a reduction in the persons risk of catching measles. Results from a sample of observations typically only generalize to a larger population when the sample is a random sample from a larger population. Some examples of random sampling methods include simple random sampling, stratified random sampling, cluster random sampling, and systematic random sampling. Most inferential methods assume the \\(n\\) observations are a simple random sample from a larger population. Simple random sampling requires that each sample of size \\(n\\) is equally likely to occur. Causal inference can be made when the data are a "],["prediction.html", "Chapter 8 Prediction", " Chapter 8 Prediction "],["assumptions-stated-and-prioritized.html", "Chapter 9 Assumptions Stated and Prioritized 9.1 Standard assumptions concisely stated 9.2 Standard assumptions prioritized", " Chapter 9 Assumptions Stated and Prioritized 9.1 Standard assumptions concisely stated There are several standard assumptions made when performing linear regression. The ones necessary to derive the theoretical properties of the estimators, confidence intervals, prediction intervals, etc., are: The structure of the model is correct: \\[\\begin{equation*} E(\\mathbf{y}|\\mathbf{X})=\\mathbf{X}\\boldsymbol{\\beta}. \\end{equation*}\\] This is equivalent to saying the model is correctly specified. This means there are no missing or extra regressors in our regression equation. The errors, conditional on the regressor values, are i.i.d. Normal with mean 0 and constant variance \\(\\sigma^2\\). This may be succinctly stated at \\[\\begin{equation*} \\mathbf{y}\\mid \\mathbf{X} \\sim N(\\boldsymbol{0}_{n\\times 1}, \\sigma^2 \\mathbf{I}_n). \\end{equation*}\\] Note that this is actually 4 assumptions: \\(E(\\epsilon \\mid \\mathbb{X})=0\\), i.e., the mean of the errors is zero regardless of the values of the regressors. Note that this is effectively the same as the structure assumption in 1. \\(\\mathrm{var}(\\epsilon \\mid \\mathbb{X})=0\\), i.e., the variance of the errors is constant (\\(\\sigma^2\\)) regardless of the values of the regressors. Any two errors are uncorrelated, regardless of the values of their regressors, as long as they are not the same error, i.e., \\(\\mathrm{cov}(\\epsilon_i,\\epsilon_j\\mid \\mathbb{X})=0\\) for all \\(i\\neq j\\). Each of the errors, regardless of the regressor values, are normally distributed. There are additional assumption needed for the practical fitting of the linear regression model. These assumptions are often not explicitly stated. They are: The columns of \\(\\mathbf{X}\\) are linearly independent, i.e., none of the regressors are linear combinations of each other. This assumption is checking by assessing whether collinearity is present. This assumption is critical for ensuring that our model is identifiable (estimatable). No observations are substantially more influential than other observations in determining the fit of the model to the observed data. Influential observations can make it difficult to determine whether Assumptions 1 and 2 are satisfied. 9.2 Standard assumptions prioritized We assume that any issues with collinearity and identifiability (Assumption 3) have already been addressed. The assumptions previously stated are not equally important. The order of importance is The structure of the model is correct (Assumption 1). If the structure of your model is incorrect, then no conclusions drawn from our model are trustworthy. Its possible that our conclusions arent correct, but we have no confidence of that if the structure of our model is wrong. No points are overly influential in determining the model fit (Assumption 4). An overly influential observation can make it seem like the model is correctly specified when it is not. It can also impact assessing the other assumptions below. The errors have constant variance (Assumption 2). If this assumption isnt satisfied, then standard confidence intervals for the regression coefficients and mean function and prediction intervals for new observations are not trustworthy. The errors are uncorrelated (Assumption 2). If this assumption isnt satisfied, then standard confidence intervals for the regression coefficients and mean function and prediction intervals for new observations are not trustworthy. The errors are normally distributed (Assumption 2). This is the least important assumption. If the previous assumptions are satisfied, then our OLS estimator of \\(\\boldsymbol{\\beta}\\) is still the best linear unbiased estimator regardless of the normality of the errors. If the sample size is large enough, the central limit theorem tells us that our confidence intervals for the regression coefficients and the mean function are still approximately valid. However, if our sample size is small or we are interested in constructing a prediction interval, then non-normal errors can lead to untrustworth confidence and prediction intervals. title: Joshua French date: 2022-06-13 output: word_document: default html_document: df_print: paged "],["basic-regression-diagnostics.html", "Chapter 10 Basic regression diagnostics", " Chapter 10 Basic regression diagnostics "],["assessing-and-addressing-collinearity.html", "Chapter 11 Assessing and addressing collinearity 11.1 Loading the Data 11.2 Question 1: Why might it be a bad a idea to simply include all regressors? 11.3 Question 2: What metrics/methods have we used to compare models thus far?", " Chapter 11 Assessing and addressing collinearity title: Variable Selection Part 1 date: 2022-06-13 output: html_document: df_print: paged # Model Selection: What is the right number of regressors we should include? Variable selection is intended to (objectively) find the best subset of predictors. So why not throw the whole kitchen sink into our model? # Motivating Example: Predicting Life Expectancy The state datasets in the base R package (no package needed to access the data) contains various data sets with data from all 50 states. Well be working with the dataset state.x77 which has 50 observations (one for each state) with the following variables of interest: Population: Population estimate as of July 1, 1975 Income: Per capita income (1974) Illiteracy: Illiteracy rate (1970, percent of population) Life Exp: life expectancy in years (196971) Murder: murder and non-negligent manslaughter rate per 100,000 population (1976) HS Grad: percent high-school graduates (1970) Frost: mean number of days with minimum temperature below freezing (19311960) in capital or large city Area: land area in square miles 11.1 Loading the Data data(state) statedata &lt;- data.frame(state.x77, row.names = state.abb) #head(statedata) summary(statedata) #&gt; Population Income Illiteracy Life.Exp #&gt; Min. : 365 Min. :3098 Min. :0.500 Min. :67.96 #&gt; 1st Qu.: 1080 1st Qu.:3993 1st Qu.:0.625 1st Qu.:70.12 #&gt; Median : 2838 Median :4519 Median :0.950 Median :70.67 #&gt; Mean : 4246 Mean :4436 Mean :1.170 Mean :70.88 #&gt; 3rd Qu.: 4968 3rd Qu.:4814 3rd Qu.:1.575 3rd Qu.:71.89 #&gt; Max. :21198 Max. :6315 Max. :2.800 Max. :73.60 #&gt; Murder HS.Grad Frost Area #&gt; Min. : 1.400 Min. :37.80 Min. : 0.00 Min. : 1049 #&gt; 1st Qu.: 4.350 1st Qu.:48.05 1st Qu.: 66.25 1st Qu.: 36985 #&gt; Median : 6.850 Median :53.25 Median :114.50 Median : 54277 #&gt; Mean : 7.378 Mean :53.11 Mean :104.46 Mean : 70736 #&gt; 3rd Qu.:10.675 3rd Qu.:59.15 3rd Qu.:139.75 3rd Qu.: 81163 #&gt; Max. :15.100 Max. :67.30 Max. :188.00 Max. :566432 Which regressors should we include if we want to predict Life Exp as the response variable? 11.2 Question 1: Why might it be a bad a idea to simply include all regressors? 11.3 Question 2: What metrics/methods have we used to compare models thus far? "],["variable-selection.html", "Chapter 12 Variable Selection 12.1 Testing-Based Procedures 12.2 Backward elimination 12.3 Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision? 12.4 Forward Selection 12.5 Stepwise Regression 12.6 Model Hierarchy 12.7 Criterion-Based Procedures 12.8 Akaikes Information Criterion (AIC) 12.9 Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination? 12.10 Bayesian Information Criterion (BIC) 12.11 Question 6: Interpret the output from the BIC plot above. What is the best model according to this metric? 12.12 Model Selection: What is the right number of regressors we should include? 12.13 Motivating Example: Predicting Life Expectancy 12.14 Loading the Data 12.15 Recap from Last Class 12.16 Testing Based Procedures 12.17 Search Strategies 12.18 Review of Criterion-Based Procedures Thus Far 12.19 Akaikes Information Criterion (AIC) and Bayesian Information Criteria (BIC) 12.20 Adjusted \\(R^2\\) 12.21 Mean Square Error (MSE) 12.22 Mallows \\(C_p\\) Statistic 12.23 Cross-validation 12.24 Example of \\(k\\)-fold Crossvalidation 12.25 Theres Still More to Consider! 12.26 Summary 12.27 Exercise", " Chapter 12 Variable Selection There are two aspects to variable selection: The strategy used to search for the optimal model. The criterion used to compare models. 12.1 Testing-Based Procedures 12.2 Backward elimination Backward elimination is the simplest of all variable selection procedures. We start with all predictors and remove the least significant predictor. Stop once all the noise has been removed. 12.3 Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision? lmod &lt;- lm(Life.Exp ~ ., data = statedata) faraway::sumary(lmod) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.0943e+01 1.7480e+00 40.5859 &lt; 2.2e-16 #&gt; Population 5.1800e-05 2.9187e-05 1.7748 0.08318 #&gt; Income -2.1804e-05 2.4443e-04 -0.0892 0.92934 #&gt; Illiteracy 3.3820e-02 3.6628e-01 0.0923 0.92687 #&gt; Murder -3.0112e-01 4.6621e-02 -6.4590 8.68e-08 #&gt; HS.Grad 4.8929e-02 2.3323e-02 2.0979 0.04197 #&gt; Frost -5.7350e-03 3.1432e-03 -1.8246 0.07519 #&gt; Area -7.3832e-08 1.6682e-06 -0.0443 0.96491 #&gt; #&gt; n = 50, p = 8, Residual SE = 0.74478, R-Squared = 0.74 back1 &lt;- update(lmod, . ~ . - Area) faraway::sumary(back1) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.0989e+01 1.3875e+00 51.1652 &lt; 2.2e-16 #&gt; Population 5.1883e-05 2.8788e-05 1.8023 0.07852 #&gt; Income -2.4440e-05 2.3429e-04 -0.1043 0.91740 #&gt; Illiteracy 2.8459e-02 3.4163e-01 0.0833 0.93400 #&gt; Murder -3.0182e-01 4.3344e-02 -6.9634 1.454e-08 #&gt; HS.Grad 4.8472e-02 2.0667e-02 2.3454 0.02369 #&gt; Frost -5.7758e-03 2.9702e-03 -1.9446 0.05839 #&gt; #&gt; n = 50, p = 7, Residual SE = 0.73608, R-Squared = 0.74 This does not imply the other variables are not related to the response. lmod1 &lt;- lm(Life.Exp ~ Illiteracy, data = statedata) faraway::sumary(lmod1) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 72.39495 0.33834 213.9734 &lt; 2.2e-16 #&gt; Illiteracy -1.29602 0.25701 -5.0427 6.969e-06 #&gt; #&gt; n = 50, p = 2, Residual SE = 1.09659, R-Squared = 0.35 12.4 Forward Selection Forward selection starts with the null model (only an intercept), and adds regressors one at a time until we can no longer improve the error criterion by adding a single regressor. For example, first add the predictor with the smallest \\(p\\)-value. Then compare models with the first predictor plus a second predictor and add the predictor which has the smallest \\(p\\)-value. 12.5 Stepwise Regression Stepwise regression is a combination of backward elimination and forward selection. This addresses the situation where variables are added or removed early in the process and we want to change our mind. Stepwise selection can miss the optimal model because we do not consider all possible models due to the one-at-a-time nature of adding/removing regressors. \\(p\\)-values should not be taken as very accurate in stepwise searches because we are bound to see small \\(p\\)-values due to chance alone. Stepwise selection tends to produce simpler models that are not necessarily the best for prediction. 12.6 Model Hierarchy We must respect hierarchy in models when it is naturally present. In polynomial models, \\(X^2\\) is a higher order term than \\(X\\). A lower order term should be retained if a higher order term is retained to increase the flexibility. The model \\(Y=\\beta_0+\\beta_2 X^2+\\epsilon\\), the maximum/minimum value MUST occur \\(x=0\\) For the model \\(y=\\beta_0+\\beta_1 X+\\beta_2 X^2+\\epsilon\\), the maximum/minimum value can occur anywhere along the real line (depending on what the data suggest). If we fit the model \\(y=\\beta_0+\\beta_1 X+\\beta_2 X^2+\\epsilon\\) and \\(\\beta_1\\) is not significant, it would NOT make sense to remove \\(X\\) from the model but still keep \\(X^2\\). 12.7 Criterion-Based Procedures Akaikes Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are two information-based criteria for variable selection. 12.8 Akaikes Information Criterion (AIC) \\(\\mbox{AIC}(\\mathcal{M})=-2L(\\mathcal{M})+2p_{\\mathcal{M}}\\), where \\(\\mathcal{M}\\) is the model, \\(L(\\mathcal{M})\\) is the log-likelihood of the model using the MLE estimates of the parameters, and \\(p_{\\mathcal{M}}\\) is the number of regression coefficients in model \\(\\mathcal{M}\\). For linear regression models, \\(-2L(\\mathcal{M})=n\\log{(\\mbox{RSS}_{\\mathcal{M}}/n)} + c\\), where \\(c\\) is a constant that depends only on the observed data and not on the model, and \\(\\mbox{RSS}_{\\mathcal{M}}\\) is the RSS of model \\(\\mathcal{M}\\). The constant \\(c\\) is the same for a given data set, so they can be ignored when comparing models that based on the same data set. 12.8.1 Interpreting AIC The formula for AIC is derived from a metric that can be used to measure how far a model is from the true model. As \\(\\mbox{RSS}_{\\mathcal{M}}\\) gets smaller (better fit), \\(n\\log{(\\mbox{RSS}_{\\mathcal{M}}/n)}\\) gets smaller (becomes more negative). Adding more predictors (that are not collinear) will improve the fit. As \\(p_{\\mathcal{M}}\\) gets bigger, the second term of AIC gets larger. The second component penalizes the model according its complexity. The more parameters, the larger the penalty. Models with more parameters will fit better (reducing the RSS), but will be penalized more for having additional parameters. AIC provides a balance between fit and simplicity. AIC identifies good fitting models (small RSS) that are simple (not a lot of predictors). We choose the model the minimizes the AIC. 12.8.2 Exhaustive Model Searches The leaps package searches all possible combinations of predictors. For each value of \\(p\\) (number of predictors), it finds the variables that give the minimum RSS. For each value of \\(p\\), the model that minimizes the RSS will have the smallest AIC, BIC, adjusted \\(R_a^2\\), and Mallows \\(C_p\\) (well discuss these soon). By default, regsubsets only goes up to \\(p=9\\). You have to set nvmax = j, where \\(j\\) is the number of regressors you want to consider. # may need to install.package the first time library(leaps) # you need to load package every time you want to use it # model selection by exhaustive search b &lt;- regsubsets(Life.Exp ~ ., data = statedata) rs &lt;- summary(b) # summarize model that minimizes RSS for each p rs$which # nicer output #&gt; (Intercept) Population Income Illiteracy Murder HS.Grad Frost Area #&gt; 1 TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; 2 TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE #&gt; 3 TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE #&gt; 4 TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE #&gt; 5 TRUE TRUE TRUE FALSE TRUE TRUE TRUE FALSE #&gt; 6 TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE #&gt; 7 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE 12.9 Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination? 12.9.1 Computing the AIC # What output is stored after running regsubsets summary(rs) #&gt; Length Class Mode #&gt; which 56 -none- logical #&gt; rsq 7 -none- numeric #&gt; rss 7 -none- numeric #&gt; adjr2 7 -none- numeric #&gt; cp 7 -none- numeric #&gt; bic 7 -none- numeric #&gt; outmat 49 -none- character #&gt; obj 28 regsubsets list n &lt;- nrow(statedata) #number observation n=50 rss &lt;- rs$rss # rss calculated for each model # Compute AIC using the formula AIC &lt;- n * log(rss/n) + (2:8)*2 # we start at 2 since include intercept in all plot(AIC ~ I(1:7), ylab = &quot;AIC&quot;, xlab = &quot;Number of Predictors&quot;, pch = 16) 12.9.2 Question 5: Interpret the output from the AIC plots above. What is the best model according to this metric? 12.10 Bayesian Information Criterion (BIC) The Bayesian Information Criterion (BIC) is another criteria that is often and is almost the same as AIC. \\[BIC(\\mathcal{M})=-2L(\\mathcal{M})+\\log{(n)}p_{\\mathcal{M}}.\\] (BIC &lt;- rs$bic) # Exactly values from rs summary #&gt; [1] -39.22051 -42.62472 -46.70678 -47.03640 -43.13738 -39.23342 -35.32373 #(BIC2 &lt;- n * log(rss/n) + log(n)* (2:8)) # Using the formula #BIC - BIC2 # The two differ by a constant plot(BIC ~ I(1:7), ylab = &quot;BIC&quot;, xlab = &quot;Number of Predictors&quot;, pch = 16) 12.11 Question 6: Interpret the output from the BIC plot above. What is the best model according to this metric? The car package has a subsets function that takes the generates nice, labeled BIC plots generated from the regsubsets function. library(car) #&gt; #&gt; Attaching package: &#39;car&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; recode subsets(b, statistic = &quot;bic&quot;, legend = FALSE) #&gt; Abbreviation #&gt; Population P #&gt; Income In #&gt; Illiteracy Il #&gt; Murder M #&gt; HS.Grad H #&gt; Frost F #&gt; Area A # Appendix ## Maximum Likelihood Estimates (MLE) The likelihood function \\(L(\\theta)= L( \\theta \\mid x_1, x_2, \\ldots x_n)\\) gives the likelihood of the parameter \\(\\theta\\) given the observed data. A maximum likelihood estimate (MLE), \\(\\mathbf{\\hat{\\theta}_{\\rm MLE}}\\),}} is a value of \\(\\theta\\) that maximizes the likelihood function. MLE is a process for finding the best parameter(s) for a model based on a given dataset Let \\(f(x; \\theta)\\) denote the pdf of a random variable \\(X\\) with associated parameter \\(\\theta\\). Suppose \\(X_1, X_2, \\ldots , X_n\\) are random samples from this distribution, and \\(x_1, x_2, \\ldots , x_n\\) are the corresponding observed values. \\[ L(\\theta \\mid x_1, x_2, \\ldots , x_n) = f(x_1; \\theta) f(x_2; \\theta) \\ldots f(x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta).\\] ### Example: Finding and MLE Find the MLE for \\(\\lambda\\) where \\(x_1, x_2, \\ldots , x_n\\) comes from \\(X \\sim \\mbox{Exp}(\\lambda)\\) with \\(f(x; \\lambda) = \\lambda e^{-\\lambda x}\\). Find a formula for the likelihood function. \\[ L(\\lambda \\mid x_1, x_2, \\ldots , x_n) = \\left(\\lambda e^{-\\lambda x_1} \\right)\\left(\\lambda e^{-\\lambda x_2} \\right) \\ldots \\left(\\lambda e^{-\\lambda x_n} \\right) = \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i} .\\] Optimize the likelihood function. Find the value of \\(\\lambda\\) that makes the observed data most likely to occur. \\[\\frac{d}{d \\lambda} \\left( \\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i} \\right) \\] Often this is really messy to solve. Taking the natural log of both sides often simplifies the calculation. The log-likelihood function is \\(y = \\ln{L(\\theta \\mid x_1, x_2, \\ldots , x_n) }\\) (often written with \\(\\log\\) though we mean \\(\\ln\\)). Since the natural log is an increasing function, the value of \\(\\theta\\) that maximizes (or minimizes) \\(L(\\theta \\mid x_1, x_2, \\ldots , x_n)\\) is the same value of \\(\\theta\\) that maximizes (or minimizes) \\(y = \\ln{L(\\theta \\mid x_1, x_2, \\ldots , x_n) }\\). \\[y = \\ln{\\left(\\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}\\right)} = n \\ln{(\\lambda)}- \\lambda \\sum_{i=1}^n x_i\\] It actually is easier to optimize the log-likelihood function in this case: \\[ \\begin{aligned} \\frac{d}{d \\lambda} \\ln{\\left(\\lambda^n e^{-\\lambda \\sum_{i=1}^n x_i}\\right)} &amp;= \\frac{d}{d \\lambda} \\left( n \\ln{(\\lambda)}- \\lambda \\sum_{i=1}^n x_i \\right) \\\\ &amp;= \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i \\end{aligned} \\] We have a critical value at \\(\\lambda = \\frac{\\sum x_i}{n} = \\bar{x}\\) which is the value of \\(\\lambda\\) that maximizes the likelihood function. If we assume the sample was randomly selected from an exponential distribution, then given the observed data, the most likely value for \\(\\lambda\\) is \\(\\bar{x}\\). This makes practical sense since if \\(X \\sim \\mbox{Exp}(\\lambda)\\), \\(E(X) = \\mu = \\lambda\\). 12.11.1 Pros of Using MLEs to Estimate Population Parameters MLEs give estimates that make practical sense (see example above). Consistency: As the sample size gets larger and larger, MLEs converge to the actual value of the parameter. Normality: As we get more data, MLEs converge to a normal distribution. Efficiency: They have the smallest possible variance for a consistent estimator. 12.12 Model Selection: What is the right number of regressors we should include? Variable selection is intended to (objectively) find the best subset of predictors. So why not throw the whole kitchen sink into our model? 12.13 Motivating Example: Predicting Life Expectancy The state datasets in the base R package (no package needed to access the data) contains various data sets with data from all 50 states. Well be working with the dataset state.x77 which has 50 observations (one for each state) with the following variables of interest: Population: Population estimate as of July 1, 1975 Income: Per capita income (1974) Illiteracy: Illiteracy rate (1970, percent of population) Life Exp: life expectancy in years (196971) Murder: murder and non-negligent manslaughter rate per 100,000 population (1976) HS Grad: percent high-school graduates (1970) Frost: mean number of days with minimum temperature below freezing (19311960) in capital or large city Area: land area in square miles 12.14 Loading the Data data(state) statedata &lt;- data.frame(state.x77, row.names = state.abb) #head(statedata) summary(statedata) #&gt; Population Income Illiteracy Life.Exp #&gt; Min. : 365 Min. :3098 Min. :0.500 Min. :67.96 #&gt; 1st Qu.: 1080 1st Qu.:3993 1st Qu.:0.625 1st Qu.:70.12 #&gt; Median : 2838 Median :4519 Median :0.950 Median :70.67 #&gt; Mean : 4246 Mean :4436 Mean :1.170 Mean :70.88 #&gt; 3rd Qu.: 4968 3rd Qu.:4814 3rd Qu.:1.575 3rd Qu.:71.89 #&gt; Max. :21198 Max. :6315 Max. :2.800 Max. :73.60 #&gt; Murder HS.Grad Frost Area #&gt; Min. : 1.400 Min. :37.80 Min. : 0.00 Min. : 1049 #&gt; 1st Qu.: 4.350 1st Qu.:48.05 1st Qu.: 66.25 1st Qu.: 36985 #&gt; Median : 6.850 Median :53.25 Median :114.50 Median : 54277 #&gt; Mean : 7.378 Mean :53.11 Mean :104.46 Mean : 70736 #&gt; 3rd Qu.:10.675 3rd Qu.:59.15 3rd Qu.:139.75 3rd Qu.: 81163 #&gt; Max. :15.100 Max. :67.30 Max. :188.00 Max. :566432 lmod &lt;- lm(Life.Exp ~ ., data = statedata) faraway::sumary(lmod) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 7.0943e+01 1.7480e+00 40.5859 &lt; 2.2e-16 #&gt; Population 5.1800e-05 2.9187e-05 1.7748 0.08318 #&gt; Income -2.1804e-05 2.4443e-04 -0.0892 0.92934 #&gt; Illiteracy 3.3820e-02 3.6628e-01 0.0923 0.92687 #&gt; Murder -3.0112e-01 4.6621e-02 -6.4590 8.68e-08 #&gt; HS.Grad 4.8929e-02 2.3323e-02 2.0979 0.04197 #&gt; Frost -5.7350e-03 3.1432e-03 -1.8246 0.07519 #&gt; Area -7.3832e-08 1.6682e-06 -0.0443 0.96491 #&gt; #&gt; n = 50, p = 8, Residual SE = 0.74478, R-Squared = 0.74 12.15 Recap from Last Class Choosing more variables is not always preferable. A solid yet simple model is often preferred. When deciding how many predictors to include (exclude), its complicated! We should consider several criteria. 12.16 Testing Based Procedures Backward elimination is the simplest of all variable selection procedures. We start with all predictors and remove the least significant predictor. Stop once all the noise has been removed. Forward selection starts with the null model (only an intercept), and adds regressors one at a time until we can no longer improve the error criterion by adding a single regressor. Stepwise regression is a combination of backward elimination and forward selection. # This handy function does stepwise regression # Evaluation based on AIC step(lmod, direction = &quot;both&quot;) #&gt; Start: AIC=-22.18 #&gt; Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + #&gt; Frost + Area #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - Area 1 0.0011 23.298 -24.182 #&gt; - Income 1 0.0044 23.302 -24.175 #&gt; - Illiteracy 1 0.0047 23.302 -24.174 #&gt; &lt;none&gt; 23.297 -22.185 #&gt; - Population 1 1.7472 25.044 -20.569 #&gt; - Frost 1 1.8466 25.144 -20.371 #&gt; - HS.Grad 1 2.4413 25.738 -19.202 #&gt; - Murder 1 23.1411 46.438 10.305 #&gt; #&gt; Step: AIC=-24.18 #&gt; Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + #&gt; Frost #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - Illiteracy 1 0.0038 23.302 -26.174 #&gt; - Income 1 0.0059 23.304 -26.170 #&gt; &lt;none&gt; 23.298 -24.182 #&gt; - Population 1 1.7599 25.058 -22.541 #&gt; + Area 1 0.0011 23.297 -22.185 #&gt; - Frost 1 2.0488 25.347 -21.968 #&gt; - HS.Grad 1 2.9804 26.279 -20.163 #&gt; - Murder 1 26.2721 49.570 11.569 #&gt; #&gt; Step: AIC=-26.17 #&gt; Life.Exp ~ Population + Income + Murder + HS.Grad + Frost #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - Income 1 0.006 23.308 -28.161 #&gt; &lt;none&gt; 23.302 -26.174 #&gt; - Population 1 1.887 25.189 -24.280 #&gt; + Illiteracy 1 0.004 23.298 -24.182 #&gt; + Area 1 0.000 23.302 -24.174 #&gt; - Frost 1 3.037 26.339 -22.048 #&gt; - HS.Grad 1 3.495 26.797 -21.187 #&gt; - Murder 1 34.739 58.041 17.456 #&gt; #&gt; Step: AIC=-28.16 #&gt; Life.Exp ~ Population + Murder + HS.Grad + Frost #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 23.308 -28.161 #&gt; + Income 1 0.006 23.302 -26.174 #&gt; + Illiteracy 1 0.004 23.304 -26.170 #&gt; + Area 1 0.001 23.307 -26.163 #&gt; - Population 1 2.064 25.372 -25.920 #&gt; - Frost 1 3.122 26.430 -23.877 #&gt; - HS.Grad 1 5.112 28.420 -20.246 #&gt; - Murder 1 34.816 58.124 15.528 #&gt; #&gt; Call: #&gt; lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost, #&gt; data = statedata) #&gt; #&gt; Coefficients: #&gt; (Intercept) Population Murder HS.Grad Frost #&gt; 7.103e+01 5.014e-05 -3.001e-01 4.658e-02 -5.943e-03 12.17 Search Strategies An exhaustive search looks at all possible models using all available regressors. This is not feasible unless the number of regressors is relatively small. If the number of regressors (including the intercept) is \\(p\\), there are \\(2^p\\) possible models. Because of our error criteria, our search often simplifies to finding the model that minimizes \\(\\mbox{RSS}_{\\mathcal{M}}\\) for each value of \\(p_{\\mathcal{M}}\\). This is the best subset searching strategy. 12.17.1 Finding the Best Subsets The leaps package performs a thorough search for the best subsets of predictors for each model size. Since the algorithm returns a best model for each size, the results do not depend on the a penalty model (such as AIC and BIC). For each model size ,it finds the variables that give the minimum RSS. By default, regsubsets only goes up to \\(p=9\\). You have to set nvmax = j, where \\(j\\) is the number of regressors you want to consider. # may need to install.package the first time library(leaps) # you need to load package every time you want to use it # model selection by best subset search best &lt;- regsubsets(Life.Exp ~ ., data = statedata) bsum &lt;- summary(best) # summarize model that minimizes RSS for each p bsum$which # nicer output #&gt; (Intercept) Population Income Illiteracy Murder HS.Grad Frost Area #&gt; 1 TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE #&gt; 2 TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE #&gt; 3 TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE #&gt; 4 TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE #&gt; 5 TRUE TRUE TRUE FALSE TRUE TRUE TRUE FALSE #&gt; 6 TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE #&gt; 7 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # What output is stored after running regsubsets summary(bsum) #&gt; Length Class Mode #&gt; which 56 -none- logical #&gt; rsq 7 -none- numeric #&gt; rss 7 -none- numeric #&gt; adjr2 7 -none- numeric #&gt; cp 7 -none- numeric #&gt; bic 7 -none- numeric #&gt; outmat 49 -none- character #&gt; obj 28 regsubsets list 12.18 Review of Criterion-Based Procedures Thus Far RSS (and \\(R^2\\)) is a measurement of the error between the data and a model. RSS will decrease when we add more predictors, regardless if they predict anything. Therefore \\(R^2 = 1 - \\mbox{RSS}/\\mbox{TSS}\\) increases, regardless. \\(p\\)-values should not be taken as very accurate in stepwise or best subset searches because well see small \\(p\\)-values due to chance alone. So we shouldnt just consider RSS or \\(R^2\\)since well always choose the most complicated model. We shouldnt just consider \\(p\\)-values since we will always get false positives. We only want to add predictors if they significantly help improve the prediction. 12.19 Akaikes Information Criterion (AIC) and Bayesian Information Criteria (BIC) \\[\\mbox{AIC}(\\mathcal{M})= n\\log{(\\mbox{RSS}_{\\mathcal{M}}/n)} +2p_{\\mathcal{M}} +c.\\] \\[\\mbox{BIC}(\\mathcal{M})= n\\log{(\\mbox{RSS}_{\\mathcal{M}}/n)} + \\log{(n)} p_{\\mathcal{M}} +c.\\] Both AIC and BIC are criteria that balance fit and complexity. As \\(RSS\\) goes down (yay!), AIC and BIC goes down. As \\(p_{\\mathcal{M}}\\) goes up, there is a penalty for making things more complicated. BIC assigns a bigger penalty for adding more predictors, so it will slightly favor simple models to complex models (compared to AIC). The constant \\(c\\) is the same for all models created from the same data, so it can be ignored. We choose the model the minimizes the AIC and/or BIC. # Storing values we&#39;ll use p &lt;- 2:8 # number of predictors (including intercept) n &lt;- nrow(statedata) # n=50 observations rss &lt;- bsum$rss # rss of each best subset BIC &lt;- bsum$bic # Exactly values from rs summary plot(BIC ~ p, ylab = &quot;BIC&quot;, xlab = &quot;Number of Predictors (incl intercept)&quot;, pch = 16) The car package has a subsets function that takes the generates nice, labeled BIC (or other statistics, not AIC though) plots generated from the regsubsets function. library(car) subsets(best, statistic = &quot;bic&quot;, legend = FALSE) # stat can be bic, cp, adjr2, rsq, rss #&gt; Abbreviation #&gt; Population P #&gt; Income In #&gt; Illiteracy Il #&gt; Murder M #&gt; HS.Grad H #&gt; Frost F #&gt; Area A AIC &lt;- BIC + p * (2 - log(n)) # Compute AIC from BIC plot(AIC ~ p, ylab = &quot;BIC&quot;, xlab = &quot;Number of Predictors (incl intercept)&quot;, pch = 16) 12.19.1 Optional if You Want to Compare With Formulas # This computes BIC from the formula (ignoring the constant c) BIC2 &lt;- n * log(rss/n) + log(n) * p # include the intercept when giving p BIC - BIC2 # This tells you what the constant c is. # This computes AIC from the formula (ignoring the constant c) AIC2 &lt;- n * log(rss/n) + 2 * p # include the intercept when giving p AIC - AIC2 # This tells you what the constant c is. 12.20 Adjusted \\(R^2\\) The adjusted \\(R^2\\) is another criterion that penalizes for the number of parameters in the model. Adjusted \\(R^2\\), \\(R_a^2\\)** is a better criterion for assessing model fit than \\(R^2\\). For model \\(\\mathcal{M}\\) with \\(p_{\\mathcal{M}}\\) regression coefficients, \\[R_a^2=1- \\frac{\\mbox{RSS}_\\mathcal{M}/(n-p_\\mathcal{M})}{\\mbox{TSS}/(n-1)} = 1 - \\left(\\frac{n-1}{n-p_{\\mathcal{M}}} \\right) \\left( 1-R^2 \\right) = 1 - \\frac{\\hat{\\sigma}^2_{\\mathcal{M}}}{\\hat{\\sigma}^2_{\\rm null}}.\\] Adding a regressor to a model only increases \\(R_a^2\\) if the regressor has some predictive value. Minimizing the variance of the prediction error amounts to minimizing \\(\\hat{\\sigma}^2_{\\mathcal{M}}\\). The smaller that \\(\\hat{\\sigma}^2_{\\mathcal{M}}\\) becomes the larger \\(R^2_a\\) becomes. We favor models that produce larger \\(R_a^2\\). Computing the Adjusted \\(R^2\\) #faraway::sumary(lmod) #gives R^2 for full model #summary(lmod) # both R^2 and R_a^2 for full model (adjr &lt;- bsum$adjr) #pulls R_a^2 from regsubsets for each subset #&gt; [1] 0.6015893 0.6484991 0.6939230 0.7125690 0.7061129 0.6993268 0.6921823 plot(adjr ~ p, ylab = expression({R^2}[a]), xlab = &quot;Number of Predictors&quot;, pch = 16) subsets(best, statistic = &quot;adjr2&quot;, legend = FALSE) #&gt; Abbreviation #&gt; Population P #&gt; Income In #&gt; Illiteracy Il #&gt; Murder M #&gt; HS.Grad H #&gt; Frost F #&gt; Area A 12.21 Mean Square Error (MSE) The Mean Square Error (MSE) of an estimator measures the average squared distance between the estimator and the parameter: \\[\\mbox{MSE} (\\hat{\\theta}) = E \\left( (\\hat{\\theta} - \\theta)^2 \\right) = \\mbox{Var} (\\hat{\\theta}) + \\left( \\mbox{Bias}(\\hat{\\theta})\\right)^2\\] MSE is a criterion the combines bias and efficiency. If two estimators are unbiased, one is more efficient than the other if and only if it has a smaller MSE. We favor models with smaller mean squared error, but the search algorithm is very important, otherwise you just use the model with the most regressors. 12.22 Mallows \\(C_p\\) Statistic Mallows \\(C_p\\) statistic is a criterion designed to quantify the predictive usefulness of a model. Mallows \\(C_p\\) statistic is used to estimate the average mean square error of the prediction, \\[ \\frac{1}{\\sigma^2} \\sum_i MSE(\\hat{y}_i) = \\frac{1}{\\sigma^2} \\sum_iE\\big( (\\hat{y}_i - E(y_i))^2 \\big)\\] The average of the mean square errors can be approximated by Mallows \\(C_p\\) Statistic: \\[C_{p_{\\mathcal{M}}} = \\frac{\\mbox{RSS}_{\\mathcal{M}}}{\\hat{\\sigma}^2} + 2p_{\\mathcal{M}} - n\\]. For the model with all regressors (model \\(\\Omega\\) with \\(p_{\\Omega}\\) regression coefficients), we have \\(C_{p_{\\Omega}}=p_{\\Omega}\\) If a model with \\(p_{\\mathcal{M}}\\) regression coefficients fits the data well and has little or no bias, then \\(E(C_{p_{\\mathcal{M}}}) \\approx p_{\\mathcal{M}}\\). A model with a biased fit will have \\(C_{p_{\\mathcal{M}}}\\) much larger than \\(p_{\\mathcal{M}}\\). Models with \\(C_{p_{\\mathcal{M}}}\\) less than \\(p_{\\mathcal{M}}\\) do not show evidence of bias. It is common to plot \\(C_{p_{\\mathcal{M}}}\\) versus \\(p_{\\mathcal{M}}\\) and compare this to \\(45^{\\circ}\\) line \\(C_{p_{\\mathcal{M}}}= p_{\\mathcal{M}}\\) . We favor models with small \\(p_{\\mathcal{M}}\\) and \\(C_{p_{\\mathcal{M}}}\\) close to \\(p_\\mathcal{M}\\). 12.22.1 Computing Mallows \\(C_p\\) Statistic cp &lt;- bsum$cp # Display the C_p for each value of p plot(cp ~ p, ylab = expression({C_p}), xlab = &quot;Number of Predictors&quot;, pch = 16) abline(0,1) # plots line y=x subsets(best, statistic = &quot;cp&quot;, legend = FALSE) #&gt; Abbreviation #&gt; Population P #&gt; Income In #&gt; Illiteracy Il #&gt; Murder M #&gt; HS.Grad H #&gt; Frost F #&gt; Area A abline(0,1) 12.22.2 Question 7: Interpret the output from the \\(C_p\\) plot above. What is the best model according to this metric? Four predictors (including the intercept) seems about right. Five predictors could be a suitable choice too. bmod &lt;- lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata) 12.22.3 Root Mean Squared Error The RMSE (root mean squared error) is simply the square root of the MSE, and is sometimes used in place of the MSE. The RMSE or MSE will produce identical variable selection results since they are 1-1 transformations of each other. 12.23 Cross-validation In the previous example, we can pat ourselves on the back and say we removed four predictors and that causes only a minor reduction in fit. Well done, but a better question might be: what would the effect of removing these variables be on a new independent sample? Well, we just used all of our sample data to construct this model. We need to see how well our data does with new data (not used in construction of the model). How can we see how good our model works? Cross-validation breaks the data into a training dataset and a test dataset to get a more accurate assessment of the predictive accuracy of a model. A model is fit to the training dataset. The fitted model is used to predict the responses of the test dataset. An error criterion (e.g, the MSE) is calculated for the test dataset. When using cross-validation as your selection criterion, we prefer the model that produces the lowest MSE (or RMSE). ## Methods For Splitting the Data There are many variations of how to choose the training and testing datasets for crossvalidation. 12.23.1 Leave-One Out Crossvalidation Leave-one-out crossvalidation uses each observation (individually) as a test data set, using the other \\(n-1\\) observations as the training data. 12.23.1.1 Should We Inlcude Population? ersq &lt;- numeric(n) for (i in 1:n){ train.ds &lt;- statedata[-i, ] test.ds &lt;- statedata[i, ] tmod &lt;- lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = train.ds) predy &lt;- predict(tmod, new = test.ds) y &lt;- test.ds[1,4] ersq[i] &lt;- (predy - y)^2 } (tmse &lt;- sum(ersq)) #&gt; [1] 29.64823 ersq &lt;- numeric(n) for (i in 1:n){ train.ds &lt;- statedata[-i, ] test.ds &lt;- statedata[i, ] tmod &lt;- lm(Life.Exp ~ Murder + HS.Grad + Frost, data = train.ds) predy &lt;- predict(tmod, new = test.ds) y &lt;- test.ds[1,4] ersq[i] &lt;- (predy - y)^2 } (tmse &lt;- sum(ersq)) #&gt; [1] 31.19675 12.23.2 \\(k\\)-Fold Crossvalidation \\(k\\)-fold crossvalidation breaks the data into \\(k\\) unique sets. For each set, the other \\(k-1\\) sets are used as training data, and then the fitted model is used to predict the responses for the \\(k\\)th testing set. We must fit \\(k\\) models to determine the mean squared error. 12.24 Example of \\(k\\)-fold Crossvalidation Lets comparison the full model to model with Population, Murder, HS.Grad, and Frost predictors using the RMSE criterion and both 10-fold crossvalidation and leave-one-out crossvalidation. The caret package (short for Classification And REgression Training) contains functions to streamline the model training process for regression and classification problems. library(caret) #&gt; Loading required package: lattice # define training/test (control) data cv_10fold &lt;- trainControl(method=&quot;cv&quot;, number = 10) # 10-fold crossvalidation train/test data # Set up fill and model with our 4 regressors f1 = Life.Exp ~ . # formula for full model f2 = Life.Exp~Population + Murder + HS.Grad + Frost # Using training data to construct each model modela &lt;- train(f1, data = statedata, trControl=cv_10fold, method = &quot;lm&quot;) #full modelb &lt;- train(f2, data = statedata, trControl=cv_10fold, method = &quot;lm&quot;) #with 4 reg print(modela) # full, 10-fold #&gt; Linear Regression #&gt; #&gt; 50 samples #&gt; 7 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold) #&gt; Summary of sample sizes: 46, 45, 45, 46, 46, 45, ... #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 0.8360468 0.6470376 0.7039428 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE print(modelb) # reduced, 10-fold #&gt; Linear Regression #&gt; #&gt; 50 samples #&gt; 4 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (10 fold) #&gt; Summary of sample sizes: 46, 45, 45, 46, 45, 43, ... #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 0.74694 0.7148088 0.6274161 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # leave-one-out crossvalidation train/test data cv_loo &lt;- trainControl(method=&quot;LOOCV&quot;) modelfull &lt;- train(f1, data = statedata, trControl=cv_loo, method = &quot;lm&quot;) #full modelred &lt;- train(f2, data = statedata, trControl=cv_loo, method = &quot;lm&quot;) #with 4 reg print(modelfull) # full, leave one out #&gt; Linear Regression #&gt; #&gt; 50 samples #&gt; 7 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Leave-One-Out Cross-Validation #&gt; Summary of sample sizes: 49, 49, 49, 49, 49, 49, ... #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 0.9090885 0.5469535 0.7196334 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE print(modelred) # reduced, leave one out #&gt; Linear Regression #&gt; #&gt; 50 samples #&gt; 4 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Leave-One-Out Cross-Validation #&gt; Summary of sample sizes: 49, 49, 49, 49, 49, 49, ... #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 0.770042 0.6657615 0.6377097 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE 12.25 Theres Still More to Consider! #library(car) #needed but we already loaded influencePlot(lmod) #&gt; StudRes Hat CookD #&gt; AK -1.6061632 0.8095223 1.320803928 #&gt; CA -0.1590567 0.4088569 0.002239186 #&gt; HI 2.7352416 0.3787617 0.493948906 #&gt; ME -2.2322062 0.1218190 0.078915835 Lets remove Alaska since it is a high leverage point. Then identify best subset using \\(R^2_a\\) as our search criterion. best &lt;- regsubsets(Life.Exp ~., data = statedata, subset = (state.abb != &quot;AK&quot;)) bsum &lt;- summary(best) bsum$which[which.max(bsum$adjr), ] #&gt; (Intercept) Population Income Illiteracy Murder HS.Grad #&gt; TRUE TRUE FALSE FALSE TRUE TRUE #&gt; Frost Area #&gt; TRUE TRUE 12.26 Summary There are other mechanisms for choosing the training and test datasets, but these are the most common. When using cross-validation as your selection criterion, we prefer the model that produces the lowest MSE or RMSE. You typically dont do an exhaustive search or stepwise selection search. You often use one of the other selection criteria/search strategies to narrow down the possible models to a few final candidate models and then use cross-validation to make a final decision. Iteration and experimentation are essential to finding better models BUT be very careful not to overtrain your model to the sample data! 12.27 Exercise For the teengamb data in the faraway package, use the methods learned in this chapter to identify the best models. library(faraway) #&gt; #&gt; Attaching package: &#39;faraway&#39; #&gt; The following object is masked _by_ &#39;.GlobalEnv&#39;: #&gt; #&gt; pima #&gt; The following object is masked from &#39;package:lattice&#39;: #&gt; #&gt; melanoma #&gt; The following objects are masked from &#39;package:car&#39;: #&gt; #&gt; logit, vif data(teengamb) # load data summary(teengamb) #&gt; sex status income verbal #&gt; Min. :0.0000 Min. :18.00 Min. : 0.600 Min. : 1.00 #&gt; 1st Qu.:0.0000 1st Qu.:28.00 1st Qu.: 2.000 1st Qu.: 6.00 #&gt; Median :0.0000 Median :43.00 Median : 3.250 Median : 7.00 #&gt; Mean :0.4043 Mean :45.23 Mean : 4.642 Mean : 6.66 #&gt; 3rd Qu.:1.0000 3rd Qu.:61.50 3rd Qu.: 6.210 3rd Qu.: 8.00 #&gt; Max. :1.0000 Max. :75.00 Max. :15.000 Max. :10.00 #&gt; gamble #&gt; Min. : 0.0 #&gt; 1st Qu.: 1.1 #&gt; Median : 6.0 #&gt; Mean : 19.3 #&gt; 3rd Qu.: 19.4 #&gt; Max. :156.0 # fit full model lmod &lt;- lm(gamble ~ ., data = teengamb) sumary(lmod) # determine least significant predictor #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 22.555651 17.196803 1.3116 0.19677 #&gt; sex -22.118330 8.211115 -2.6937 0.01011 #&gt; status 0.052234 0.281112 0.1858 0.85349 #&gt; income 4.961979 1.025392 4.8391 1.792e-05 #&gt; verbal -2.959493 2.172150 -1.3625 0.18031 #&gt; #&gt; n = 47, p = 5, Residual SE = 22.69034, R-Squared = 0.53 # Values we&#39;ll need n &lt;- nobs(lmod) p &lt;- 2:5 # perform backward elimination using update function on previous model # use alpha_crit = 0.05 lmod &lt;- update(lmod, . ~ . - status) sumary(lmod) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 24.13897 14.76859 1.6345 0.109459 #&gt; sex -22.96022 6.77057 -3.3912 0.001502 #&gt; income 4.89809 0.95512 5.1283 6.644e-06 #&gt; verbal -2.74682 1.82528 -1.5049 0.139667 #&gt; #&gt; n = 47, p = 4, Residual SE = 22.43416, R-Squared = 0.53 lmod &lt;- update(lmod, . ~ . - verbal) sumary(lmod) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.04083 6.39435 0.6319 0.530698 #&gt; sex -21.63439 6.80880 -3.1774 0.002717 #&gt; income 5.17158 0.95105 5.4378 2.245e-06 #&gt; #&gt; n = 47, p = 3, Residual SE = 22.75428, R-Squared = 0.5 # model selection in terms of AIC library(leaps) # model selection by exhaustive search best &lt;- regsubsets(gamble ~ ., data = teengamb) bsum &lt;- summary(best) # summarize model that minimizes RSS for each p bsum$which # best subset models (in terms of RSS) #&gt; (Intercept) sex status income verbal #&gt; 1 TRUE FALSE FALSE TRUE FALSE #&gt; 2 TRUE TRUE FALSE TRUE FALSE #&gt; 3 TRUE TRUE FALSE TRUE TRUE #&gt; 4 TRUE TRUE TRUE TRUE TRUE # calculate AIC of each model aic &lt;- bsum$bic + (2 - log(n)) * p # plot AIC vs p plot(aic ~ p, xlab = &quot;Number of Predictors&quot;, ylab = &quot;AIC&quot;, pch =16) # calculate BIC of each model # plot BIC vs p library(car) subsets(best, statistic = &quot;bic&quot;, legend = FALSE) #&gt; Abbreviation #&gt; sex sx #&gt; status st #&gt; income i #&gt; verbal v # Construct Cp plot subsets(best, statistic = &quot;cp&quot;, legend = FALSE) #&gt; Abbreviation #&gt; sex sx #&gt; status st #&gt; income i #&gt; verbal v abline(0, 1) # Construct adjusted R^2 plot subsets(best, statistic = &quot;adjr2&quot;, legend = FALSE) #&gt; Abbreviation #&gt; sex sx #&gt; status st #&gt; income i #&gt; verbal v # backward elimination lmod &lt;- lm(gamble ~ ., data = teengamb) step(lmod) #with AIC #&gt; Start: AIC=298.18 #&gt; gamble ~ sex + status + income + verbal #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - status 1 17.8 21642 296.21 #&gt; &lt;none&gt; 21624 298.18 #&gt; - verbal 1 955.7 22580 298.21 #&gt; - sex 1 3735.8 25360 303.67 #&gt; - income 1 12056.2 33680 317.00 #&gt; #&gt; Step: AIC=296.21 #&gt; gamble ~ sex + income + verbal #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 21642 296.21 #&gt; - verbal 1 1139.8 22781 296.63 #&gt; - sex 1 5787.9 27429 305.35 #&gt; - income 1 13236.1 34878 316.64 #&gt; #&gt; Call: #&gt; lm(formula = gamble ~ sex + income + verbal, data = teengamb) #&gt; #&gt; Coefficients: #&gt; (Intercept) sex income verbal #&gt; 24.139 -22.960 4.898 -2.747 step(lmod, k = log(n)) # with BIC #&gt; Start: AIC=307.43 #&gt; gamble ~ sex + status + income + verbal #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - status 1 17.8 21642 303.62 #&gt; - verbal 1 955.7 22580 305.61 #&gt; &lt;none&gt; 21624 307.43 #&gt; - sex 1 3735.8 25360 311.07 #&gt; - income 1 12056.2 33680 324.40 #&gt; #&gt; Step: AIC=303.62 #&gt; gamble ~ sex + income + verbal #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; - verbal 1 1139.8 22781 302.18 #&gt; &lt;none&gt; 21642 303.62 #&gt; - sex 1 5787.9 27429 310.90 #&gt; - income 1 13236.1 34878 322.19 #&gt; #&gt; Step: AIC=302.18 #&gt; gamble ~ sex + income #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 22781 302.18 #&gt; - sex 1 5227.3 28009 308.04 #&gt; - income 1 15309.8 38091 322.49 #&gt; #&gt; Call: #&gt; lm(formula = gamble ~ sex + income, data = teengamb) #&gt; #&gt; Coefficients: #&gt; (Intercept) sex income #&gt; 4.041 -21.634 5.172 library(caret) f1 = gamble ~ sex + income f2 = gamble ~ sex + verbal + income # 5-fold crossvalidation train/test data cv_5fold &lt;- trainControl(method = &quot;cv&quot;, number = 5) model1 &lt;- train(f1, data = teengamb, trControl = cv_5fold, method = &quot;lm&quot;) model2 &lt;- train(f2, data = teengamb, trControl = cv_5fold, method = &quot;lm&quot;) # compare mse (rmse) for the two models using 5-fold cv print(model1) # p = 3 #&gt; Linear Regression #&gt; #&gt; 47 samples #&gt; 2 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold) #&gt; Summary of sample sizes: 37, 38, 36, 38, 39 #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 22.29144 0.6368292 16.78134 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE print(model2) # p = 4 #&gt; Linear Regression #&gt; #&gt; 47 samples #&gt; 3 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold) #&gt; Summary of sample sizes: 37, 38, 38, 36, 39 #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 21.23541 0.5782447 15.21191 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE We prefer the model with smaller RMSE and MSE. This can switch depending on the random 5-fold data set selected. # trying an interaction model f3 = gamble ~ sex*income model3 &lt;- train(f3, data = teengamb, trControl = cv_5fold, method = &quot;lm&quot;) print(model3) # even better #&gt; Linear Regression #&gt; #&gt; 47 samples #&gt; 2 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold) #&gt; Summary of sample sizes: 39, 37, 37, 39, 36 #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 22.16232 0.488795 13.60712 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE residualPlots(lm(f3, data = teengamb)) # try a transformed model # trying another model f4 = sqrt(gamble) ~ sex*income model4 &lt;- train(f4, data = teengamb, trControl = cv_5fold, method = &quot;lm&quot;) print(model4) #&gt; Linear Regression #&gt; #&gt; 47 samples #&gt; 2 predictor #&gt; #&gt; No pre-processing #&gt; Resampling: Cross-Validated (5 fold) #&gt; Summary of sample sizes: 36, 38, 38, 38, 38 #&gt; Resampling results: #&gt; #&gt; RMSE Rsquared MAE #&gt; 2.115957 0.4698357 1.733394 #&gt; #&gt; Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE residualPlots(lm(f4, data = teengamb)) #&gt; Test stat Pr(&gt;|Test stat|) #&gt; sex 0.5637 0.5759 #&gt; income -0.8070 0.4242 #&gt; Tukey test -0.8196 0.4124 Not comparable to previous model since the response is transformed. Should really go through variable selection process again. "],["transformations.html", "Chapter 13 Transformations", " Chapter 13 Transformations "],["advanced-regressor-variables.html", "Chapter 14 Advanced regressor variables", " Chapter 14 Advanced regressor variables "],["categorical-predictors-1.html", "Chapter 15 Categorical predictors 15.1 Indicator/dummy variables 15.2 Common of linear models with categorical predictors", " Chapter 15 Categorical predictors Categorical predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the categorical variables. In what follows, we consider several common linear regression models that involve a categorical variable. To simplify our discussion, we only consider the setting where there is a single categorical variable to add to our model. Similarly, we only consider the setting where there is a single numeric variable. We begin by defining some notation. Let \\(X\\) denote a numeric regressor, with \\(x_i\\) denoting the value of \\(X\\) for observation \\(i\\). Let \\(F\\) denote a categorical variable with levels \\(L_1, L_2, \\ldots, L_K\\). The \\(F\\) stands for factor, while the \\(L\\) stands for level. The notation \\(f_i\\) denotes the value of \\(F\\) for observation \\(i\\). 15.1 Indicator/dummy variables We may recall that if \\(\\mathbf{X}\\) denotes our matrix of regressors and \\(\\mathbf{y}\\) our vector of responses, then (assuming the columns of \\(\\mathbf{X}\\) are linearly independent) the OLS solution for \\(\\boldsymbol{\\beta}\\) is \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\\] In order to compute the estimated coefficients, both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) must contain numeric values. How can we use a categorical predictor in our regression model when the levels are not numeric values? In order to use a categorical predictor in a regression model, we must transform it into a set of one or more indicator or dummy variables, which we explain in more detail below. An indicator function is a function that takes the value 1 of a certain property is true and 0 otherwise. An indicator variable is the variable that results from applying an indicator function to each observation in a data set. Many notations exist for indicator functions. We will adopt the notation \\[\\begin{equation*} I_S(x) = \\begin{cases} 1 &amp; \\textrm{if}\\;x \\in S\\\\ 0 &amp; \\textrm{if}\\;x \\notin S \\end{cases}, \\end{equation*}\\] which is shorthand for a function that returns 1 if \\(x\\) is in the set \\(S\\) and 0 otherwise. We let \\(D_j\\) denote the indicator (dummy) variable for factor level \\(L_j\\) of \\(F\\). The value of \\(D_j\\) for observation \\(i\\) is denoted \\(d_{i,j}\\), with \\[d_{i,j} = I_{\\{L_j\\}}(f_i),\\] i.e., \\(d_{i,j}\\) is 1 if observation \\(i\\) has factor level \\(L_j\\) and 0 otherwise. 15.2 Common of linear models with categorical predictors It is common to use notation like \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 +\\beta_2 X_2\\) when discussing linear regression models. That notation is generally simple and convenient, but can be unclear. Asking a researcher what the estimate of \\(\\beta_2\\) is in a model is ambiguous because it will depend on the order the researcher added the variables to the model. To more closely connect each coefficient with the regressor to which it is related, we will use the notation \\(\\beta_X\\) to denote the coefficient for regressor \\(X\\) and \\(\\beta_{D_j}\\) to denote the coefficient for regressor \\(D_j\\). Similarly, \\(\\beta_{int}\\) denotes the intercept included in our model. 15.2.1 One-way ANOVA 15.2.1.1 Definition A one-way analysis of variance (ANOVA) assumes a constant mean for each level of a categorical variable. A general one-way ANOVA relating a response variable \\(Y\\) to a factor variable \\(F\\) with \\(K\\) levels may be formulated as \\[E(Y|F) = \\beta_{int} + \\beta_{D_2} D_2 + \\ldots + \\beta_{D_K} D_K.\\] Alternatively, in terms of the individual responses, we may formulate the one-way ANOVA model as \\[Y_i = \\beta_{int} + \\beta_{D_2} d_{i,2} + \\cdots + \\beta_{D_K} d_{i,K} + \\epsilon_i,\\quad i=1,2,\\ldots n.\\] This may bring up some questions that need answering. Why does the one-way ANOVA model only contains dummy variables for the last \\(K-1\\) levels of \\(F\\)? This is not a mistake. If we included dummy variables for all levels of \\(F\\), then the matrix of regressors would have linearly dependent columns because the sum of the dummy variables for all \\(K\\) levels would equal the column of 1s for the intercept. Why do we omit the dummy variable for the first level of \\(F\\)? This is simply convention. We could omit the dummy variable for any single level of \\(F\\). However, it is conventional to designate one level the reference level and to omit that variable. As we will see when discussing interpretation of the coefficient, the reference level becomes the level of \\(F\\) that all other levels are compared to. Could we omit the intercept instead of one of the dummy variables? Yes, you could. There is no mathematical or philosophical issues with this. However, this can create problems when you construct models including both categorical and numeric regressors. The standard approach is recommended because it typically makes our model easier to interpret and extend. 15.2.1.2 Interpretation We interpret the coefficients of our one-way ANOVA with respect to the change in the mean response. Suppose an observation of level \\(L_1\\). We can determine that the mean response is \\[\\begin{align*} E(Y|F=L_1) &amp;= \\beta_{int} + \\beta_{D_2} 0 + \\cdots + \\beta_{D_K} 0 \\\\ &amp;= \\beta_{int}. \\end{align*}\\] Similarly, when an observation has level \\(L_2\\), then \\[\\begin{align*} E(Y|F=L_2) &amp;= \\beta_{int} + \\beta_{D_2} 1 + \\beta_{D_3} 0 + \\cdots + \\beta_{D_K} 0 \\\\ &amp;= \\beta_{int} + \\beta_{D_2}. \\end{align*}\\] This helps us to see the general relationship that \\[E(Y|F=L_j) = \\beta_{int} + \\beta_{D_j},\\quad j=2,\\ldots,K.\\] In the context of a one-way ANOVA: \\(\\beta_{int}\\) represents the expected response for observations having the reference level. \\(\\beta_{L_j}\\), for \\(j=2,\\ldots,K\\), represents the expected change in the response when comparing observations having the reference level and level \\(L_j\\). You can verify this by computing \\(E(Y|F=L_j) - E(Y|F=L_1)\\) (for \\(j = 2, \\ldots, K\\)). 15.2.2 Main effects models A main effects model is also called a parallel lines model since the regression equations for each factor level produce lines parallel to one another. A parallel lines model is formulated as \\[ Y_i = \\beta_{int} + \\beta_{X} x_i + \\beta_{L_2} d_{i,2} + \\cdots + \\beta_{L_K} d_{i,K} + \\epsilon_i,\\quad i=1,2,\\ldots n.\\] When an observation has level \\(L_1\\), then the expected response is \\(\\beta_{int} + \\beta_1 X\\). More specifically, \\[E(Y|X = x, F=L_1) = \\beta_{int} + \\beta_X x + \\beta_{L_2} 0 + \\cdots + \\beta_{L_K} 0 = \\beta_{int} + \\beta_X x.\\] Thus, \\(E(Y|X = 0, F=L_1) = \\beta_{int}.\\) When an observation has level \\(L_2\\), the expected response is \\(\\beta_{int} + \\beta_{X} X + \\beta_{L_2}\\). More formally, \\[E(Y|X = x, F=L_2) = \\beta_{int} + \\beta_X x + \\beta_{L_2} 1 + \\beta_{L_3} 0 + \\cdots + \\beta_{L_K} 0 = \\beta_{int} + \\beta_X x + \\beta_{L_2}.\\] Thus, the mean response for observations having level \\(L_2\\) is \\(\\beta_{int} + \\beta_{L_2} + \\beta_{X} x\\). In general, \\[E(Y|X = x, F=L_j) = \\beta_{int} + \\beta_X x + \\beta_{L_j},\\quad j = 2,\\ldots,K.\\] Thus, \\[E(Y|X=x, F=L_j) - E(Y|X=x, F=L_1) = (\\beta_{int} + \\beta_X x + \\beta_{L_j}) - (\\beta_{int} + \\beta_X x) = \\beta_{L_j}.\\] In the context of parallel lines models: \\(\\beta_{int}\\) represents the expected response for observations having the reference level when the numeric regressor \\(X = 0\\). \\(\\beta_X\\) is the expected change in the response when \\(X\\) increases by 1 unit for a fixed level of \\(F\\). \\(\\beta_{L_j}\\), for \\(j=2,\\ldots,K\\) represents the expected change in the response when comparing observations having level \\(L_1\\) and \\(L_j\\) with \\(X\\) fixed at the same value. 15.2.3 Interaction models An interaction model is also called a separate lines model since the regression equations for each factor level produce lines that are distinct and separate. A separate lines model is formulated as \\[ Y_i = \\beta_{int} + \\beta_{X} x_i + \\beta_{L_2} d_{i,2} + \\cdots + \\beta_{L_K} d_{i,K} + + \\beta_{X L_2} x_i d_{i,2} + \\cdots + \\beta_{X L_j} x_i d_{i,K} + \\epsilon_i,\\quad i=1,2,\\ldots n.\\] When an observation has level \\(L_1\\), then the expected response is \\(\\beta_{int} + \\beta_1 X\\). More specifically, \\[E(Y|X = x, F=L_1) = \\beta_{int} + \\beta_X x + \\beta_{L_2} 0 + \\cdots + \\beta_{L_K} 0 + \\beta_{X L_2} x_i 0 + \\cdots + \\beta_{X L_K} x_i 0 = \\beta_{int} + \\beta_X x.\\] Thus, \\(E(Y|X = 0, F=L_1) = \\beta_{int}.\\) When an observation has level \\(L_j\\), for \\(j=2,\\ldots,K\\), the expected response is \\(\\beta_{int} + \\beta_{X} X + \\beta_{L_j} + \\beta_{X L_J} X.\\) More formally, \\[E(Y|X = x, F=L_j) = \\beta_{int} + \\beta_X x + \\beta_{L_j} + \\beta_{X L_j} x.\\] Note that \\[E(Y|X=0, F=L_1) = \\beta_{int}.\\] Additionally, we note that \\[E(Y|X=0, F=L_j) - E(Y|X=0, F=L_1) = (\\beta_{int} + \\beta_X 0 + \\beta_{L_j} + \\beta_{X L_J} 0) - (\\beta_{int} + \\beta_X 0) = \\beta_{L_j}.\\] In the context of separate lines models: \\(\\beta_{int}\\) represents the expected response for observations having the reference level when the numeric regressor \\(X = 0\\). \\(\\beta_{L_j}\\), for \\(j=2,\\ldots,K\\), represents the expected change in the response when comparing observations having level \\(L_1\\) and \\(L_j\\) with \\(X=0\\). \\(\\beta_X\\) represents the expected change in the response when \\(X\\) increases by 1 unit for observations having the reference level. \\(\\beta_X L_j\\), for \\(j=2,\\ldots,K\\), represents the difference in the expected rate of change when \\(X\\) increases by 1 unit for observations have the baseline level in comparison to level \\(L_j\\). 15.2.4 Extensions In the models above, we have only discussed possibilities with a single numeric variable and a single factor variable. Naturally, one can consider models with multiple factor variables, multiple numeric variables, interactions between factor variables, interactions between numeric variables, etc. The models become more complicated, but the ideas are similar. One simply has to keep track of what role each coefficient plays in the model. title: Joshua French date: 2022-06-13 output: html_document: df_print: paged "],["advanced-model-structure.html", "Chapter 16 Advanced model structure", " Chapter 16 Advanced model structure "],["references.html", "References", " References "],["defining-and-fitting-a-linear-model.html", "Chapter 17 Defining and fitting a linear model 17.1 Background and terminology 17.2 Goals of regression 17.3 Definition of a linear model 17.4 Summarizing the components of a linear model 17.5 Types of regression models 17.6 Standard linear model assumptions and implications 17.7 Mathematical interpretation of coefficients 17.8 Exercises 17.9 Parameter estimation for linear models 17.10 OLS estimation of the simple linear regression model 17.11 Penguins simple linear regression example 17.12 Fitting a linear model using R", " Chapter 17 Defining and fitting a linear model In this chapter, we define a linear model and discuss the basic estimation of its parameters. We leave discussion of more theoretical aspects of the model to subsequent chapters. 17.1 Background and terminology Regression models are used to model the relationship between: one or more response variables and one or more predictor variables. The distinction between these two types variables is their purpose in the model. Predictor variables are used to predict the value of the response variable. Response variables are also known as outcome, output, or dependent variables. Predictor variables are also known as explanatory, regressor, input, dependent, or feature variables. Note: Because the variables in our model are often interrelated, describing these variables as independent or dependent variables is vague and is best avoided. A distinction is sometimes made between regression models and classification models. In that case: Regression models attempt to predict a numerical response. Classification models attempt to predict the category level a response will have. 17.2 Goals of regression The basic goals of a regression model are to: Predict future or unknown response values based on specified values of the predictors. What will the selling price of a home be? Describe relationships (associations) between predictor variables and the response. What is the general relationship between the selling price of a home and the number of bedrooms the home has? With our regression model, we also hope to be able to: Generalize our results from the sample to the a larger population of interest. E.g., we want to extend our results from a small set of college students to all college students. Infer causality between our predictors and the response. E.g., if we give a person a vaccine, then this causes the persons risk of catching the disease to decrease. A true model doesnt exist for real data. The data-generating process is far more complex than the models we can realistically fit to the data. Thus, finding the true model should not be the goal of a regression analysis. A regression analysis should attempt to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.) 17.3 Definition of a linear model A linear model is a regression model in which the regression coefficients (to be discussed later) enter the model linearly. A linear model is just a specific type of regression model. 17.3.1 Basic construction and relationships We begin by defining notation for the objects we will need and clarifying some of their important properties. \\(Y\\) denotes the response variable. The response variable is treated as a random variable. We will observe realizations of this random variable for each observation in our data set. \\(X\\) denotes a single predictor variable. \\(X_1\\), \\(X_2\\), , \\(X_{p-1}\\) denote the predictor variables \\(1,2,\\ldots,p\\). The predictor variables are treated as non-random variables. We will observe values of the predictors variables for each observation in our data set. \\(\\beta_0\\), \\(\\beta_1\\), , \\(\\beta_{p-1}\\) denote regression coefficients. Regression coefficients are statistical parameters that we will estimate from our data. Like all statistical parameters, regression coefficients are treated as fixed (non-random) but unknown values. Regression coefficients are not observable. \\(\\epsilon\\) denotes error. The error is not observable. The error is treated as a random variable. The error is assumed to have mean 0, i.e., \\(E(\\epsilon) = 0\\). Since \\(E(\\epsilon) = 0\\) and \\(X\\) is non-random, the expectation of \\(\\epsilon\\) conditional on \\(X\\) is also 0, i.e., \\(E(\\epsilon | X) = 0\\). In this context, error doesnt mean mistake or malfunction. \\(\\epsilon\\) is simply the deviation of the response from its mean. A linear model for \\(Y\\) is defined by the equation \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon. \\tag{5.1} \\end{equation}\\] We now emphasize the relationship between the response, the mean response, and the error. The mean of the response variable will depend on the values of the predictor variables. Thus, we can only discuss the expectation of the response variable conditional on the values of the predictor variables. This is denoted as \\(E(Y | X_1, \\ldots, X_{p-1})\\). For simplicity, assume our linear model only has a single predictor (this is an example of simple linear regression). Based on what weve presented, we have that \\[\\begin{align} E(Y|X) &amp;= E(\\beta_0 + \\beta_1 X + \\epsilon | X) \\\\ &amp;= E(\\beta_0 | X) + E(\\beta_1 X | X) + E(\\epsilon | X) \\\\ &amp;= \\beta_0 + \\beta_1 X + 0\\\\ &amp;= \\beta_0 + \\beta_1 X. \\end{align}\\] The second line follows from the fact that the expectation of a sum of random variables is the sum of the expectation of the random variables. The third line follows from the fact that the expected value of a constant (non-random) value is the constant (the regression coefficients and \\(X\\) are non-random) and by our assumption that the errors have mean 0 (unconditionally or conditionally on the predictor variable.) Thus, we see that we see that for a simple linear regression model \\[ Y = E(Y|X) + \\epsilon.\\] For a model with multiple predictors, this extends to \\[Y = E(Y|X_1, X_2, \\ldots, X_{p-1}) + \\epsilon.\\] Thus, our response may be written as the sum of the mean response conditional on the predictors, \\(E(Y|X_1, X_2, \\ldots, X_{p-1})\\), and the error. This is why previously we discussed the fact that the error is simply the deviation of the response from its mean. Alternatively, one can say that a regression model is linear if the mean function can be written as a linear combination of the regression coefficients and known values, i.e., \\[E(Y|X_1, X_2, \\ldots, X_{p-1}) = \\sum_{j=0}^{p-1} c_j \\beta_j,\\] where \\(c_0, c_1, \\ldots, c_{p-1}\\) are known values. In fact, the \\(c_i, i = 1,2,\\ldots,n\\) can be any function of \\(X_1,X_2,\\ldots,X_n\\)! e.g., \\(c_1 = X_1 * X_2 * X_3\\), \\(c_3 = X_2^2\\), \\(c_8 = ln(X_1)/X_2^2\\). Some examples of linear models: \\(E(Y|X) = \\beta_0 + +\\beta_1 X + \\beta_2 X^2\\). \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 * X_2\\). \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 \\ln(X_1) + \\beta_2 X_2^{-1}\\). \\(E(\\ln(Y)|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). \\(E(Y^{-1}|X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\). Some examples of non-linear models: \\(E(Y|X) = \\beta_0 + e^{\\beta_1 X}\\). \\(E(Y|X) = \\beta_0 + \\beta_1 X/(\\beta_2 + X)\\). 17.3.2 As a system of equations A linear regression analysis will model the data using a linear model. Suppose we have sampled \\(n\\) observations from a population. We now introduce some additional notation: \\(Y_1, Y_2, \\ldots, Y_n\\) denote the response values for the \\(n\\) observations. \\(x_{i,j}\\) denotes the observed value of predictor \\(j\\) for observation \\(i\\). We use lowercase \\(x\\) to indicate that this is the observed value of the predictor. \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\) denote the errors for the \\(n\\) observations. The linear model relating the responses, the predictors, and the errors is defined by the system of equations \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n. \\tag{5.10} \\end{equation}\\] Based on our previous work, we can also write Equation (5.10) as \\[\\begin{equation} Y_i = E(Y_i | X_1 = x_{i,1}, \\ldots, X_{p-1} = x_{i,p-1}) + \\epsilon_i,\\quad i=1,2,\\ldots,n. \\end{equation}\\] 17.3.3 Using matrix notation The regression coefficients are said to enter the model linearly, which is why this type of model is called a linear model. To see this more clearly, we represent the model using matrices. We define the following notation: \\(\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]^T\\) denotes the column vector containing the \\(n\\) responses. \\(\\mathbf{X}\\) denotes the matrix containing a column of 1s and the observed predictor values, specifically, \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,p-1} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,p-1} \\end{bmatrix}.\\] \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]^T\\) denotes the column vector containing the \\(p\\) regression coefficients. \\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]^T\\) denotes the column vector contained the \\(n\\) errors. Then the system of equations defining the linear model in (5.10) can be written as \\[\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\boldsymbol{\\epsilon}.\\] Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model. 17.4 Summarizing the components of a linear model We have already introduced a lot of objects. To aid in making sense of their notation, their purpose in the model, whether they can be observed, and whether they are modeled as a random variable (vector) or fixed, non-random values, we summarize things below. Weve already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable. On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation (5.10) is for the errors to be random. We summarize this information in the table below for the objects previously discussed using the various notations introduced. Notation Description Observable Random \\(Y\\) response variable Yes Yes \\(Y_i\\) response value for the \\(i\\)th observation Yes Yes \\(\\mathbf{y}\\) the \\(n\\times 1\\) column vector of response values Yes Yes \\(X\\) predictor variable Yes No \\(X_j\\) the \\(j\\)th predictor variable Yes No \\(x_{i,j}\\) the value of the \\(j\\)th predictor variable for the \\(i\\)th observation Yes No \\(\\mathbf{X}\\) the \\(n\\times p\\) matrix of predictor values Yes No \\(\\beta_j\\) the regression coefficient associated with the \\(j\\)th predictor variable No No \\(\\boldsymbol{\\beta}\\) the \\(p\\times 1\\) column vector of regression coefficients No No \\(\\epsilon\\) the error No Yes \\(\\epsilon_i\\) the error associated with observation \\(i\\) No Yes \\(\\boldsymbol{\\epsilon}\\) the \\(n\\times 1\\) column vector of errors No Yes 17.5 Types of regression models The are many named types of regression models. You may hear or see people use these terms when describing their model. Here is a brief overview of some common regression models. Name Defining characteristics Simple an intercept term and one predictor variable Multiple more than one predictor variable Multivariate more than one response variable Linear the regression coefficients enter the model linearly Analysis of variance (ANOVA) predictors are all categorical Analysis of covariance (ANCOVA) at least one quantitative predictor and at least one categorical predictor Generalized linear model (GLM) a type of generalized regression model when the responses do not come from a normal distribution. 17.6 Standard linear model assumptions and implications The formulation of a linear model typically makes additional assumptions beyond the ones previously mentioned, specifically, about the errors, \\(\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n\\). We have already mentioned that fact that we are assuming \\(E(\\epsilon_i)=0\\) for \\(i=1,2,\\ldots,n\\). We also typically assume that the errors have constant variances, i.e., \\[var(\\epsilon_i) = \\sigma^2, \\quad i=1,2,\\ldots,n,\\] and that the errors are uncorrelated, i.e., \\[cov(\\epsilon_i, \\epsilon_j) = 0, \\quad i,j=1,2,\\ldots,n,\\quad i\\neq j.\\] Additionally, we assume that the errors are identically distributed. Formally, that may be written as \\[\\begin{equation} \\epsilon_i \\sim F, i=1,2,\\ldots,n, \\tag{17.1} \\end{equation}\\] where \\(F\\) is some arbitrary distribution. The \\(\\sim\\) means distributed as. In other words, Equation (17.1) means, \\(\\epsilon_i\\) is distributed as \\(F\\) for \\(i\\) equal to \\(1,2,\\ldots,n\\). However, it is more common to assume the errors have a normal (Gaussian) distribution. Two uncorrelated normal random variables are also independent (this is true for normal random variables, but is not generally true for other distributions). Thus, we may concisely state the typical error assumptions as \\[\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n \\stackrel{i.i.d.}{\\sim} N(0, \\sigma^2),\\] which combines the following assumptions: \\(E(\\epsilon_i)=0\\) for \\(i=1,2,\\ldots,n\\). \\(var(\\epsilon_i)=\\sigma^2\\) for \\(i=1,2,\\ldots,n\\). \\(cov(\\epsilon_i,\\epsilon_j)=0\\) for \\(i\\neq j\\) with \\(i,j=1,2,\\ldots,n\\). \\(\\epsilon_i\\) has a normal distribution for \\(i=1,2,\\ldots,n\\). Using the notation previously developed, the assumptions above may be stated in vector notation as \\[\\begin{equation} \\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}_{n\\times 1}, \\sigma^2 I_n), \\tag{17.2} \\end{equation}\\] where \\(\\mathbf{0}_{n\\times 1}\\) is the \\(n \\times 1\\) vector of zeros and \\(I_n\\) is the \\(n\\times n\\) identity matrix. Writing the error assumptions in the form of Equation (17.2) allows us to see other important properties of a linear model. Specifically, if we make the assumptions in Equation (17.2), then \\[\\begin{equation} \\mathbf{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 I_n). \\tag{17.3} \\end{equation}\\] How do we know that Equation (17.3) is true? Show that: \\(E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta}\\) and \\(\\mathrm{var}(\\mathbf{y}) = \\sigma^2 I_n\\). Note \\(\\mathbf{y}\\) is a linear function of the multivariate normal vector \\(\\boldsymbol{\\epsilon}\\), so \\(\\mathbf{y}\\) must also have a multivariate normal distribution. 17.7 Mathematical interpretation of coefficients The regression coefficients have simple mathematical interpretations in basic settings. 17.7.1 Coefficient interpretation in simple linear regression Suppose we have a simple linear regression model, so that \\(E(Y|X)=\\beta_0 + \\beta_1 X.\\) The interpretations of the coefficients are: \\(\\beta_0\\) is the expected response when the predictor is 0, i.e., \\(\\beta_0=E(Y|X=0)\\). \\(\\beta_1\\) is the expected change in the response when the predictor increases 1 unit, i.e., \\(\\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)\\). 17.7.2 Coefficient interpretation in multiple linear regression Suppose we have a multiple linear regression model, so that \\(E(Y|X_1,\\ldots,X_{p-1})=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1}.\\) Let \\(\\mathbb{X} = \\{X_1,\\ldots,X_{p-1}\\}\\) be the set of predictors and \\(\\mathbb{X}_{-j} = \\mathbb{X}\\setminus\\{X_j\\}\\), i.e., the set of predictors without \\(X_j\\). The interpretations of the coefficients are: \\(\\beta_0\\) is the expected response when all predictors are 0, i.e., \\(\\beta_0=E(Y|X_1=0,\\ldots,X_{p-1}=0)\\). \\(\\beta_j\\) is the expected change in the response when predictor \\(j\\) increases 1 unit and the other predictors stay the same, i.e., \\(\\beta_j=E(Y|\\mathbb{X}_{-j} = \\mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\\mathbb{X}_{-j} = \\mathbf{x}^*, X_{j+1} = x_{0})\\) where \\(\\mathbf{x}^*\\in \\mathbb{R}^{p-2}\\) is a fixed vector of length \\(p-2\\) (the number of predictors excluding \\(X_j\\)). 17.8 Exercises If given a set of data with several variables, how would you decide what the response variable and the predictor variables would be? Which objects in the linear model formula in Equation (5.1) are considered random? Which are considered fixed? Which objects in the linear model formula in Equation (5.1) are observable? Which are not observable? What are the typical goals of a regression analysis? List the typical assumptions made for the errors in a linear model. Without using a formula, what is the basic difference between a linear model and a non-linear model? Assuming that \\(\\boldsymbol{\\epsilon} ~ N(\\mathbf{0}_{n\\times 1}, \\sigma^2 I_n)\\) and \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\), show that: \\(E(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\beta}\\). \\(\\mathrm{var}(\\mathbf{y}) = \\sigma^2 I_n\\). In the context of simple linear regression under the standard assumptions, show that: \\(\\beta_0=E(Y|X=0)\\). \\(\\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)\\). In the context of multiple linear regression under the standard assumptions, show that: \\(\\beta_0=E(Y|X_1=0,\\ldots,X_{p-1}=0)\\). For \\(j=1,2,\\ldots,p-1\\), \\(\\beta_j=E(Y|\\mathbb{X}_{-j} = \\mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\\mathbb{X}_{-j} = \\mathbf{x}^*, X_{j+1} = x_{0})\\) where \\(\\mathbf{x}^*\\) is a fixed vector of the appropriate size. 17.9 Parameter estimation for linear models In this chapter we focus on estimating the parameters of a linear regression model. We also discuss important properties of the parameter estimators. To make the discussion easier to follow, we start by describing the simple linear regression model, which is a linear model with only a single predictor. There are many different methods of parameter estimation in statistics: method-of-moments, maximum likelihood, Bayesian, etc. The most common parameter estimation method for linear models is least squares method, which is perhaps comonly called Ordinary Least Squares (OLS) estimation. OLS estimation estimates the regression coefficients with the values that minimize the residuals sum of squares (RSS), which we will define shortly. 17.10 OLS estimation of the simple linear regression model In a simple linear regression context, we have \\(n\\) observed responses \\(Y_1,Y_2,\\ldots,Y_n\\) and \\(n\\) predictor values \\(x_1,x_2,\\ldots,x_n\\). Recall that for a simple linear regression model \\[Y = \\beta_0 + \\beta_1 X + \\epsilon = E(Y|X) + \\epsilon\\] with \\[E(Y|X) = \\beta_0 + \\beta_1 X.\\] We need to define some new notation and objects to define the RSS. Let \\(\\hat{\\beta}_j\\) denote the estimated value of \\(\\beta_j\\) and the estimated mean response as a function of the predictor \\(X\\) is \\[\\hat{E}(Y|X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X.\\] The \\(i\\)th fitted value is defined as \\[\\hat{Y}_i = \\hat{E}(Y|X = x_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i.\\] Thus, the \\(i\\)th fitted value is the estimated mean of \\(Y\\) when the predictor \\(X=x_i\\). More specifically, the \\(i\\)th fitted value is the estimated mean response of the \\(i\\)th observation. The \\(i\\)th residual is defined as \\[\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i.\\] The \\(i\\)th residual is the difference between the response and estimated mean response of observation \\(i\\). The RSS of a regression model is the sum of its squared residuals. The RSS for a simple linear regression model, as a function of the estimated regression coefficients, is \\[\\begin{align*} RSS(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp;= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - \\hat{E}(Y|X=x_i))^2 \\\\ &amp;= \\sum_{i=1}^n (Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2. \\end{align*}\\] The fitted model is the estimated model that minimizes the RSS. In a simple linear regression context, the fitted model is known as the line of best fit. In Figure 17.1, we attempt to visualize the response values, fitted values, residuals, and line of best fit in a simple linear regression context. Notice that: The fitted values are the value returned by the line of best fit when it is evaluated at the observed predictor values. Alternatively, the fitted value for each observation is the y-value obtained when intersecting the line of best fit with a vertical line drawn from each observed predictor value. The residual is the vertical distance between each response value and the fitted value. The RSS seeks to minimize the sum of the squared vertical distances between the response and fitted values. Figure 17.1: Visualization of the response values, fitted values, residuals, and line of best fit. 17.10.1 Visualizing the RSS as a function of the estimated coefficients As we have attempted to emphasize through its notation, \\(RSS(\\hat{\\beta}_0, \\hat{\\beta}_1)\\) is a function of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). OLS estimation for the simple linear regression model seeks to find the values of the estimated coefficients that minimize the \\(RSS(\\hat{\\beta}_0, \\hat{\\beta}_1)\\). In the example below, we visualize this three-dimensional surface to see how difficult it would be to optimize the RSS computationally . Consider the Pearson and Lees height data (PearsonLee in the HistData package) previously discussed. For that data set, we tried to model the childs height (child) based on the height of the childs parents (parent). Thus, our response variable is child and our predictor variable is parent. We seek to estimate the regression equation \\[E(\\mathtt{child} \\mid \\mathtt{parent}) = \\beta_0 + \\beta_1 \\mathtt{parent}\\] with the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the associated RSS. We first load the height data, extract the response and predictor and assign them the names y and x. # load height data data(PearsonLee, package = &quot;HistData&quot;) # extract response and predictor variables from data set y &lt;- PearsonLee$child x &lt;- PearsonLee$parent We now create a function that computes the RSS as a function of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) (called b0 and b1, respectively in the code below). The function takes the vector b = c(b0, b1), extracts b0 and b1 from this vector, computes the fitted values (yhat) for the provided b0 and b1, computes the corresponding residuals (ehat), and the returns the sum of the squared residuals, i.e., the RSS. # function to compute the RSS # b = c(b0, b1) compute_rss &lt;- function(b) { b0 = b[1] # extract b0 from b b1 = b[2] # extract b1 from b yhat &lt;- b0 + b1 * x # compute fitted values ehat &lt;- y - yhat # compute residuals return(sum(ehat^2)) # return RSS } Next, we specify sequences of b0 and b1 values to consider for optimizing the RSS. We create a matrix, rss_mat to store the computed RSS for each combination of b0 and b1. We then use a double for loop to evaluate the RSS for each combination of b0 and b1 in our sequences. # sequences of candidate b0 and b1 values b0_seq &lt;- seq(41.06, 41.08, len = 101) b1_seq &lt;- seq(0.383, 0.385, len = 101) # matrix to store rss values rss_mat &lt;- matrix(nrow = length(b0_seq), ncol = length(b1_seq)) # use double loop to compute RSS for all combinations of b0_seq and b1_seq # seq_along(b0_seq) returns the vector 1:length(b0_seq), but is safer for (i in seq_along(b0_seq)) { for (j in seq_along(b1_seq)) { rss_mat[i, j] &lt;- compute_rss(c(b0_seq[i], b1_seq[j])) } } We draw a contour plot of the RSS surface using the contour function. # draw a contour plot of the RSS surface contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = &quot;b0&quot;, ylab = &quot;b1&quot;) title(&quot;RSS surface of Pearson and Lee height data&quot;) A contour plot uses contour lines to describe the height of the \\(z\\) dimension of a 3-dimensional \\((x, y, z)\\) surface. Each line/contour indicates the height of the surface along that line. Note that in the graphic above, the contours are basically straight lines. Theres no easily identifiable combinations of b0 and b1 the produce the minimum RSS. We can approximate the optimal values of b0 and b1 that minimize the RSS through the optim function. The optim function takes two main arguments: par: a vector of starting values for the optimization algorithm. In our case, this will be the starting values for b0 and b1. fn: a function of par to minimize. The optim function will return a list with several pieces of information (see ?stats::optim) for details. We want the par component of the returned list, which is the par vector that (approximately) minimizes fn. We then use the points function to plot the optimal values of b0 and b1 that minimize the RSS. # use the optim function to find the values of b0 and b1 that minimize the RSS # par is the vector of initial values # fn is the function to minimize # $par extracts the values found by optim to minimize fn optimal_b &lt;- optim(par = c(41, 0.4), fn = compute_rss)$par # print the optimal values of b optimal_b #&gt; [1] 41.0655877 0.3842737 # plot optimal value as an X on the contour plot contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = &quot;b0&quot;, ylab = &quot;b1&quot;) title(&quot;RSS surface of Pearson and Lee height data&quot;) points(x = optimal_b[1], y = optimal_b[2], pch = 4) What is our takeaway from this example? Its probably not ideal to numerically search for the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize \\(RSS(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1)\\). Instead, we should seek an exact solution using mathematics. 17.10.2 OLS estimators of the simple linear regression parameters Define \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\) and \\(\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\). The OLS estimators of the regression coefficients for a simple linear regression coefficients are \\[\\begin{align*} \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i=1}^n x_i Y_i - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)\\biggl(\\sum_{i=1}^n Y_i\\biggr)}{\\sum_{i=1}^n x_i^2 - \\frac{1}{n} \\biggl(\\sum_{i=1}^n x_i\\biggr)^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\ &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})Y_i}{\\sum_{i=1}^n (x_i - \\bar{x})x_i} \\end{align*}\\] and \\[\\begin{equation} \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{x}. \\end{equation}\\] Thought its already been said, we state once again that the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\) shown above are the estimators that minimize the RSS. The other parameter weve discussed is the error variance, \\(\\sigma^2\\). The most common estimator of the error variance is \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{RSS}{n-p}, (\\#eq:sigmasq-hat) \\end{equation}\\] where \\(p\\) is the number of regression coefficients. In general, \\(n-p\\) is the degrees of freedom of the RSS. In a simple linear regression context, the denominator of (5.9) is \\(n-2\\). 17.11 Penguins simple linear regression example We will use the penguins data set in the palmerpenguins package (Horst, Hill, and Gorman 2020) to illustrate a very basic simple linear regression analysis. The penguins data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by Gorman, Williams, and Fraser (2014). We start by loading the data into memory. data(penguins, package = &quot;palmerpenguins&quot;) The data set includes 344 observations of 8 variables. The variables are: species: a factor indicating the penguin species island: a factor indicating the island the penguin was observed bill_length_mm: a numeric variable indicating the bill length in millimeters bill_depth_mm: a numeric variable indicating the bill depth in millimeters flipper_length_mm: an integer variable indicating the flipper length in millimeters body_mass_g: an integer variable indicating the body mass in grams sex: a factor indicating the penguin sex (female, male) year: an integer denoting the study year the penguin was observed (2007, 2008, or 2009) We begin by creating a scatter plot of bill_length_mm versus body_mass_g (y-axis versus x-axis) in Figure 17.2. We see a clear positive association between body mass and bill length: as the body mass increases, the bill length tends to increase. The pattern is linear, i.e., roughly a straight line. plot(bill_length_mm ~ body_mass_g, data = penguins, ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;, main = &quot;Penguin size measurements&quot;) Figure 17.2: A scatter plot of penguin bill length (mm) versus body mass (g) We first perform a single linear regression analysis manually using the equations previously provided by regressing bill_length_mm on body_mass_g. Using the summary function on the penguins data frame, we see that both bill_length_mm and body_mass_g have NA values. summary(penguins) #&gt; species island bill_length_mm bill_depth_mm #&gt; Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10 #&gt; Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60 #&gt; Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30 #&gt; Mean :43.92 Mean :17.15 #&gt; 3rd Qu.:48.50 3rd Qu.:18.70 #&gt; Max. :59.60 Max. :21.50 #&gt; NA&#39;s :2 NA&#39;s :2 #&gt; flipper_length_mm body_mass_g sex year #&gt; Min. :172.0 Min. :2700 female:165 Min. :2007 #&gt; 1st Qu.:190.0 1st Qu.:3550 male :168 1st Qu.:2007 #&gt; Median :197.0 Median :4050 NA&#39;s : 11 Median :2008 #&gt; Mean :200.9 Mean :4202 Mean :2008 #&gt; 3rd Qu.:213.0 3rd Qu.:4750 3rd Qu.:2009 #&gt; Max. :231.0 Max. :6300 Max. :2009 #&gt; NA&#39;s :2 NA&#39;s :2 We want to remove the rows of penguins where either body_mass_g or bill_length_mm have NA values. We do that below using the na.omit function (selecting only the relevant variables) and assign the cleaned object the name penguins_clean. # remove rows of penguins where bill_length_mm or body_mass_g have NA values penguins_clean &lt;- na.omit(penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)]) We extract the bill_length_mm variable from the penguins data frame and assign it the name y since it will be the response variable. We extract the body_mass_g variable from the penguins data frame and assign it the name y since it will be the predictor variable. We also determine the number of observations and assign that value the name n. # extract response and predictor from penguins_clean y &lt;- penguins_clean$bill_length_mm x &lt;- penguins_clean$body_mass_g # determine number of observations n &lt;- length(y) We now compute \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\). Note that placing () around the assignment operations will both perform the assign and print the results. # compute OLS estimates of beta1 and beta0 (b1 &lt;- (sum(x * y) - sum(x) * sum(y) / n)/(sum(x^2) - sum(x)^2/n)) #&gt; [1] 0.004051417 (b0 &lt;- mean(y) - b1 * mean(x)) #&gt; [1] 26.89887 The estimated value of \\(\\beta_0\\) is \\(\\hat{\\beta}_0=26.90\\) and the estimated value of \\(\\beta_1\\) is \\(\\hat{\\beta}_1=0.004\\). The basic mathematical interpretation of our results is that: (\\(\\hat{\\beta}_1\\)): If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguins bill length to be 0.004 millimeters longer. (\\(\\hat{\\beta}_0\\)):A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters. The latter interpretation is clearly non-sensical and is caused by the fact that we are extrapolating far outside the observed body mass values. The relationship between body mass and bill length is different for penguin chicks versus adults. We can use the abline function to overlay the fitted model on the observed data. Note that in simple linear regression, \\(\\hat{\\beta}_1\\) corresponds to the slope of the fitted line and \\(\\hat{\\beta}_0\\) will be the intercept. plot(bill_length_mm ~ body_mass_g, data = penguins, ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;, main = &quot;Penguin size measurements&quot;) # a is the intercept and b is the slope abline(a = b0, b = b1) The fit of the model to our observed data seems reasonable. We can also compute the residuals, \\(\\hat{\\epsilon}_1,\\ldots,\\hat{\\epsilon}_n\\), the fitted values \\(\\hat{y}_1,\\ldots,\\hat{y}_n\\), and the associated RSS, \\(RSS=\\sum_{i=1}^n \\hat{\\epsilon}_i^2\\). yhat &lt;- b0 + b1 * x # compute fitted values ehat &lt;- y - yhat # compute residuals (rss &lt;- sum(ehat^2)) # sum of the squared residuals #&gt; [1] 6564.494 (sigmasqhat &lt;- rss/(n-2)) # estimated error variance #&gt; [1] 19.30734 17.12 Fitting a linear model using R We now describe how to use R to fit a linear model to data. The lm function uses OLS to fit a linear model to data. The function has two major arguments: data: the data frame in which the model variables are stored. This can be omitted if the variables are already stored in memory. formula: a Wilkinson and Rogers (1973) style formula describing the linear regression model. Assuming the y is the response, x, x1, x2, x3 are available numeric predictors: y ~ x describes a simple linear regression model based on \\(E(Y|X)=\\beta_0+\\beta_1 X\\). y ~ x1 + x2 describes a multiple linear regression model based on \\(E(Y|X_1, X_2)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2\\). y ~ x1 + x2 + x1:x2 and y ~ x1 * x2 describe a multiple linear regression model based on \\(E(Y|X_1, X_2)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2\\). y ~ -1 + x1 + x2 describes a multiple linear regression model without an intercept, in this case, \\(E(Y|X_1, X_2)=\\beta_1 X_1 + \\beta_2 X_2\\). y ~ x + I(x^2) describe a multiple linear regression model based on \\(E(Y|X)=\\beta_0+\\beta_1 X + \\beta_2 X^2\\). We fit a linear model regressing body_mass_g on bill_length_mm using the penguins data frame and store it in the object lmod. lmod is an object of class lm. lmod &lt;- lm(bill_length_mm ~ body_mass_g, data = penguins) # fit model class(lmod) # class of lmod #&gt; [1] &quot;lm&quot; There are a number of methods (generic function that do something specific when applied to a certain type of object). Commonly used ones include: residuals: extracts \\(\\hat{\\boldsymbol{\\epsilon}}\\) from an lm object. fitted: extracts \\(\\hat{\\mathbf{y}}\\) from an lm object. coef or coefficients: extracts \\(\\hat{\\boldsymbol{\\beta}}\\) from an lm object. deviance: extracts the RSS from an lm object. sigma: extracts \\(\\hat{\\sigma}\\) from an lm object. df.residual: extracts \\(n-p\\), the degrees of freedom for the RSS, from an lm object. summary: provides: A 5-number summary of the \\(\\hat{\\boldsymbol{\\epsilon}}\\) A table that lists the predictors, the Estimate of the associated coefficients, the estimated standard error of the estimates (Std.Error), the computed test statistic associated with testing \\(H_0: \\beta_j = 0\\) versus \\(H_a: \\beta_j \\neq 0\\) for \\(j=0,1,\\ldots,p-1\\) (t value), and the associated p-value of each test Pr(&gt;|t|). We now use some of the methods to extract important characteristics of our fitted model. We then check whether the values obtained from these methods match our manual calculations. (coeffs2 &lt;- coefficients(lmod)) # extract, assign, and print coefficients #&gt; (Intercept) body_mass_g #&gt; 26.898872424 0.004051417 ehat2 &lt;- residuals(lmod) # extract and assign residuals yhat2 &lt;- fitted(lmod) # extract and assign fitted values rss2 &lt;- deviance(lmod) # extract and assign rss sigmasqhat2 &lt;- rss2/df.residual(lmod) # estimated error variance # compare to manually computed values all.equal(c(b0, b1), coeffs2, check.attributes = FALSE) #&gt; [1] TRUE all.equal(ehat, ehat2, check.attributes = FALSE) #&gt; [1] TRUE all.equal(rss, rss2) #&gt; [1] TRUE all.equal(sigmasqhat, sigmasqhat2) #&gt; [1] TRUE # methods(class=&quot;lm&quot;) 17.12.1 Derivation of OLS simple linear regression estimators Use calculus to derive the OLS estimator of the regression coefficients. Take the partial derivatives of \\(RSS(\\hat{\\beta}_0, \\hat{\\beta}_1)\\) with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), set the derivatives equal to zero, and solve for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to find the critical points of \\(RSS(\\hat{\\beta}_0, \\hat{\\beta}_1)\\). Technically, you must show that the Hessian matrix of \\(RSS(\\hat{\\beta}_0, \\hat{\\beta}_1)\\) is positive definite to verify that our solution minimizes the RSS, but we wont do that here. \\[\\\\[4in]\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
