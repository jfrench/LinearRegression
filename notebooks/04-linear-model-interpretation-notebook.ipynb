{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4 - Linear Model Interpretation\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click the Open in Colab graphic below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/04-linear-model-interpretation-notebook.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"> </a>\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "44a60e43-8540-4439-827f-23e4160d90f7"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(!require(palmerpenguins, quietly = TRUE)) {\n",
        "  install.packages(\"palmerpenguins\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(palmerpenguins)\n",
        "}"
      ],
      "id": "37f4c96b-8fda-4223-9d34-53395486b254"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(!require(car, quietly = TRUE)) {\n",
        "  install.packages(\"car\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(car)\n",
        "}"
      ],
      "id": "6da0af4c-22bc-4537-81dc-ae90dee55ed2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "if(!require(effects, quietly = TRUE)) {\n",
        "  install.packages(\"effects\", repos = \"https://cran.rstudio.com/\")\n",
        "  library(effects)\n",
        "}"
      ],
      "id": "0e637fb9-e00b-4d20-a6c0-a0af27255e8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpretation of coefficients\n",
        "\n",
        "The standard approach to interpreting the coefficients of a fitted linear model is to consider the expected change in the response in relation to changes in the regressors in the model.\n",
        "\n",
        "Consider the typical multiple linear regression model of the response\n",
        "\n",
        "$$\n",
        "Y=\\beta_0+\\beta_1 X_1 +\\ldots + \\beta_{p-1}X_{p-1}+\\epsilon.\n",
        "$$\n",
        "\n",
        "-   We treat the values of our regressor variables as being fixed, known values\n",
        "-   The error term is treated as a random variable\n",
        "-   Consequently, the response variable is also a random variable.\n",
        "\n",
        "**Random Error Assumption**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We assume that the errors all have mean 0, conditional on the values of the regressor variables.\n",
        "\n",
        "$$\n",
        "E(\\epsilon \\mid X_1, X_2, \\ldots, X_{p-1})=0.\n",
        "$$ Or, using alternative notation:\n",
        "\n",
        "$$\n",
        "E(\\epsilon \\mid \\mathbb{X})=0.\n",
        "$$\n",
        "\n",
        "**Expected Value of Response**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Using the assumption of mean zero errors, we have:\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Note: All the coefficient $\\beta_i$ terms are fixed, non-random.\n",
        "\n",
        "**Interpretation for simple linear regression**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Suppose we have a simple linear regression model, so that $$\n",
        "E(Y\\mid X)=\\beta_0 + \\beta_1 X.\n",
        "$$ The interpretations of the coefficients are:\n",
        "\n",
        "-   $\\beta_0$ is the expected response when the regressor is 0, i.e., $\\beta_0=E(Y\\mid X=0)$.\n",
        "-   $\\beta_1$ is the expected change in the response when the regressor increases 1 unit, i.e., $\\beta_1=E(Y\\mid X=x^*+1)-E(Y\\mid X=x^*)$, where $x^*$ is a fixed, real number.\n",
        "\n",
        "**Interpretation of Intercept**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The intercept term $\\beta_0$ is the expected value of the response when $X= 0$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Interpretation of Slope**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Similarly, for $\\beta_1$, we notice that\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Thus, $\\beta_1$ literally equals the change in the expected response when the regressor increases by 1 unit.\n",
        "\n",
        "It may not make sense to say “we increase $X$ by 1 unit” or “when $X$ increases by 1 unit”. For example??\n",
        "\n",
        "To illustrate the interpretations given above, we interpret the simple linear regression model fit to the `penguins` data. The fitted simple linear regression model of `body_mass_g` regressed on `body_mass_g` is $$\n",
        "\\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g})=26.9+0.004 \\,\\mathtt{body\\_mass\\_g}.\n",
        "$$\n",
        "\n",
        "Some basic interpretations of the coefficients are:\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Rescaling Predictors to aid Interpretation**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "-   A weight difference of 1 gram is negligible in the context of penguin weights.\n",
        "-   A bill length change of 0.004 mm is unlikely to be noticed.\n",
        "\n",
        "In the code below, we divide the `body_mass_g` variable by 1000 to convert the variable from grams to kilograms.\n",
        "\n",
        "We then fit the model regressing `bill_length_mm` on `body_mass_kg` and extract the estimated coefficients."
      ],
      "id": "6e12aeae-1811-48b2-8679-638adc0dc13b"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load penguins data\n",
        "data(penguins, package = \"palmerpenguins\")\n",
        "# transform body mass variable from g to kg\n",
        "penguins <- penguins |> transform(body_mass_kg = body_mass_g/1000)\n",
        "# fit model with body_mass_kg\n",
        "slmod_scaled <- lm(bill_length_mm ~ body_mass_kg, data = penguins)\n",
        "# extract coefficients\n",
        "coefficients(slmod_scaled)"
      ],
      "id": "ad839530-ba03-4019-b687-70d9ce69a66f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**\n",
        "\n",
        "-   How do we reinterpret the model in the context of kilograms?\n",
        "\n",
        "Dividing `body_mass_g` by 1000 resulted in the estimated coefficient changing by a factor of 1000.\n",
        "\n",
        "More generally, if $\\hat{\\beta}_j$ is the estimated coefficient for $X_j$, then the regressor $(X_j + a)/c$ will have an estimated coefficient of $c\\hat{\\beta}_j$, where $a$ and $c$ are fixed, real numbers and assuming nothing else in the fitted model changes.\n",
        "\n",
        "**Interpretation for first-order multiple linear regression models**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Suppose we have a multiple linear regression model with $p-1$ *numeric* regressors, so that\n",
        "\n",
        "$$\n",
        "E(Y\\mid X_1,\\ldots,X_{p-1})=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1}.\n",
        "$$\n",
        "\n",
        "Relying on the definition of $\\mathbb{X}$, we denote the set of regressors without $X_j$ as $\\mathbb{X}_{-j} = \\mathbb{X}\\setminus\\{X_j\\}$.\n",
        "\n",
        "The interpretations of the coefficients from the model in Equation are:\n",
        "\n",
        "-   $\\beta_0$ is the expected response when all regressors are 0, i.e., $\\beta_0=E(Y\\mid X_1=0,\\ldots,X_{p-1}=0)$.\n",
        "-   $\\beta_j$, $j = 1,\\ldots,p-1$, represents the expected change in the response when regressor $j$ increases 1 unit and the other regressors stay the same, i.e., $\\beta_j=E(Y\\mid \\mathbb{X}_{-j} = \\mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*+1)-E(Y\\mid \\mathbb{X}_{-j} = \\mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*)$ where $\\mathbf{x}_{-j}^*=[x^*_1,\\ldots,x_{j-1}^*,x_{j+1}^*,\\ldots,x_{p-1}^*]\\in \\mathbb{R}^{p-2}$ is a vector with $p-2$ fixed values (the number of regressors excluding $X_j$) and $x_j^*$ is a fixed real number. The non-intercept coefficients of a multiple linear regression model are known as *partial slopes*.\n",
        "\n",
        "Regarding the interpretation of $\\beta_0$, from the regression model:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E(Y\\mid X_1=0,\\ldots,X_{p-1}=0) &= \\beta_0 + \\beta_1 \\cdot 0 + \\cdots + \\beta_{p-1} \\cdot 0\\\\\n",
        "&= \\beta_0.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Question:**\n",
        "\n",
        "-   Does the interpretation of the intercept always make sense?\n",
        "-   What if $X_1$ was heart rate?\n",
        "\n",
        "**Issues Interpreting Polynomial Models**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "In multiple regression models, a single predictor can be used more than once in the model.\n",
        "\n",
        "E.g., in the 2nd-degree polynomial regression model:\n",
        "\n",
        "$$E(Y\\mid X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2,$$\n",
        "\n",
        "$X$ is used in both the second and third terms.\n",
        "\n",
        "**Questions**\n",
        "\n",
        "-   How does this affect the interpretation of $\\beta_1$?\n",
        "-   Is is possible to increase $X$ while keeping $X^2$ fixed?\n",
        "\n",
        "The standard interpretation we discussed is applicable to first-order linear regression models.\n",
        "\n",
        "*First-order linear regression model:* no regressor is a function of any other regressor.\n",
        "\n",
        "**Penguins Example Revisited**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We interpret the first-order multiple linear regression model fit to the `penguins` data. The fitted multiple linear regression model is:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g}, \\mathtt{flipper\\_length\\_mm})\\\\\n",
        "&=-3.44+0.0007 \\,\\mathtt{body\\_mass\\_g}+0.22\\,\\mathtt{flipper\\_length\\_mm}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Some basic interpretations of the coefficients are:\n",
        "\n",
        "-   *Intercept:* We expect a penguin with a body mass of 0 grams and a flipper length of 0 mm to to have a bill length of -3.44 mm.\n",
        "-   `body_mass_g`: If we increase body mass by 1 gram, we expect bill length to increase 0.0007 mm *holding all else constant*.\n",
        "-   `flipper_length_mm`: If we increase flipper length by 1 mm, we expect the bill length to increase 0.22 mm *holding all else constant*.\n",
        "\n",
        "**Roles of regressor variables**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Did you notice that the estimated coefficients for the intercept and the `body_mass_g` regressor changed between the simple model and the multiple regression model?\n",
        "\n",
        "| Regression Model | Intercept Coef. | Body Mass Coef. |\n",
        "|:-----------------|----------------:|----------------:|\n",
        "| Simple Linear    |           26.90 |           0.004 |\n",
        "| Multiple         |           -3.44 |          0.0007 |\n",
        "\n",
        "-   Why?\n",
        "\n",
        "The role a regressor plays in a regression model depends on what other regressors are in the model.\n",
        "\n",
        "-   Generally, we can’t provide a definitive interpretation of a regressor’s role in a fitted model without knowing what other regressors are in the model.\n",
        "-   When interpreting a regressor, it is common to include something like *after accounting for the other variables in the model*.\n",
        "\n",
        "If our model had different variables, then our interpretation would be different!\n",
        "\n",
        "Mathematically, why do the estimated coefficients change as we add or remove regressors from a model?\n",
        "\n",
        "-   If a regressor is correlated with other regressors in a model, then adding or removing that regressor will impact the estimated coefficients in the new model.\n",
        "-   The more correlated the regressors are, the more they tend to affect each others’ estimated coefficients.\n",
        "-   A regressor will impact the estimated coefficients of the other regressors in a model unless it is *orthogonal* to the other regressors.\n",
        "\n",
        "*Note:* Orthogonality is related to correlation, but there are important differences.\n",
        "\n",
        "# Effect plots\n",
        "\n",
        "An effect plot is a visual display that aids in helping us intuitively interpret the impact of a *predictor* in a model. As stated by Fox et al. (2020):\n",
        "\n",
        "> Summarization of the effects of predictors using tables of coefficient estimates is often incomplete. Effects, and particularly plots of effects, can in many instances reveal the relationship of the response to the predictors more clearly. This conclusion is especially true for models with linear predictors that include interactions and multiple-coefficient terms such as regression splines and polynomials ….\n",
        "\n",
        "An *effect plot* is a plot of the estimated mean response as a function of a *focal predictor* with the other *predictors* being held at “typical values”.\n",
        "\n",
        "*Recall:* the difference between predictors and regressors.\n",
        "\n",
        "-   A *predictor* variable is a variable available to model the response variable.\n",
        "-   A *regressor* variable is a variable used in our regression model, whether that is an unmodified predictor variable, some transformation of a predictor, some combination of predictors, etc.\n",
        "\n",
        "**First-Order Model Effects Plot**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "In a first-order linear model, none of the regressors interact, i.e., none of the regressors are functions of each other. Fox et al. (2020) use the terminology *fixed group* to refer to the group of predictors that do not interact with the focal predictor.\n",
        "\n",
        "To create our effect plot, we must first find the equation for the estimated mean response as a function of a focal predictor while holding the other predictors at their “typical” values. We set numeric fixed group predictors equal to their sample means when finding this function.\n",
        "\n",
        "We now construct effect plots for the estimated regression model of the `penguins` data that regressed `bill_length_mm` on `body_mass_g` and `flipper_length_mm`. The fitted model is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g}, \\mathtt{flipper\\_length\\_mm})\\\\\n",
        "&=-3.44+0.0007 \\,\\mathtt{body\\_mass\\_g}+0.22\\,\\mathtt{flipper\\_length\\_mm}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We fit this model in R below."
      ],
      "id": "2e62a1cf-2063-4be5-8b35-241ea52aaeb5"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load penguins data since it hasn't been loaded in this chapter\n",
        "data(penguins, package = \"palmerpenguins\")\n",
        "# refit multiple linear regression model\n",
        "mlmod <- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, data = penguins)\n",
        "# extract estimated coefficients\n",
        "coef(mlmod)"
      ],
      "id": "7eac2a48-416d-4e86-830e-f2639fba339c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are two predictors in the model, so we can create effect plots for both variables using the sample mean as a reference value."
      ],
      "id": "120b3ec9-e617-45b1-9209-1d3fee0d28c8"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "colMeans(mlmod$model)"
      ],
      "id": "59c23ef7-b2ba-444c-9919-78550fef1fe6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The effect plot for $\\mathtt{body\\_mass\\_g}$ (on the response $\\mathtt{bill\\_length\\_mm}$) is a plot of $$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g}, \\mathtt{flipper\\_length\\_mm} = 200.92)\\\\\n",
        "&=-3.44+0.0007 \\,\\mathtt{body\\_mass\\_g}+0.22\\cdot 200.92 \\\\\n",
        "&=41.14+0.0007 \\,\\mathtt{body\\_mass\\_g}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "as a function of $\\mathtt{body\\_mass\\_g}$.\n",
        "\n",
        "Note: we used exact values in the calculation above. The intercept will be 40.76 instead of 41.14 if we use the rounded values. Similarly, the effect plot for $\\mathtt{flipper\\_length\\_mm}$ is a plot of $$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g} = 4201.75,\\mathtt{flipper\\_length\\_mm}) \\\\\n",
        "&=-3.44+0.0007 \\cdot 4201.75+0.22\\,\\mathtt{flipper\\_length\\_mm} \\\\\n",
        "&=-0.65 + 0.22\\,\\mathtt{flipper\\_length\\_mm}\n",
        "\\end{aligned}\n",
        "$$ as a function of $\\mathtt{flipper\\_length\\_mm}$.\n",
        "\n",
        "The `effects` R package can be used to generate effect plots for the predictors of a fitted linear model.\n",
        "\n",
        "-   We use the `effects::predictorEffect` function to compute the information needed to draw the plot.\n",
        "-   We use the `plot` function to display the information.\n",
        "\n",
        "The `predictorEffect` function computes the estimated mean response for different values of the focal predictor while holding the other predictors at their typical values. The main arguments of `predictorEffect` are:\n",
        "\n",
        "-   `predictor`: the name of the predictor we want to plot. This is the “focal predictor”.\n",
        "-   `mod`: the fitted model. The function works with `lm` objects and many other types of fitted models.\n",
        "\n",
        "We load the **effects** package."
      ],
      "id": "d04ea5aa-478b-4fbb-83d4-008d50bb0276"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(effects)"
      ],
      "id": "2c14e92e-7a86-4f2a-91af-2082358410a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now draw the effects plot for `body_mass_g`."
      ],
      "id": "4e69dd4e-2491-4854-a402-feb9919e0d5b"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(predictorEffect(\"body_mass_g\", mlmod))"
      ],
      "id": "70379e8e-4bef-4695-8604-869035d7c120"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a clear positive association between `body_mass_g` and `bill_length_mm` after accounting for the `flipper_length_mm` variable.\n",
        "\n",
        "The shaded area indicates the 95% confidence interval bands for the estimated mean response.\n",
        "\n",
        "We next create an effect plot for `flipper_length_mm` using the code below."
      ],
      "id": "d31a742e-ccdf-41e0-904f-aa25a29a9b33"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(predictorEffect(\"flipper_length_mm\", mlmod))"
      ],
      "id": "f7ba73c1-3478-4782-997b-db5ea0bdf4c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a clear positive association between `flipper_length_mm` and `bill_length_mm` after accounting for `body_mass_g`.\n",
        "\n",
        "Alternatively, we could use `effects::allEffects` to compute the necessary effect plot information for all predictors simultaneously, but it is harder to control figure sizing."
      ],
      "id": "5833ca6b-3c57-400c-9fb3-40e90dbf6037"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(allEffects(mlmod))"
      ],
      "id": "faa37b31-accf-4a19-86de-e8c1a0374194"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpretation for categorical predictors\n",
        "\n",
        "**Coefficient interpretation for parallel lines models**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Consider a parallel lines model with numeric regressor $X$ and categorical predictor $C$ with levels $L_1$, $L_2$, and $L_3$. Predictor $C$ must be transformed into two indicator variables, $D_2$ and $D_3$, for category levels $L_2$ and $L_3$, to be included in our linear model. $L_1$ is the reference level. The parallel lines model is formulated as $$\n",
        "E(Y \\mid X, C) = \\beta_{int} + \\beta_{X} X + \\beta_{L_2} D_2 +  \\beta_{L_3} D_3\n",
        "$$\n",
        "\n",
        "We replaced the usual $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ with notation the indicates the regressor each coefficient is associated with.\n",
        "\n",
        "When an observation has level $L_1$ and $X=0$, then the expected response is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Thus, $\\beta_{int}$ is the expected response for an observation with level $L_1$ when $X=0$.\n",
        "\n",
        "When an observation has a fixed level $L_j$ and $X$ increases from $x^*$ to $x^*+1$, then the change in the expected response is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Thus, $\\beta_X$ is the expected change in the response for an observation with fixed level $L_j$ when $X$ increases by 1 unit.\n",
        "\n",
        "When an observation has level $L_2$, the expected response is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Thus, $\\beta_{L_2}$ is the expected change in the response for a fixed value of $X$ when comparing on observation having level $L_1$ to level $L_2$ of predictor $C$. More specifically, $\\beta_{L_2}$ indicates the distance between the estimated regression lines for observations having levels $L_1$ and $L_2$.\n",
        "\n",
        "To summarize, assuming categorical predictor $C$ has $K$ levels instead of 3:\n",
        "\n",
        "-   $\\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.\n",
        "-   $\\beta_X$ is the expected change in the response when $X$ increases by 1 unit for a fixed level of $C$.\n",
        "-   $\\beta_{L_j}$, for $j=2,\\ldots,K$, represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X$ fixed at the same value.\n",
        "\n",
        "We previously fit a parallel lines model to the `penguins` data that used both `body_mass_g` and `species` to explain the behavior of `bill_length_mm`. Letting $D_C$ denote the indicator variable for the `Chinstrap` level and $D_G$ denote the indicator variable for the `Gentoo` level, the fitted parallel lines model was\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species})\\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 D_C + 3.56 D_G.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "In the context of this model:\n",
        "\n",
        "-   The expected bill length for an Adelie penguin with a body mass of 0 grams is 24.92 mm.\n",
        "-   If two penguins are of the same species, but one penguin has a body mass 1 gram larger, then the larger penguin is expected to have a bill length 0.004 mm longer than the smaller penguin.\n",
        "-   A Chinstrap penguin is expected to have a bill length 9.92 mm longer than an Adelie penguin, assuming their body mass is the same.\n",
        "-   A Gentoo penguin is expected to have a bill length 3.56 mm longer than an Adelie penguin, assuming their body mass is the same.\n",
        "\n",
        "**Effect plots for fitted models with non-interacting categorical predictors**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "How do we create an effect plot for a numeric focal predictor when a non-interacting categorical predictor is in the model (such as for the parallel lines model we have been discussing)?\n",
        "\n",
        "-   First, determine the fitted model as a function of the focal predictor for each level of the categorical predictor.\n",
        "-   Then, we compute the weighted average of the equation with the weights being proportional to the number of observations in each group.\n",
        "\n",
        "Let’s construct an effect plot for the `body_mass_g` predictor in the context of the `penguins` parallel lines model discussed in the previous section. We previously determined that the fitted parallel lines model simplified to:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Adelie}) \\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g} \\\\\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Chinstrap}) \\\\\n",
        "&= 34.84 + 0.004 \\mathtt{body\\_mass\\_g} \\\\\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Gentoo}) \\\\\n",
        "&= 28.48 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We recreate the fitted model producing these equations in R using the code below."
      ],
      "id": "ede66f00-6bcc-4816-941e-e062dddd2136"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# refit the parallel lines model\n",
        "lmodp <- lm(bill_length_mm ~ body_mass_g + species, data = penguins)\n",
        "# double-check coefficients\n",
        "coef(lmodp)"
      ],
      "id": "121f0d2b-a78e-49b1-a9b8-d8625a50ab9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below determines the number of observations with each level of `species` for the data used in the fitted model `lmodp`."
      ],
      "id": "f55f176c-1feb-4414-a13e-0411507482ae"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "table(lmodp$model$species)\n",
        "print(paste('Total:', length(lmodp$model$species)))"
      ],
      "id": "0b19494c-5218-4347-8e1d-7d7e00b2a68f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The equation used to create the effect plot of `body_mass_g` is the weighted average of the response, with weights proportional to the number of observational having each level of the categorical predictor. Specifically,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{typical}) \\\\\n",
        "&= \\frac{151}{342}(24.92 + 0.004 \\mathtt{body\\_mass\\_g})\\\\\n",
        "&\\quad + \\frac{68}{342}(34.84 + 0.004 \\mathtt{body\\_mass\\_g})\\\\\n",
        "&\\quad + \\frac{123}{342}(28.48 + 0.004 \\mathtt{body\\_mass\\_g}) \\\\\n",
        "&=28.17 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The following code produces the effect plot for `body_mass_g` for the fitted parallel lines model. The association between `bill_length_mm` and `body_mass_g` is positive after accounting for `species`."
      ],
      "id": "9833c3bd-104a-4b68-87d4-6311dafde624"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# draw effect plot for body_mass_g\n",
        "plot(predictorEffect(\"body_mass_g\", lmodp))"
      ],
      "id": "e67f0f5d-e937-409d-ac9a-166e7fd74748"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An effect plot for a categorical predictor, assuming all other predictors in the model are non-interacting numerical predictors (i.e., fixed group predictors), is a plot of the estimated mean response for each level of the categorical variable when the fixed group group predictors are held at their sample mean. The sample mean is:"
      ],
      "id": "19baacc1-5648-4e45-a2f4-933b70898201"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample mean of body_mass_g variable used to fit lmodp\n",
        "mean(lmodp$model$body_mass_g)"
      ],
      "id": "78650e9d-ff34-4b91-a87d-7fbe6438deaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimated mean for the Adelie species when `body_mass_g` is fixed at 4201.75 is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g} = 4201.75, \\mathtt{species}=\\mathtt{Adelie}) \\\\\n",
        "&= 24.92 + 0.004 \\cdot 4201.75 \\\\\n",
        "&= 40.67.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Similarly,\n",
        "\n",
        "$$\n",
        "\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g} = 4201.75, \\mathtt{species}=\\mathtt{Chinstrap}) = 50.59\n",
        "$$\n",
        "\n",
        "And,\n",
        "\n",
        "$$\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g} = 4201.75, \\mathtt{species}=\\mathtt{Gentoo}) = 44.23$$\n",
        "\n",
        "The code below produces the effect plot for `species`. The confidence bands for the estimated mean response are shown by the vertical bars."
      ],
      "id": "c2e5093d-665a-4dd4-9f42-ae2759529948"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(predictorEffect(\"species\", lmodp))"
      ],
      "id": "501491b6-35d1-4bdd-a77f-2eb9443a7fd9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Coefficient interpretation for separate lines models**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Consider a separate lines model with numeric regressor $X$ and categorical predictor $C$ with levels $L_1$, $L_2$, and $L_3$. The predictor $C$ will be transformed into two indicator variables, $D_2$ and $D_3$, for category levels $L_2$ and $L_3$, with $L_1$ being the reference level. The separate lines model is formulated as: $$\n",
        "E(Y \\mid X, C) = \\beta_{int} + \\beta_{X} X + \\beta_{L_2} D_2 +  \\beta_{L_3} D_3 + \\beta_{XL_2} XD_2+\\beta_{XL_3}XD_3\n",
        "$$\n",
        "\n",
        "When an observation has level $L_1$ and $X=x^*$, then the expected response is\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "We can verify that:\n",
        "\n",
        "-   $\\beta_{int} = E(Y\\mid X = 0, C=L_1)$.\n",
        "-   $\\beta_{X} = E(Y\\mid X = x^* + 1, C=L_1) - E(Y\\mid X = x^*, C=L_1)$.\n",
        "\n",
        "Similarly, when $C=L_2$,\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Following this same pattern, when $C=L_3$ we have\n",
        "\n",
        "$$\n",
        "E(Y|X = x^*, C=L_3) = (\\beta_{int} + \\beta_{L_3}) + (\\beta_X + \\beta_{XL_3})x^*\n",
        "$$\n",
        "\n",
        "We can verify that for $j=2,3$,\n",
        "\n",
        "$$\\beta_{L_j}= E(Y\\mid X = 0, C=L_j) - E(Y\\mid X = 0, C=L_1)$$\n",
        "\n",
        "And, $$\n",
        "\\begin{aligned}\n",
        "& \\beta_{XL_j} \\\\\n",
        "&= [E(Y\\mid X = x^*+1, C=L_j) - E(Y\\mid X = x^*, C=L_j)]\\\\\n",
        "&\\quad-[E(Y\\mid X = x^*+1, C=L_1) - E(Y\\mid X = x^*, C=L_1)].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To summarize the interpretation of the coefficients in separate lines models, assuming categorical predictor $C$ has $K$ levels instead of 3:\n",
        "\n",
        "-   $\\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.\n",
        "-   $\\beta_{L_j}$, for $j=2,\\ldots,K$, represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X=0$.\n",
        "-   $\\beta_X$ represents the expected change in the response when $X$ increases by 1 unit for observations having the reference level.\n",
        "-   $\\beta_X L_j$, for $j=2,\\ldots,K$, represents the difference in the expected response between observations having the reference level in comparison to level $L_j$ when $X$ increases by 1 unit. More simply, these terms represent the difference in the rate of change for observations having level $L_j$ compared to the reference level.\n",
        "\n",
        "We illustrate these interpretation using the separate lines model to the `penguins` data. The fitted separate lines model was,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) \\\\\n",
        "&= 26.99 + 0.003 \\mathtt{body\\_mass\\_g} + 5.18 D_C - 0.25 D_G \\\\\n",
        "&\\quad + 0.001 D_C \\mathtt{body\\_mass\\_g} + 0.0009 D_G \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "In the context of this model:\n",
        "\n",
        "-   The expected bill length for an Adelie penguin with a body mass of 0 grams is 26.99 mm.\n",
        "-   If an Adelie penguin has a body mass 1 gram larger than another Adelie penguin, then the larger penguin is expected to have a bill length 0.003 mm longer than the smaller penguin.\n",
        "-   A Chinstrap penguin is expected to have a bill length 5.18 mm longer than an Adelie penguin when both have a body mass of 0 grams.\n",
        "-   A Gentoo penguin is expected to have a bill length 0.25 mm shorter than an Adelie penguin when both have a body mass of 0 grams.\n",
        "-   For each 1 gram increase in body mass, we expect the change in bill length by Chinstrap penguins to be 0.001 mm larger than the corresponding change in bill length by Adelie penguins.\n",
        "-   For each 1 gram increase in body mass, we expect the change in bill length by Gentoo penguins to be 0.0009 mm larger than the corresponding change in bill length by Adelie penguins.\n",
        "\n",
        "**Effect plots for interacting categorical predictors**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "We now discuss construction of effect plots for a separate lines model, which has an interaction between a categorical and numeric predictor.\n",
        "\n",
        "Fox et al. (2020) also discuss predictors in the *conditioning group*, which is the set of predictors that interact with the focal predictor.\n",
        "\n",
        "When some predictors interact with the focal predictor, the effect plot of the focal predictor is a plot of the estimated mean response when the fixed group predictors are held at their typical values and the conditioning group predictors vary over different combinations of discrete values.\n",
        "\n",
        "By default, to compute the estimated mean response as a function of the focal predictor, we:\n",
        "\n",
        "-   Hold the numeric fixed group predictors at their sample means.\n",
        "-   Average the estimated mean response equation across the different levels of a fixed group categorical predictor, with weights equal to the number of observations with each level.\n",
        "-   Compute the estimated mean response function for 5 discrete values of numeric predictors in the conditioning group.\n",
        "-   Compute the estimated mean response function for different levels of a categorical predictor in the conditioning group.\n",
        "\n",
        "We provide examples of the effect plots for the `body_mass_g` and `species` predictors for the separate lines model fit to the `penguins` data. We first run the code below to fit the separate lines model we previously fit:"
      ],
      "id": "4f9c2981-a61e-423c-b05e-e6778d864b71"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit separate lines model\n",
        "lmods <- lm(bill_length_mm ~ body_mass_g + species + body_mass_g:species,\n",
        "            data = penguins)\n",
        "# extract estimated coefficients\n",
        "coef(lmods)"
      ],
      "id": "6efa48c4-8444-489d-ac4b-7085333d1ce7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We previously determined that the model simplifies depending on the level of species:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Adelie}) \\\\\n",
        "&= 26.99 + 0.003 \\mathtt{body\\_mass\\_g},\\\\\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Chinstrap}) \\\\\n",
        "&= 31.17 + 0.004 \\mathtt{body\\_mass\\_g},\\\\\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}=\\mathtt{Chinstrap}) \\\\\n",
        "&= 26.74 + 0.004 \\mathtt{body\\_mass\\_g}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The effect plot of `body_mass_g` for the separate lines model is displayed using the code below.\n",
        "\n",
        "-   The `axes` argument to rotates the x-axis labels (otherwise the text overlaps)\n",
        "-   The `lines` argument displays all three lines in one graphic instead of a separate panel for each level of `species`."
      ],
      "id": "049040d3-609e-47e6-a353-d1a593d6a446"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# effect plot of body mass for separate lines model\n",
        "# axes ... rotates the x-axis labels 90 degrees\n",
        "# lines ... plots the effect of body mass in one graphic\n",
        "plot(predictorEffect(\"body_mass_g\", lmods),\n",
        "     axes = list(x = list(rotate = 90)),\n",
        "     lines = list(multiline = TRUE))"
      ],
      "id": "4a2e6ac4-3e21-4cd3-9295-bab131160858"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation:\n",
        "\n",
        "-   Chinstrap penguins tend to have the largest bill lengths for a given value of body mass and the bill lengths increase more quickly as a function of body mass then for the Adelie and Gentoo penguins.\n",
        "-   Adelie penguins tend to have the smallest bill length for a fixed value of body mass and the bill length tends to increase more slowly as body mass increases compared to the other two types of penguins.\n",
        "\n",
        "The effect plot of `species` for the separate lines model will be a plot of the estimated mean response for each level of `species` when varying `body_mass_g` over 5 discrete values.\n",
        "\n",
        "By specifying `lines = list(multiline = TRUE)`, the estimated mean responses for each level of `species` are connected for each discrete value of `body_mass_g`.\n",
        "\n",
        "This plot allows us to determine the effect of `species` on `bill_length_mm` when we vary `body_mass_g` over 5 discrete values."
      ],
      "id": "dc8ebfa7-c06f-4e21-9b74-39a6d6c15c60"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# effect plot of body mass for separate lines model\n",
        "# axes ... rotates the x-axis labels 90 degrees\n",
        "# lines ... plots the effect of species in one graphic\n",
        "plot(predictorEffect(\"species\", lmods),\n",
        "     axes = list(x = list(rotate = 90)),\n",
        "     lines=list(multiline = FALSE))"
      ],
      "id": "508ddbd9-fb9f-426a-8c81-215b3b539467"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation: When varying `body_mass_g` across the values 2700, 3600, 4500, 5300, and 6300 g, we see greater changes in the estimated mean of `bill_lengh_mm` for Chinstrap penguins in comparison to Adelie and Gentoo penguins.\n",
        "\n",
        "For more details about how to construct effect plots, run the following code to access the “vignette”."
      ],
      "id": "f6e8bc76-460e-43b3-bde6-52f7365e2c17"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "vignette(\"predictor-effects-gallery\", package = \"effects\")"
      ],
      "id": "1d43b78d-0975-4496-9a26-97f3768d9246"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Added-variable and leverage plots\n",
        "\n",
        "**Added-variable plots**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "As we saw before, an *effect plot* is a plot of the estimated mean relationship between the response and a focal predictor while holding the model’s predictors at typical values,\n",
        "\n",
        "An *added-variable plot* or *partial regression plot* displays the marginal effect of a regressor on the response after accounting for the other regressors in the model. An added-variable plot is a plot of two sets of residuals against one other.\n",
        "\n",
        "We create an added-variable plot for regressor $X_j$ in the following way:\n",
        "\n",
        "1.  Compute the residuals of the model regressing the response $Y$ on all regressors except $X_j$. We denote these residuals $\\hat{\\boldsymbol{\\epsilon}}(Y\\mid \\mathbb{X}_{-j})$. These residuals represent the part of the response variable not explained by the regressors in $\\mathbb{X}_{-j}$.\n",
        "2.  Compute the residuals of the model regressing the regressor $X_j$ on all regressors except $X_j$. We denote these residuals $\\hat{\\boldsymbol{\\epsilon}}(X_j \\mid \\mathbb{X}_{-j})$. These residuals represent the part of the $X_j$ not explained by the regressors in $\\mathbb{X}_{-j}$. Alternatively, these residuals represent the amount of additional information $X_j$ provides after accounting for the regressors in $\\mathbb{X}_{-j}$.\n",
        "3.  The added-variable plot for $X_j$ is a plot of $\\hat{\\boldsymbol{\\epsilon}}(Y\\mid \\mathbb{X}_{-j})$ on the y-axis and $\\hat{\\boldsymbol{\\epsilon}}(X_j \\mid \\mathbb{X}_{-j})$ on the x-axis.\n",
        "\n",
        "Added-variable plots allow us to visualize the impact a regressor has when added to an existing regression model. We can use the added-variable plot for $X_j$ to visually estimate the partial slope $\\hat{\\beta}_{j}$. The simple linear regression line that minimizes the RSS for the added-variable plot of $X_j$ will have slope $\\hat{\\beta}_j$.\n",
        "\n",
        "We can use an added-variable plot in several ways:\n",
        "\n",
        "1.  To assess the marginal relationship between $X_j$ and $Y$ after accounting for all of the other variables in the model.\n",
        "2.  To assess the strength of this marginal relationship.\n",
        "3.  To identify deficiencies in our fitted model.\n",
        "4.  To identify outliers and observations influential in determining the estimated partial slope.\n",
        "\n",
        "In regards to point 1 and 2:\n",
        "\n",
        "-   If the added-variable plot for $X_j$ is essentially a scatter of points with slope zero, then $X_j$ can do little to explain $Y$ after accounting for the other regressors.\n",
        "-   If the points in an added-variable plot for $X_j$ have a linear relationship, then adding $X_j$ to the model regressing $Y$ on $\\mathbb{X}_{-j}$ is expected to improve our model’s ability to predict the behavior of $Y$.\n",
        "-   If the points in an added-variable plot for $X_j$ are curved, it indicates that that there is a deficiency in the fitted model (likely because we need to include one or more additional regressors to the model)."
      ],
      "id": "10433d5e-5f3c-4c43-8299-9b341fa7fb28"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAALVBMVEUAAABNTU1oaGh8fHyMjIya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\n"
          }
        }
      ],
      "source": [],
      "id": "f720d67c-98e3-4be1-8f0f-bd36d0f6a916"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**\n",
        "\n",
        "-   What do the three plots tell us about our model?\n",
        "\n",
        "From the **car** package, the `avPlot` function will produce an added-variable plot for a single regressor while the `avPlots` function will produce added-variable plots for one or more regressors.\n",
        "\n",
        "The main arguments to the `avPlot` function are:\n",
        "\n",
        "-   `model`: the fitted `lm` (or `glm`) object.\n",
        "-   `variable`: the regressor for which to create an added-variable plot.\n",
        "-   `id`: a logical value indicating whether unusual observations should be identified. By default, the value is `TRUE`, which means the 2 points with the largest residuals and the 2 points with the largest partial leverage are identified, though this can be customized.\n",
        "\n",
        "The `avPlots` function replaces the `variable` argument with the `terms` argument.\n",
        "\n",
        "The `terms` argument should be a one-sided formula to indicate the regressors for which we want to construct added-variable plots (one plot for each term).\n",
        "\n",
        "We now create and interpret added-variable plots for the model regressing `bill_length_mm` on `body_mass_g` and `flipper_length_mm`."
      ],
      "id": "f417658c-e31c-44c7-aac1-875367f089b6"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(car)\n",
        "# create added-variable plots for all regressors in mlmod\n",
        "avPlots(mlmod)"
      ],
      "id": "87f94661-d4ea-4ac9-a7c5-e51c3901d092"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue line is the simple linear regression model that minimizes the RSS of the points.\n",
        "\n",
        "**Question:**\n",
        "\n",
        "-   Which variable seems to have more explanatory power?\n",
        "\n",
        "To create the added-variable plots for all regressors in the parallel lines model from `penguins` data, we use the fitted model:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species})\\\\\n",
        "&= 24.92 + 0.004 \\mathtt{body\\_mass\\_g} + 9.92 D_C + 3.56 D_G,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $D_C$ and $D_G$ are indicator variables for the Chinstrap and Gentoo penguin species (Adelie penguins are the reference species)."
      ],
      "id": "35b71fec-66d0-4c98-a90b-f7f15eeb80dd"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "avPlots(lmodp)"
      ],
      "id": "5122df9b-c8d7-485a-9093-a60e969dbbfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question:**\n",
        "\n",
        "-   How should we interpret the relative strength of the linear relationships?\n",
        "-   Is there any clustering apparent?\n",
        "\n",
        "**Leverage Plots**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "It sometimes doesn’t make sense to talk about the effect of adding a single regressor when all of the other regressors are in the model.\n",
        "\n",
        "-   When we add a categorical predictor to our model, we simultaneously add $K-1$ indicator variables as regressors; we do not add the indicator variables one-at-a-time.\n",
        "\n",
        "We refer to regressors with this behavior as “multiple degrees-of-freedom terms”.\n",
        "\n",
        "-   A categorical variable with 3 or more levels is the most basic multiple degrees-of-freedom term.\n",
        "-   We could also consider regressors related to the interaction between two or more predictors, polynomial regressors, etc.\n",
        "\n",
        "A *leverage plot* allows us to visualize the impact of multiple degrees-of-freedom terms. The interpretation of leverage plots is similar to the interpretation of added-variable plots, though we refer to “predictors” or “terms” instead regressors (which may be combined into one plot).\n",
        "\n",
        "The `leveragePlot` and `leveragePlots` functions in the `car` package produce single or multiple leverage plots, respectively, with arguments similar to the `avPlot` and `avPlots` functions."
      ],
      "id": "9f604381-d35e-4015-8bc5-ff2d7c665742"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "leveragePlots(lmodp)"
      ],
      "id": "8b29efc1-3ef4-4f68-9b75-36cc4a0edd79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation:\n",
        "\n",
        "-   The leverage plot for `body_mass_g` has a moderate linear relationship, so we expect `body_mass_g` to have moderate value in explaining the behavior of `bill_length_mm` after accounting for `species`\n",
        "-   The points in the leverage plot for `species` have a moderately strong linear relationship, so we expect `species` to have moderate value in explaining the behavior of `bill_length_mm` after accounting for `body_mass_g`.\n",
        "\n",
        "We next examine the leverage plot for the separate lines model fit to the `penguins` data. The fitted separate lines model is:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\hat{E}(\\mathtt{bill\\_length\\_mm} \\mid \\mathtt{body\\_mass\\_g}, \\mathtt{species}) \\\\\n",
        "&= 26.99 + 0.003 \\mathtt{body\\_mass\\_g} + 5.18 D_C - 0.25 D_G \\\\\n",
        "&\\quad + 0.001 D_C \\mathtt{body\\_mass\\_g} + 0.0009 D_G \\mathtt{body\\_mass\\_g},\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "There are 6 estimated coefficients, but the fitted model has only 3 non-intercept terms. Recall the formula we fit for the separate lines model:"
      ],
      "id": "604cbcb0-abc4-4e35-8f61-b2eb766d7bfb"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function call for separate lines model\n",
        "lm(formula = bill_length_mm ~ body_mass_g + species + body_mass_g:species,\n",
        "   data = penguins)"
      ],
      "id": "34c52627-ad45-4755-888d-a6e4ef1440e3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus, we have terms for `body_mass_g`, `species`, and the interaction term `body_mass_g:species`.\n",
        "\n",
        "We use the code below to create the leverage plots:"
      ],
      "id": "72c6a533-ef96-4605-8df1-71f671114912"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "leveragePlots(lmods)"
      ],
      "id": "d6b530bc-1633-47ed-b1c7-72f5764f1730"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation:\n",
        "\n",
        "-   The leverage plot for `body_mass_g` has a moderate linear relationship, so we expect `body_mass_g` to have moderate additional value in explaining the behavior of `bill_length_mm` after accounting for `species` and the interaction term `body_mass_g:species`.\n",
        "    -   It is unlikely we would include the `body_mass_g:species` term in our model prior to including `body_mass_g`, so philosophically, this plot provides little useful information.\n",
        "-   Interpreting the leverage plot for `species` has limited utility because the leverage plot includes the influence of the interaction term `body_mass_g:species`.\n",
        "    -   We are unlikely to fit a model that includes the interaction term without also including the `species` term directly. Instead it makes more sense to judge the utility of adding `species` to the model regressing `bill_length_mm` on `body_mass_g` alone, which we already considered.\n",
        "-   Examining the leverage plot for the interaction term `body_mass_g:species` , we see the points have only a weak linear relationship. Thus, we expect limited utility in adding the interaction term `body_mass_g:species` to the parallel lines regression model that regresses `bill_length_mm` on `body_mass_g` and `species`.\n",
        "\n",
        "# Going deeper\n",
        "\n",
        "**Orthogonality**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Let $$\\mathbf{X}_{[j]}=[x_{1,j},\\ldots,x_{n,j}]$$ denote the $n\\times 1$ column vector of observed values for regressor $X_j$. (We can’t use the notation $\\mathbf{x}_j$ because that is the $p\\times 1$ vector of regressor values for the $j$th observation). Regressors $\\mathbf{X}_{[j]}$ and $\\mathbf{X}_{[k]}$ are *orthogonal* if $\\mathbf{X}_{[j]}^T \\mathbf{X}_{[k]}=0$.\n",
        "\n",
        "Let $\\boldsymbol{1}_{n\\times1}$ denote an $n\\times 1$ column vector of 1s. The definition of orthogonal vectors above implies that $\\mathbf{X}_{[j]}$ is orthogonal to $\\boldsymbol{1}_{n\\times1}$ if $$\n",
        "\\mathbf{X}_{[j]}^T \\boldsymbol{1}_{n\\times1} = \\sum_{i=1}^n x_{i,j} = 0,$$ i.e., if the values in $\\mathbf{X}_{[j]}$ sum to zero.\n",
        "\n",
        "Let $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{i,j}$ denote the sample mean of $\\mathbf{X}_{[j]}$ and $\\bar{\\mathbf{x}}_j = \\bar{x}_j \\boldsymbol{1}_{n\\times 1}$ denote the column vector that repeats $\\bar{x}_j$ $n$ times.\n",
        "\n",
        "*Centering* $\\mathbf{X}_{[j]}$ involves subtracting the sample mean of $\\mathbf{X}_{[j]}$ from $\\mathbf{X}_{[j]}$, i.e., $\\mathbf{X}_{[j]} - \\bar{\\mathbf{x}}_j$.\n",
        "\n",
        "Regressors $\\mathbf{X}_{[j]}$ and $\\mathbf{X}_{[k]}$ are *uncorrelated* if they are orthogonal after being centered, i.e., if $$\n",
        "(\\mathbf{X}_{[j]} - \\bar{\\mathbf{x}}_j)^T (\\mathbf{X}_{[k]} - \\bar{\\mathbf{x}}_k)=0.\n",
        "$$ Note that the sample covariance between vectors $\\mathbf{X}_{[j]}$ and $\\mathbf{X}_{[k]}$ is $$\n",
        "\\begin{aligned}\n",
        "\\widehat{\\mathrm{cov}}(\\mathbf{X}_{[j]}, \\mathbf{X}_{[k]}) &= \\frac{1}{n-1}\\sum_{i=1}^n (x_{i,j} - \\bar{x}_j)(x_{i,k} - \\bar{x}_k) \\\\\n",
        " &= \\frac{1}{n-1}(\\mathbf{X}_{[j]} - \\bar{\\mathbf{x}}_j)^T (\\mathbf{X}_{[k]} - \\bar{\\mathbf{x}}_k).\n",
        "\\end{aligned}\n",
        "$$Thus, two centered regressors are orthogonal if their covariance is zero.\n",
        "\n",
        "It is a desirable to have orthogonal regressors in our fitted model because they simplify estimating the relationship between the regressors and the response. Specifically:\n",
        "\n",
        "*If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from our model will not impact the estimated regression coefficients of the other regressors.*\n",
        "\n",
        "Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s.\n",
        "\n",
        "We consider a simple example with $n=5$ observations to demonstrate how orthogonality of regressors impacts the estimated regression coefficients. In the code below:\n",
        "\n",
        "-   `y` is a vector of response values.\n",
        "-   `ones` is the column vector of 1s.\n",
        "-   `X1` is a column vector of regressor values.\n",
        "-   `X2` is a column vector of regressor values chosen to be orthogonal to `x1` but not to `ones`.\n",
        "-   `X3` is a column vector of regressor values orthogonal to both `x1` and `ones`.\n",
        "-   `X4` is a column vector of regressor values orthogonal to `ones`, `x1`, and `x3`, but not `x2`.\n",
        "-   `X5` is a column vector of regressor values orthogonal to `ones` and `x1`, but not the other regressor vectors."
      ],
      "id": "336aa1d2-8482-4f8a-b40c-438fb6f6ac9b"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "y <- c(1, 4, 6, 8, 9) # response vector\n",
        "ones <- rep(0, 5) # column of 1s\n",
        "x1 <- c(7, 5, 5, 7, 7) # regressor 1\n",
        "x2 <- c(-1, 2, -3, 1, 5/7) # create regressor 2 to be orthogonal to x1\n",
        "crossprod(x1, x2) # cross product is zero\n",
        "x3 <- c(0, -1, 1, 0, 0) # orthogonal to ones, x1\n",
        "x4 <- c(0, 0, 0, 1, -1) # orthogonal to ones, x1, x3\n",
        "x5 <- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4\n",
        "\n",
        "crossprod(cbind(1, x1, x4, x5))\n",
        "\n",
        "lm(y ~ x1)\n",
        "lm(y ~ x4 + x5 - 1)\n",
        "lm(y ~ x1 + x4 + x5)"
      ],
      "id": "ea0d5e5c-4209-4966-8aef-cc0c6ee92924"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code below, we define vectors `y`, `X1`, and `X2`."
      ],
      "id": "7febb176-0f6c-4136-8c9b-ce4506ca5dee"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "y <- c(1, 4, 6, 8, 9)       # create an arbitrary response vector\n",
        "X1 <- c(7, 5, 5, 7, 7)      # create regressor 1\n",
        "X2 <- c(-1, 2, -3, 1, 5/7)  # create regressor 2 to be orthogonal to x1"
      ],
      "id": "fdaf276b-9509-4c86-ab21-ec08515bc5aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the `crossprod` function computes the cross product of two vectors or matrices, so that `crossprod(A, B)` computes $\\mathbf{A}^T B$, where the vectors or matrices must have the correct dimension for the multiplication to be performed.\n",
        "\n",
        "The regressor vectors `X1` and `X2` are orthogonal since their cross product $\\mathbf{X}_{[1]}^T \\mathbf{X}_{[2]}$ (in R, `crossprod(X1, X2)`) equals zero, as shown in the code below."
      ],
      "id": "0638934c-9e7b-4a03-93bb-2142bdb480af"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cross product is zero, so X1 and X2 are orthogonal\n",
        "crossprod(X1, X2)"
      ],
      "id": "3253941f-7266-4a1d-8400-784cf533987b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code below, we regress `y` on `x1` without an intercept (`lmod1`). The estimated coefficient for `X1` is $\\hat{\\beta}_1=0.893$. Next, we then regress `y` on `X1` and `X2` without an intercept (`lmod2`). The estimated coefficients for `X1` and `X2` are $\\hat{\\beta}_1=0.893$ and $\\hat{\\beta}_2=0.221$, respectively. Because `X1` and `X2` are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for `X1` stays the same in both models."
      ],
      "id": "aa82b647-69f5-4687-9732-9a541d29da25"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# y regressed on X1 without an intercept\n",
        "lmod1 <- lm(y ~ x1 - 1)\n",
        "coef(lmod1)\n",
        "# y regressed on X1 and X2 without an intercept\n",
        "lmod2 <- lm(y ~ x1 + x2 - 1)\n",
        "coef(lmod2)"
      ],
      "id": "4b883349-d025-4f4d-a4b6-8e07ac62efd9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous models (`lmod1` and `lmod2`) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our $\\mathbf{X}$ matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s.\n",
        "\n",
        "However, neither `X1` nor `X2` is orthogonal with the column of ones. We define the vector `ones` below, which is a column of 1s, and compute the cross product between `ones` and the two regressors. Since the cross products are not zero, `X1` and `X2` are not orthogonal to the column of ones."
      ],
      "id": "7dda0ae8-8e3e-40cf-8de2-17c4c232f56d"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "ones <- rep(1, 5)   # column of 1s\n",
        "crossprod(ones, X1) # not zero, so not orthogonal\n",
        "crossprod(ones, X2) # not zero, so not orthogonal"
      ],
      "id": "40060c25-586b-40e7-b499-51bdd9ea137f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create `lmod3` by adding adding a column of ones to `lmod2` (i.e., if we include the intercept in the model). The the coefficients for both `X1` and `X2` change when going from `lmod2` to `lmod3` because these regressors are not orthogonal to the column of 1s. Comparing the coefficients `lmod2` above and `lmod3`, $\\hat{\\beta}_1$ changes from $0.893$ to $0.397$ and $\\hat{\\beta}_2$ changes from $0.221$ to $0.279$."
      ],
      "id": "320453a0-45e1-4f9f-a43e-23d9d8f6b598"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lmod2) # coefficients for lmod2\n",
        "# y regressed on X1 and X2 with an intercept\n",
        "lmod3 <- lm(y ~ x1 + x2)\n",
        "coef(lmod3) # coefficients for lmod3"
      ],
      "id": "093e55c9-27c9-4abb-acbc-de77946b7501"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For orthogonality of our regressors to be most impactful, the model’s regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesn’t impact the estimated coefficients of the other regressors. In the code below, we define centered regressors `x3` and `x4` to be uncorrelated, i.e., `X3` and `X4` have sample mean zero and are orthogonal to each other."
      ],
      "id": "df337584-f5b5-408d-a898-79a6d29e325c"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "X3 <-  c(0, -1, 1, 0, 0) # sample mean is zero\n",
        "X4 <- c(0, 0, 0, 1, -1)  # sample mean is zero\n",
        "cov(X3, X4)              # 0, so X3 and X4 are uncorrelated and orthogonal"
      ],
      "id": "b694169d-ec0f-45b0-b56b-620cbb40a6d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we fit linear regression models with any combination of `ones`, `X3`, or `X4` as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below."
      ],
      "id": "ae94b1ce-a6e6-4a43-bb29-608b4606803c"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lm(y ~ 1))           # only column of 1s\n",
        "coef(lm(y ~ x3 - 1))      # only x3\n",
        "coef(lm(y ~ x4 - 1))      # only x4\n",
        "coef(lm(y ~ x3))          # 1s and x3\n",
        "coef(lm(y ~ x4))          # 1s and x4\n",
        "coef(lm(y ~ x3 + x4 - 1)) # x3 and x4\n",
        "coef(lm(y ~ x3 + x4))     # 1s, x3, and x4"
      ],
      "id": "f48b0812-b58b-462e-a1c8-f07612a4fa35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesn’t impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were $\\hat{\\beta}_{0}=5.6$, $\\hat{\\beta}_{3}=1.0$, $\\hat{\\beta}_{4}=-0.5$ when the relevant regressor was included in the model.\n",
        "\n",
        "The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the cross product of the $\\mathbf{X}$ matrix for the largest set of regressors we are considering. Consider the matrix of cross products for the columns of 1s, `x1`, `x2`, `x3`, and `x4`."
      ],
      "id": "ae164678-c15b-4633-8023-95f727647942"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "crossprod(model.matrix(~ X1 + X2 + X3 + X4))"
      ],
      "id": "1f07fb68-75be-4114-9138-ad5d3d938511"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the sequence of models below."
      ],
      "id": "2c81630d-bc7c-4766-8616-8635bee12feb"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lm(y ~ 1))"
      ],
      "id": "0151db5f-2874-4d0d-92e2-0b8c566294c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model with only an intercept has an estimated coefficient of $\\hat{\\beta}_{int}=5.6$. If we add the `X1` to the model with an intercept, then both coefficients change because they are not orthogonal to each other."
      ],
      "id": "b51c5290-eea8-4728-b60a-e38759de8e78"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "lmod4 <- lm(y ~ x1) # model with 1s and x1\n",
        "coef(lmod4)"
      ],
      "id": "dcc8b234-4df5-4e45-9ae6-63f8e4671a7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we add `X2` to `lmod4`, we might think that only $\\hat{\\beta}_{0}$ will change because `X1` and `X2` are orthogonal to each other. However, because `X2` is not orthogonal to all of the other regressors in the model (`X1` and the column of 1s), both $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_1$ will change. The easiest way to realize this is to look at `lmod2` above with only `x1` and `x2`. When we add the column of 1s to `lmod2`, both $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$ will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term."
      ],
      "id": "7e8b9bb9-2ecf-47da-97f8-3c72a4c615f3"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lm(y ~ x1 + x2))"
      ],
      "id": "28a78311-0f70-4a54-9223-55a5bc539428"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, note that `X3` is orthogonal to the column of 1s and `X1`. Thus, if we add `X3` to `lmod4`, which includes both a column of 1s and `X1`, `X3` will not change the estimated coefficients for the intercept or `X1`."
      ],
      "id": "2bba1c91-8064-4dba-a6f7-6137fa6fd6fc"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lm(y ~ x1 + x3))"
      ],
      "id": "678ea452-fc22-47a7-9c77-90e570cfdb9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, since `X4` is orthogonal to the column of 1s, `x1`, and `x3`, adding `X4` to the previous model will not change the estimated coefficients for any of the other variables already in the model."
      ],
      "id": "92074f6d-b7ca-4f35-90ba-4d43d084e4f7"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "coef(lm(y ~ x1 + x3 + x4))"
      ],
      "id": "8ea5363d-8905-4e55-95bc-48cb50d2156c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, if we can partition our $\\mathbf{X}$ matrix such that $\\mathbf{X}^T \\mathbf{X}$ is a block diagonal matrix, then none of the blocks of variables will affect the estimated coefficients of the other variables.\n",
        "\n",
        "Define a new regressor `X5` below. `X5` is orthogonal to the column of 1s and `X1`, but not `X4`."
      ],
      "id": "c1e2c286-ac7d-4b33-a2b3-dc5f46151883"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "X5 <- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4\n",
        "# note block of 0s\n",
        "crossprod(cbind(ones, X1, X4, X5))"
      ],
      "id": "8f1b8534-f666-40e3-9193-f8aee6123b2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the block of zeros in the lower left and upper right corners of the cross product matrix above. The block containing `ones` and `X1` is orthogonal to the block containing `X4` and `X5`. This means that if we fit the model with only the column of 1s and `X1`, the model only with `X4` and `X5`, and then fit the model with the column of 1s, `x1`, `x4`, and `x5`, then the coefficients $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ are not impacted when `X4` and `X5` are added to the model. Similarly, $\\hat{\\beta}_{4}$ and $\\hat{\\beta}_{5}$ are not impacted when the column of 1s and `X1` are added to the model with `X4` and `X5`. See the output below."
      ],
      "id": "327bd6da-1d45-4364-bad3-07abbdf215c4"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "lm(y ~ x1)           # model with 1s and x1\n",
        "lm(y ~ x4 + x5 - 1)  # model with x4 and x5 only\n",
        "lm(y ~ x1 + x4 + x5) # model with 1s, x1, x4, x5"
      ],
      "id": "af440ecf-752d-4f15-ba69-9b0f5be8e425"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    }
  }
}