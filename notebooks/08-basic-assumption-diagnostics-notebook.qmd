---
title: Chapter 8 - Basic assumption diagnostics
author: Joshua French
date: ''
# format: html
format: ipynb
jupyter: ir
execute:
  output: false
self-contained: true
title-block-banner: true
wrap: 'none'
---

To open this information in an interactive Colab notebook, click the Open in Colab graphic below.

<a href="https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/08-basic-assumptions-diagnostics-notebook.ipynb">
   <img src="https://colab.research.google.com/assets/colab-badge.svg">
</a>

---

```{r}
if(!require(faraway, quietly = TRUE)) {
  install.packages("faraway",
                   repos = "https://cran.rstudio.com/")
}
if(!require(KingCountyHouses, quietly = TRUE)) {
  install.packages("KingCountyHouses",
                   repos = "https://cran.rstudio.com/")
}
if(!require(car, quietly = TRUE)) {
  install.packages("car",
                   repos = "https://cran.rstudio.com/")
  library(car)
}
if(!require(lmtest, quietly = TRUE)) {
  install.packages("lmtest",
                   repos = "https://cran.rstudio.com/")
  library(lmtest)
}
```

We adjust some printing options for clarity. 

```{r}
options(digits = 7, scipen = 2)
```

We discuss basic, but effective approaches for assessing the validity of several of the assumptions discussed in the previous notebook.

# Illustration data sets

To facilitate this discussion we will consider two data sets. 

The first data set we will consider is the `prostate` data set, which is available in the **faraway** R package. The data provide information about 97 men who were going to receive a radical prostatectomy. The data set has 97 rows and 9 columns with the variables:

The prostate data frame has 97 rows and 9 columns. A study on 97 men with prostate cancer who were due to receive a radical prostatectomy. The **faraway** package provides the following descriptions of the variables:

- `lcavol`: log(cancer volume).
- `lweight`: log(prostate weight).
- `age`: age in years.
- `lbph`: log(benign prostatic hyperplasia amount).
- `svi`: seminal vesicle invasion.
- `lcp`: log(capsular penetration).
- `gleason`: Gleason score.
- `pgg45`: percentage Gleason scores 4 or 5.
- `lpsa`: log(prostate specific antigen).

We load this data set.

```{r}
data(prostate, package = "faraway")
```

The `svi` variable is a factor, so we convert it to a factor with appropriate levels.

```{r}
prostate$svi <- factor(prostate$svi,
                       labels = c("non-invasive", "invasive"))
```

We will also use the `home_prices` data set in the **KingCountyHouses** package. The `home_prices` data set contains the sales prices of homes sold in King County, WA between 2014-05-02 and 2015-05-27. The data set is a data frame (tibble) with 21,613 rows and 19 columns. A subset of the relevant variables includes:

- `date_sold`: date of sale.
- `price`: sale price (log10 units).
- `bedrooms`: number of bedrooms.
- `bathrooms`: number of bathrooms.
- `sqft_living`: size of living space.
- `sqft_lot`: size of property.
- `floors`: number of floors.
- `waterfront`: indicator variable for a waterfront view.
- `view`: a numeric rating of the quality of the home's view.
- `condition`: a factor variable indicating the condition of the house (poor to very good).
- `sqft_above`: size of living space above ground.
- `sqft_basement` (numeric):size of living space below ground.
- `yr_built`: year the home was built.
- `year_renovated`: the year renovated and, if not renovated, the year built.
- `zip_code`: the zip code of the home.

We load the data below.

```{r}
data(home_prices, package = "KingCountyHouses")
```

We do not wish to work with all of the variables for practical reasons, so we keep only the first 13 variables of the data frame.

```{r}
home_prices <- home_prices[,1:13]
```

We will also remove the `sqft_basement` variable because the `sqrt_above` and `sqft_basement` variables sum to equal `sqft_living`, so we will have an issue with collinearity if we don't remove one of hte variables.

```{r}
home_prices <- home_prices[,-12]
```

#

The validity of our assumptions depend on the model fit to the data.

For the prostate data we will regress `lpsa` on all the remaining variables in the `prostate` data frame. 

- A shortcut for this is to use the formula `y ~ .`, where `y` is the response and `.` indicates to use all of the variables in the data frame containing `y`.

```{r}
lmod_prostate <- lm(lpsa ~ ., data = prostate)
coef(lmod_prostate)
```

Similarly, for the `home_prices` data set, we will regress the `price` variable on all of the remaining variables in `home_prices`.

```{r}
lmod_homes <- lm(price ~ ., data = home_prices)
coef(lmod_homes)
```

# Checking structure

As mentioned in the previous chapter, the most important assumption is that the model structure is correct.

Technically, this assumption is impossible to verify. However, we can verify that the model does an adequate job of approximating the behavior of the observed data.

- If the model doesn't do a good job of mimicking the behavior of the observed data, then we can say this this assumption is violated.

**Residual plot**

---

The most basic approach for assessing whether the structural assumption is satisfied is to determine whether the residuals plot of the residuals versus fitted values.

- If the residuals are randomly, but approximately symmetrically, scattered around a horizontal line running through 0 on the y-axis, then this assumption is satisfied.

Why does this approach work?

- We assume that the errors have mean zero for any combination of our predictors.
- If our fitted model adequately captures the mean of the data, then the residuals will have the same property.
- We expect our residuals to randomly fluctuate above and below 0 with no systematic pattern related to a specific combination of regressors. 

Note: assessing whether the structure of the model is correct is difficult to separate from assessing the validity of the assumption that the mean of the errors is zero. 

- Even if the errors didn't have mean zero (but the model was correct), then our regression model would be non-identifiable because we wouldn't be able to separate the mean of the errors from the mean of the other components of the model.

We plot the residuals versus the fitted values because it is a single plot relating the residuals to different combinations of regressors and links nicely with checkign several other assumptions.

The `residualPlot` function in the **car** package is a convenient way of producing a plot of the residuals versus fitted values for a fitted model.

The `residualPlot` function is a complex function, so we set `quadratic = FALSE` to get a simple plot.

**Structural check for `prostate` data**

---

We examine a plot of the residuals versus fitted values for the model fitted to the `prostate` data.

```{r}
residualPlot(lmod_prostate, quadratic = FALSE)
```

The points appear to be randomly scattered above and below 0 as we move from left to right across the x-axis. There is no clear violate of the structural assumption.

A more details assessment of this assumption involves plotting the residuals versus each of the predictors.

- If the structural assumption is correct, then the residuals should randomly scatter above and below 0 as the predictor value changes.

The `residualsPlots` function provided by the **car** package will produce plots of the residuals versus each first-order predictor and the fitted values.

- Setting `tests = FALSE` and `quadratic = FALSE` simplifies the output produced by the function.
- Setting `fitted = FALSE` prevents the function from display a plot of the residuals versus fitted values.

We consider a the residual plots for the model fit to the `prostate` data below. Because there would be 9 plots by default, we select only 4 of the predictors to simplify our display.

```{r}
residualPlots(lmod_prostate, 
              terms = ~ lcavol + age + svi + gleason,
              tests = FALSE, quadratic = FALSE, fitted = FALSE)
```


The residual plots for `lcavol` and `age` looking relatively symmetric around 0 with no clear patterns.

For a categorical predictor like `svi`, we want the boxplot of the residuals to by symmetric around zero, which is what we see for this data.

Looking at the residual plot for the `gleason` predictor, it appears that the variability of the residuals may be smaller for higher values of `gleason`. 

- This may be something to explore further.
- There are relatively few obserations with high gleason values, so it is difficult to draw a definitive conclusion.


**Structural check for `home_prices` data**

---

We now examine a plot of the residuals versus fitted values for the `home_prices` data.

```{r}
residualPlot(lmod_homes, quadratic = FALSE)
```

There are many many points in this plot compared to the residual plot for the `prostate` data, making it a bit more difficult to assess this assumption.

The bulk of the data between the fitted values 5.25 and 6.5 is approximately symmetric around zero (as far as we can tell). 

There is one unusual point in the upper left part of the plot. This is an outlier. Since this is only a single unusual point for a data set with a lot of observations, this is not something to be concerned about.

However, the residuals begin to systematically fall below 0 when the fitted values are above 6.5. This is an indication that we have a structural deficiency in our fitted model that should be addressed.

# Checking constant variance 

Another key assumption is that the variance of the errors is constant for all combinations of regressor values.

Similar to the check of structure, we can check this assumption through residual plots.

However, it is important to note that even if the errors have constant variance, the residuals do not.

Even if the variance of the errors is constant,

$$\mathrm{var}(\hat{\epsilon}_i) = \frac{\sigma^2}{\sqrt{1-h_i}}, \quad i = 1,2,\ldots,n,$$

where $h_i=\mathbf{H}_{i,i}$ is $i$th diagonal element of the hat matrix $\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$.

It is recommended that instead of assessing the validity of this assumption with the ordinary residuals, we instead use the standardized residuals given by the expression

$$r_i = \frac{\hat{\epsilon}_i}{\hat{\sigma}\sqrt{1-h_i}}, \quad i=1,2,\ldots,n.$$

The standardized residuals should have mean 0 and variance 1.

**Standardized residual plot**

---

The constant variance assumption can be assessed using plots of the standardized residuals versus fitted values or predictors.

- If the assumption is satisfied, then the points of these plots should have constant thickness symmetric about 0 as we move from left to right along the x-axis.

Consider the 4 plots below:

- Panel (a) shows data that perfectly satisfy this assumption. However, the plot itself doesn't have perfect constant thickness as we move from left to right along the axis.
- Panel (b) shows data with increasing variability as we move from left to right across the x-axis. This is difficult to see in the plot because the variability increases but then appears to decrase.
- Panel (c) shows data that has slightly decreasing variability as we move from left to right along the x-axis. However, this is difficult to see in the plot..
- Panel (d) shows data that has a structural issue (notice the curve in the data compared to the 0-line) while the points also increase in variability as we move from left to right along the x-axis.

```{r}
#| echo: false
#| eval: true
#| output: true
set.seed(1)
x <- runif(30, 0.5, 7.5)
y1 <- 2 + x + rnorm(30)
y2 <- 2 + x + rnorm(30, sd = x)
y3 <- 2 + x + rnorm(30, sd = 10 - x)
y4 <- 2 + x + rnorm(30, x + 2 * (x-4)^2, sd = (1 + x^1.25))
lmod1 <- lm(y1 ~ x)
lmod2 <- lm(y2 ~ x)
lmod3 <- lm(y3 ~ x)
lmod4 <- lm(y4 ~ x)
par(mfrow = c(2, 2))
plot(rstandard(lmod1) ~ fitted(lmod1),
     xlab = "fitted", ylab = "standardized resid",
     main = "(a)")
abline(0, 0, lty = 2)
plot(rstandard(lmod2) ~ fitted(lmod2),
     xlab = "fitted", ylab = "standardized resid",
     main = "(b)")
abline(0, 0, lty = 3)
plot(rstandard(lmod1) ~ fitted(lmod3),
     xlab = "fitted", ylab = "standardized resid",
     main = "(c)")
abline(0, 0, lty = 2)
plot(rstandard(lmod4) ~ fitted(lmod4),
     xlab = "fitted", ylab = "standardized resid",
     main = "(d)")
abline(0, 0, lty = 2)
par(mfrow = c(1, 1))
```

What should we do if the plots are inconclusive? 

- We don't conclude an assumption is violated unless the violation is clear.
- Try to use a different method.

**Scale-location plot**

---

A scale-location plot can be used to assess whether the variance is constant when the standardized residual plot is inconclusive AND the structural assumption doesn't appear to be violated.

A *scale-location* plot is a plot of $\sqrt{|\hat{\boldsymbol{\epsilon}}|}$) versus $\hat{\mathbf{y}}$, i.e., a plot of the square root of the absolute value of the residuals versus the fitted values.

- If the constant variance assumption is satisfied (i.e., not clearly violated), then the points of this plot should should have constant thickness as we move from left to right along the x-axis.

Recall that in panel (c) of the previous plot that we thought there was a slight decrease in the variability of the residuals as we moved from left-to-right across the x-axis. 

We produce a scale-location plot for panel (c) above.

- The red line is a visual indication of how the variability changes from left to right and should be approximately flat if this assumption is not violated. 
- The red line clearly decreases in our plot indicating a violation of the constant variance assumption.

```{r}
#| echo: false
#| eval: true
#| output: true
plot(lmod3, which = 3)
```

**Constant variance check for `prostate` data**


We now assess the validity of the constant variance assumption for the `prostate` data. 

The `residualPlot` frunction from the **car** package can be used to construct a plot of the standardized residuals versus the fitted values.

- Set `type = "rstandard"`.

We produce a standarized residual plot for the model fitted to the `prostate` data.

```{r}
residualPlot(lmod_prostate, type = "rstandard", quadratic = FALSE)
```


The plot of the standardized residuals versus the fitted values has almost perfectly constant thickness as we move from left to right along the x-axis, so there is not indication of a non-constant variance issue for our fitted model.

We can produce plots of the standardized residuals versus the predictors using the `residualPlots` function.

There is no indication of a violation of the constant variance assumption in the plots below.

```{r}
residualPlots(lmod_prostate, type = "rstandard",
              terms = ~ lcavol + age + svi + pgg45,
              tests = FALSE, quadratic = FALSE, fitted = FALSE)
```

**Constant variance check for `home_prices` data**


We now assess the validity of the constant error variance assumption for the `home_prices` data. 

Our standardized residual plot seems to show clear evidence that the variability tends to decrease as we move from left to right along the x-axis.

```{r}
residualPlot(lmod_homes, type = "rstandard", quadratic = FALSE)
```


The verify our belief, we can look at a scale-location plot for the fitted model

The `plot` function can be used to produce a scale-location plot.

- The `plot` function has a method for `lm` objects.
- Setting the `which` argument to 3 will produce a scale-location plot.

We produce a scale-location plot of the fitted model for `home_prices`.

```{r}
plot(lmod_homes, which = 3)
```

The red line shows that the variability decreases as we move from left to right. It then appears to increase, but this is an artifact of the model having a structural deficiency for larger fitted values. Focusing on the fitted values less than 6.25, we do see the absolute value of the standardized residuals tend to decrease as we move from left to right.

# Checking normality

A key assumption of many of our tests and confidence intervals is that the errors are normally distributed.

We have previously shown that if our errors are normal that our residuals are also normal.

- If our errors have mean zero, then the residuals also have mean zero.

**The Q-q plot**

---

To assess whether the assumption of normally-distributed errors is reasonable, we will compare the residuals to the quantiles we would expect to get from the same number of i.i.d. normally-distributed observations with mean 0 and variance 1.

- If the normality assumption is satisfied, then a a plot of the observed values compared to the theoretical quantiles will fall approximately in a straight line.
- This plot is called a quantile-quantile or *q-q plot*.

The `qqnorm` function can be used to create a q-q plot of data we believe to come from a normal distribution.

We provide an example of a q-q plot for normally-distributed data below with a mean of 0 and a variance of 1 below.

- We can use the `qqline` function to add a reference line to see how closely the points fall to a straight line.

```{r}
e <- rnorm(30)
qqnorm(e)
qqline(e)
```

The points of the q-q plot fall fairly close to a straight line. There tends to be more variability in the tails of a q-q plot (i.e., for the smallest and largest quantiles) so those points tend to fall a little farther from a straight line.

Observations that deviate substantially from normality will result in a q-q plot that doesn't look like a straight line of points.

Consider a q-q plot of data sampled from a chi-square distribution with 5 degrees of freedom.

- This distribution is positively skewed.
- The points of the q-q plot do not fall that closely to a straight line.

```{r}
e2 <- rchisq(30, 5)
qqnorm(e2)
qqline(e2)
```

Q-q plots are generally constructed under the assumption that the data are independent and identically distributed. 

- Ordinary residuals do not have constant variance.
- It is preferable to scale the residuals to have constant variance prior to constructing a q-q plot.
    - The standardized residuals, $r_i$, $i=1,2,\ldots,n$ are one form of scaled residuals with variance equal to 1.
    - The studentized residuals, $t_i$, $i=1,2,\ldots,n$ are another form of residuals scaled to have a variance of 1.
        - The denominator uses a different estimate of $\sigma$ in the denominator. We do not discuss those details here.
        - In that case, we actually want to compare the observed values to quantiles from a $t$ distribution with $n-p-1$ degrees of freedom, assuming our model has $p$ estimated regression coefficients.
- Though the residuals are not independent, the correlation is typically weak and can be ignored when constructing a q-q plot.

Standardized or studentized residuals should be used to assess the normality assumption.

The `qqPlot` function in the **car** packages produced q-q plots of the studentized residuals.

**The Shapiro-Wilk test**

---

Another approach to assess normality is the Shapiro-Wilk test (Shapiro and Wilk, 1965).

The null hypothesis for this test is that the data are a random sample from a normal distribution.

The alternative hypothesis is that the data are not a random sample from a normal distribution.

If the p-value of the test is less then the desired significance level, then we conclude that the data are not a random sample from a normal distribution. Otherwise, we fail to reject the null hypothesis that the data are a random sample from a normal distribution.

The Shapiro-Wilk test can be applied to a vector of data using the `shapiro.test` function.

**Normality check for `prostate` data**

---

We now use the `qqPlot` function to create a q-q plot of the studentized residuals for the model fitted to the prostate data.

```{r}
qqPlot(lmod_prostate)
```

The points of the q-q plot fall close to a straight line, so the assumption of normally-distributed errors appears to be reasonable assumption for the fitted model.

The `qqPlot` function also produced pointwise 95% confidence envelopes for the quantiles, giving us an idea of how much the points can deviate from a straight line without the assumption being violated.

- The envelopes are pointwise, which means they don't adjust for multiple comparisons.

To help confirm our belief, we apply the Shapiro-Wilk test to the residual of our fitted model.

```{r}
shapiro.test(residuals(lmod_prostate))
```

Since our p-value is large, we believe our residuals (and by proxy, our errors) are a sample from a normal distribution.


**Normality check for `prostate` data**

---

We start with a q-q plot of the studentized residuals of the fitted model for the `home_prices` data.

```{r}
qqPlot(lmod_homes)
```

There are so many points in our plot that it is difficult to assess the model part of the plot. However, it is clear that the tails of the q-q plot deviate from a straight line. The normal error assumption appears to be violated for our fitted model.

We could confirm our beliefs using the Shapiro-Wilk test, but the R function takes a maximum of 5000 observations, and we have over 20,000 residuals.

Even though the normal error assumption appears to be violated for our fitted model, this may not be an issue in practice.

- The Central Limit Theorem applies to our estimated coefficients, so our confidence and hypothesis test should still be approximately valid.
- Our prediction intervals are very sensitive to the normality assumption, so that type of inference should not be trusted in this setting.

# Checking for correlated errors

Our errors are supposed to be uncorrelated.

This assumption is impractical to check unless our data are time series or longitudinal data (observations observed over a series of time) or spatial data (the observations are associated with a specific geographic location).

- We will examine how to assess this assumption for time series data.

If the data are not time series or spatial data then we don't need to bother checking this assumption.

If the errors $\boldsymbol\epsilon$ are uncorrelated, then the residuals $\hat{\boldsymbol\epsilon}$ are typically close to uncorrelated, so we will use them as a proxy to assess the validity of this assumption.

We will illustrate checking this assumption using a classic data set provided by Box et al. (1975) of the monthly total of international airline passengers in thousands from 1949 to 1960. The data are available in the `AirPassengers` data set in the **datasets** package.

We first convert the data to a more usable form for our purposes. We will let month 1 be January of 1949 and month 144 be December of 1960 (the 12th month of the 12th year)

```{r}
air <- data.frame(month = 1:144, passengers = c(AirPassengers))
```

We begin by creating a scatter plot of the passenger totals versus the month.

```{r}
plot(passengers ~ month, data = air)
```

The passenger totals are clearly increasing over time and also increasing in variability.

We will regress `log(passengers)` on `month`.

- We take the `log` of passengers to correct for the increasing variability of our data over time.

The transformed data doesn't appear to have the same issue.

```{r}
plot(log(passengers) ~ month, data = air)
```

```{r}
lmod_air <- lm(log(passengers) ~ month, data = air)
```


**Residuals versus time**

---

The easiest way to check for correlated residuals in time series data is to plot the residuals over the time variable.

- If the residuals tend to cluster with sequences of positive or negative residuals, then the data have a positive correlation.

We create a scatter plot of the residuals versus month for the model fitted to the passengers data.

- We add a horizontal reference line at $y = 0$.

```{r}
plot(residuals(lmod_air) ~ month, data = air,
     ylab = "residuals")
abline(0, 0)
```

The pattern is a bit difficult to see because there is so much data.

If we focus in on the first 24 months of data, we can clearly see sequences of times where the residuals are almost all positive or almost all negative. The residuals appear to have non-neglible positive correlation.

```{r}
plot(residuals(lmod_air) ~ month, data = air,
     ylab = "residuals", xlim = c(0, 24))
abline(0, 0)
```

**Successive residuals plot**

---

A better way to assess whether there is strong correlation in the residuals is to plot pairs of successive residuals versus each other, i.e., to plot $\hat{\epsilon}_{i+1}$ versus $\hat{\epsilon}_i$ for $i=1,2,\ldots,n-1$.

- If the successive residuals show a positive trend then there is positive correlation in the residuals.
- If the successive residuals show a negative trend then there is negative correlation in the residuals.
    - Negative correlation is very difficult to see visually when looking at a plot of the residuals over time because the pattern will look random.
- If there residuals appear to be a random scatter around 0 then there is no evidence of temporal correlation in the residuals.

To construct the plot of success residuals we can use the following code.

```{r}
n <- nobs(lmod_air)
ehat <- residuals(lmod_air)
plot(ehat[2:n] ~ ehat[1:(n-1)],
     xlab = expression(hat(epsilon)[i]),
     ylab =expression(hat(epsilon)[i+1]))
abline(h = 0 , v = 0, col = "grey")
```

`ehat[2:n]` extracts the last $n-1$ residuals while `ehat[1:(n-1)]` extracts the first $n-1$ residuals.

The remaining code ads horizontal and vertical references lines at $y=0$ and $x=0$, respectively.

Since the lagged residuals plot has a positive trend, there is clear evidence of positive temporal correlation among the residuals (and by proxy, the errors).

If either of the previous plots do not provide a satisfactory con

**The Durbin-Watson test**

---

A formal test for temporal correlation between residuals can be performed using the *Durbin-Watson test*.

- Let $\rho$ denote the temporal correlation between residuals.
  - H~0~: The residuals are uncorrelated, $\rho = 0$.
  - H~a~: The residuals are related in some way ($\rho = 0$, $\rho > 0$, or $\rho < 0$).
- The test statistic is:

$$DW = \frac{\sum_{i=2}^n(\hat{\epsilon}_i - \hat{\epsilon}_{i-1})^2}{\sum_{i=1}^n \hat{\epsilon}_i^2}.$$

- The `dwtest` function in the `lmtest` package can be used to test this hypothesis.

We will test for positive temporal correlation among the residuals using the `dwtest` functions.

- Setting `alternative = "greater"` sets H~a~ to be $\rho > 0$. 

```{r}
dwtest(lmod_air, alternative = "greater")
```

The p-value for this test is close to 0, so we conclude that there is positive correlation among the residuals.


# Summary of Methods for Checking Error Assumptions

**Structural assumption**

- Plot of residuals versus fitted values.
    - If this assumption is satisfied, the residuals should be a random, symmetric scatter of points around $y=0$ as we move from left to right along the x-axis.

**Constant error variance assumption**

- Plot of residuals versus fitted values.
- Plot of residuals versus each regressor.
- Plot of standardized residuals versus fitted values.
    - If this assumption is satisfied, then the points of each of the 3 plots above should have constant thickness around a horizontal line at $y=0$ as we move from left to right along the x-axis.
- Plot of $\sqrt{| \hat{\epsilon} |}$ versus fitted values.
    - Only use this check if the structural assumption appears to be satisfied.
    - If this assumption is satisfied, then the points of the plot should have constant thickness as we move from left to right along the x-axis.
    
**Normal error assumption**

- q-q plot of residuals.
    - The points should follow approximately a straight line.
- Shapiro-Wilk test.
    - The null hypothesis is that the residuals are consistent with a sample from a normal distribution, so we want the p-value of this test to be relative large.

**Uncorrelated errors**

This only needs to be checked for time series/longitudinal data or geographically-referenced data.

For assessing temporal autocorrelation:

- Plot of residuals versus time.
    - The plot should be a random scatter of points with no clear pattern if this assumption is satisfied.
    - If there are sequences where there are many positive residuals or many negative residuals, then this assumption is violated and the residuals exhibit positive temporal correlation.
- Plot of successive pairs of residuals.
    - A positive trend indicates positive temporal correlation.
    - A negative trend indicates negative temoral correlation.
    - A random scatter indicates the residuals are consistent with being uncorrelated.
- Durbin-Watson test.
    - A large p-value indicates the residuals are compatible with being uncorrelated.
    - A small p-value indicates the residuals are temporally correlated, with the type of autocorrelation detected being associated with the H~a~.

# References

Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976). "Time Series Analysis, Forecasting and Control". Third Edition. Holden-Day. Series G.

Shapiro, S. S. and Wilk, M. B. (1965). "An analysis of variance test for normality (complete samples)". Biometrika. 52(3–4): 591–611.
