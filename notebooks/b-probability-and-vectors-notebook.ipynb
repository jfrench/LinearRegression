{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix B: Probability and Vectors\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click the\n",
        "Open in Colab graphic below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/b-probability-and-vectors-notebook.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n",
        "</a>\n",
        "\n",
        "# Overview of probability, random variables, and random vectors\n",
        "\n",
        "## Probability Basics\n",
        "\n",
        "Probability attempts to quantify how likely certain outcomes are, where\n",
        "the outcomes are produced by a random experiment (defined below).\n",
        "\n",
        "*Required Background:* basic set theory.\n",
        "\n",
        "The table below summarizes basic probability-related terminology.\n",
        "\n",
        "| term         | notation              | definition                                                                           |\n",
        "|:--------|:-------------|:-------------------------------------------------|\n",
        "| experiment   | N/A                   | A mechanism that produces outcomes that cannot be predicted with absolute certainty. |\n",
        "| outcome      | $\\omega$              | The simplest kind of result produced by an experiment.                               |\n",
        "| sample space | $\\Omega$              | The set of all possible outcomes an experiment can produce.                          |\n",
        "| event        | $A$, $A_i$, $B$, etc. | Any subset of $\\Omega$.                                                              |\n",
        "| empty set    | $\\emptyset$           | The event that includes no outcomes.                                                 |\n",
        "\n",
        "Some comments about the terms,\n",
        "\n",
        "-   **Outcomes**: also called **points**, **realizations**, or\n",
        "    **elements**.\n",
        "-   **Event:** a subset of outcomes.\n",
        "-   The **empty set** is a subset of $\\Omega$, but not an outcome of\n",
        "    $\\Omega$.\n",
        "-   The **empty set** is a subset of every event $A\\subseteq \\Omega$.\n",
        "\n",
        "### Basic Set Operations\n",
        "\n",
        "Let $A$ and $B$ be two events contained in $\\Omega$.\n",
        "\n",
        "-   The **intersection** of $A$ and $B$ is the set of outcomes that are\n",
        "    common to both $A$ and $B$,\n",
        "    -   Denoted $A \\cap B$\n",
        "    -   Set definition:\n",
        "        $A \\cap B = \\{\\omega \\in \\Omega: \\omega \\in A\\;\\mathrm{and}\\;\\omega \\in B\\}$.\n",
        "-   Events $A$ and $B$ are **disjoint** if $A\\cap B = \\emptyset$, or $A$\n",
        "    and $B$ have no common outcomes.\n",
        "-   The **union** of $A$ and $B$ is the set of outcomes that are in $A$\n",
        "    or $B$ or both.\n",
        "    -   Denoted $A \\cup B$\n",
        "    -   Set definition:\n",
        "        $A \\cup B = \\{\\omega \\in \\Omega: \\omega \\in A\\;\\mathrm{or}\\;\\omega \\in B\\}$.\n",
        "-   The **complement** of $A$ is the set of outcomes that are in\n",
        "    $\\Omega$ but are not in $A$.\n",
        "    -   Denoted $A^c$, $\\overline{A}$, or $A'$.\n",
        "    -   Set definition: $A^c = \\{\\omega \\in \\Omega: \\omega \\not\\in A\\}$.\n",
        "-   The set **difference** between $A$ and $B$ is the elements of $A$\n",
        "    that are not in $B$.\n",
        "    -   Denoted $A \\setminus B$\n",
        "    -   Set definition:\n",
        "        $A\\setminus B = \\{\\omega \\in A: \\omega \\not\\in B\\}$.\n",
        "    -   The set difference between $A$ and $B$ may also be denoted by\n",
        "        $A-B$.\n",
        "    -   The set difference is order specific, i.e.,\n",
        "        $(A\\setminus B) \\not= (B\\setminus A)$ in general.\n",
        "\n",
        "### Probability Function\n",
        "\n",
        "A function $P$ that assigns a real number $P(A)$ to every event $A$ is a\n",
        "probability distribution if it satisfies three properties:\n",
        "\n",
        "1.  $P(A)\\geq 0$ for all $A\\in \\Omega$.\n",
        "2.  $P(\\Omega)=P(\\omega \\in \\Omega) = 1$. Alternatively,\n",
        "    $P(A \\subseteq \\Omega) = 1$.\n",
        "3.  If $A_1, A_2, \\ldots$ are disjoint, then\n",
        "    $P\\left(\\bigcup_{i=1}^\\infty A_i \\right)=\\sum_{i=1}^\\infty P(A_i)$.\n",
        "\n",
        "A set of events $\\{A_i:i\\in I\\}$ are **independent** if\n",
        "\n",
        "$$\n",
        "P\\left(\\cap_{i\\in J} A_i \\right)=\\prod_{i\\in J} P(A_i )\n",
        "$$\n",
        "\n",
        "for every finite subset $J\\subseteq I$.\n",
        "\n",
        "The **conditional probability** of $A$ given $B$, denoted as\n",
        "$P(A\\mid B)$, is the probability that $A$ occurs given that $B$ has\n",
        "occurred, and is defined as\n",
        "\n",
        "$$\n",
        "P(A\\mid B) = \\frac{P(A\\cap B)}{P(B)}, \\quad P(B) > 0.\n",
        "$$\n",
        "\n",
        "Some additional facts about probabilities:\n",
        "\n",
        "-   **Complement rule**: $P(A^c) = 1 - P(A)$.\n",
        "-   **Addition rule**: $P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$.\n",
        "-   **Bayes’ rule**: Assuming $P(A) > 0$ and $P(B) > 0$, then\n",
        "\n",
        "$$P(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)}.$$\n",
        "\n",
        "-   **Law of Total Probability**: Let $B_1, B_2, \\ldots$ be a countably\n",
        "    infinite partition of $\\Omega$. Then\n",
        "    $$P(A) = \\sum_{i=1}^{\\infty} P(A \\cap B_i) = \\sum_{i=1}^{\\infty} P(A \\mid B_i) P(B_i).$$\n",
        "\n",
        "## Random Variables\n",
        "\n",
        "A **random variable** $Y$ is a mapping/function\n",
        "\n",
        "$$\n",
        "Y:\\Omega\\to\\mathbb{R}\n",
        "$$\n",
        "\n",
        "that assigns a real number $Y(\\omega)$ to each outcome $\\omega$. (We\n",
        "typically drop the $(\\omega)$ notation for simplicity.)\n",
        "\n",
        "The **cumulative distribution function (CDF)** of $Y$, $F_Y$, is a\n",
        "function $F_Y:\\mathbb{R}\\to [0,1]$ defined by\n",
        "\n",
        "$$\n",
        "F_Y (y)=P(Y \\leq y).\n",
        "$$\n",
        "\n",
        "The subscript of $F$ indicates the random variable the CDF describes.\n",
        "E.g., $F_X$ denotes the CDF of the random variable $X$ and $F_Y$ denotes\n",
        "the CDF of the random variable $Y$. The subscript can be dropped when\n",
        "the context makes it clear what random variable the CDF describes. An\n",
        "$F$-distributed random variable is one that has the $F$ distribution.\n",
        "\n",
        "The **support** of $Y$, $\\mathcal{S}$, is the smallest set such that\n",
        "$P(Y\\in \\mathcal{S})=1$.\n",
        "\n",
        "### Discrete random variables\n",
        "\n",
        "$Y$ is a **discrete** random variable if it takes countably many values\n",
        "$\\{y_1, y_2, \\dots \\} = \\mathcal{S}$.\n",
        "\n",
        "The **probability mass function (pmf)** for $Y$ is $f_Y (y)=P(Y=y)$,\n",
        "where $y\\in \\mathbb{R}$, and must have the following properties:\n",
        "\n",
        "1.  $0 \\leq f_Y(y) \\leq 1$.\n",
        "2.  $\\sum_{y\\in \\mathcal{S}} f_Y(y) = 1$.\n",
        "\n",
        "Additionally, the following statements are true:\n",
        "\n",
        "-   $F_Y(c) = P(Y \\leq c) = \\sum_{y\\in \\mathcal{S}:y \\leq c} f_Y(y)$.\n",
        "-   $P(Y \\in A) = \\sum_{y \\in A} f_Y(y)$ for some event $A$.\n",
        "-   $P(a \\leq Y \\leq b) = \\sum_{y\\in\\mathcal{S}:a\\leq y\\leq b} f_Y(y)$.\n",
        "\n",
        "The **expected value**, **mean**, or first moment of $Y$ is defined as\n",
        "\n",
        "$$ E(Y) = \\sum_{y\\in \\mathcal{S}} y f_Y(y), $$\n",
        "\n",
        "assuming the sum is well-defined.\n",
        "\n",
        "The **variance** of $Y$ is defined as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{var}(Y)&=E(Y-E(Y))^2 \\\\\n",
        "&=\\sum_{y\\in \\mathcal{S}} (y - E(Y))^2 f_Y(y).\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Note that $\\mathrm{var}(Y)=E(Y-E(Y))^2=E(Y^2)-[E(Y)]^2$. The last\n",
        "expression is often easier to compute.\n",
        "\n",
        "The **standard deviation** of Y is\n",
        "\n",
        "$$SD(Y)=\\sqrt{\\mathrm{var}(Y)  }.$$\n",
        "\n",
        "#### Example (Bernoulli)\n",
        "\n",
        "A random variable $Y$ is said to have a Bernoulli distribution with\n",
        "probability $\\theta$, denoted $Y\\sim \\mathsf{Bernoulli}(\\theta)$, if:\n",
        "\n",
        "-   $\\mathcal{S} = \\{0, 1\\}$\n",
        "-   $P(Y = 1) = \\theta$, where $\\theta\\in (0,1)$.\n",
        "\n",
        "**Bernoulli PMF:**\n",
        "\n",
        "$$f_Y(y) = \\theta^y (1-\\theta)^{(1-y)}.$$\n",
        "\n",
        "Determine the mean and variance of $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "### Continuous random variables\n",
        "\n",
        "$Y$ is a **continuous** random variable if there exists a function\n",
        "$f_Y (y)$ such that:\n",
        "\n",
        "1.  $f_Y (y)\\geq 0$ for all $y$,\n",
        "2.  $\\int_{-\\infty}^\\infty f_Y (y) dy = 1$,\n",
        "3.  $a\\leq b$, $P(a<Y<b)=\\int_a^b f_Y (y) dy$.\n",
        "\n",
        "The function $f_Y$ is called the **probability density function (pdf)**.\n",
        "\n",
        "Additionally, $F_Y (y)=\\int_{-\\infty}^y f_Y (y) dy$ and\n",
        "$f_Y (y)=F'_Y(y)$ for any point $y$ at which $F_Y$ is differentiable.\n",
        "\n",
        "The **mean** of a continuous random variables $Y$ is defined as\n",
        "\n",
        "$$\n",
        "E(Y) =\n",
        "\\int_{-\\infty}^{\\infty} y f_Y(y)  dy =\n",
        "\\int_{y\\in\\mathcal{S}} y f_Y(y).\n",
        "$$\n",
        "\n",
        "assuming the integral is well-defined.\n",
        "\n",
        "The **variance** of a continuous random variable $Y$ is defined by\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(Y)=\n",
        "E(Y-E(Y))^2\\\\=\\int_{-\\infty}^{\\infty} (y - E(Y))^2 f_Y(y)  dy \\\\\n",
        "=\\int_{y\\in\\mathcal{S}} (y - E(Y))^2 f_Y(y) dy\n",
        "$$\n",
        "\n",
        "#### Example (Exponential distribution)\n",
        "\n",
        "A random variable $Y$ is said to have an exponential distribution rate\n",
        "parameter $\\lambda$, denoted with $Y \\sim \\mathsf{Exp}(\\lambda)$ if\n",
        "$\\mathcal{S} = \\{y\\in \\mathbb{R}:y\\geq 0\\}$ and has distribution,\n",
        "\n",
        "**Exponential PDF:**\n",
        "\n",
        "$$f_Y(y)=\\lambda\\exp(-\\lambda y)$$\n",
        "\n",
        "Determine the mean and variance of $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "### Useful facts for transformations of random variables\n",
        "\n",
        "Let $Y$ be a random variable and $a\\in\\mathbb{R}$ be a constant. Then:\n",
        "\n",
        "-   $E(a) = a$.\n",
        "-   $E(aY) = a E(Y)$.\n",
        "-   $E(a + Y) = a + E(Y)$.\n",
        "-   $\\mathrm{var}(a) = 0$.\n",
        "-   $\\mathrm{var}(aY) = a^2 \\mathrm{var}(Y)$.\n",
        "-   $\\mathrm{var}(a + Y) = \\mathrm{var}(Y)$.\n",
        "-   For a discrete random variable and a function $g$,\n",
        "\n",
        "$$E(g(Y))=\\sum_{y\\in\\mathcal{S}}g(y)f_Y(y),$$\n",
        "\n",
        "assuming the sum is well-defined. - For a continuous random variable and\n",
        "a function $g$,\n",
        "\n",
        "$$E(g(Y))=\\int_{y\\in\\mathcal{S}}g(y)f_Y(y)\\;dy,$$\n",
        "\n",
        "assuming the integral is well-defined.\n",
        "\n",
        "## Multivariate distributions\n",
        "\n",
        "### Basic properties\n",
        "\n",
        "Let $Y_1,Y_2,\\ldots,Y_n$ denote $n$ random variables with supports\n",
        "$\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_n$, respectively.\n",
        "\n",
        "If the random variables are **jointly discrete** (i.e., all discrete),\n",
        "then the joint pmf $f(y_1,\\ldots,y_n)=P(Y_1=y_1,\\ldots,Y_n=y_n)$\n",
        "satisfies the following properties:\n",
        "\n",
        "1.  $0\\leq f(y_1,\\ldots,y_n )\\leq 1$,\n",
        "2.  $\\sum_{y_1\\in\\mathcal{S}_1}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) = 1$,\n",
        "3.  $P((Y_1,\\ldots,Y_n)\\in A)=\\sum_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n)$.\n",
        "\n",
        "In this context,\n",
        "\n",
        "$$\n",
        "E(Y_1 \\cdots Y_n)=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n}y_1 \\cdots y_n  f(y_1,\\ldots,y_n).\n",
        "$$\n",
        "\n",
        "In general,\n",
        "\n",
        "$$\n",
        "E(g(Y_1,\\ldots,Y_n))=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n),\n",
        "$$\n",
        "\n",
        "where $g$ is a function of the random variables.\n",
        "\n",
        "If the random variables are **jointly continuous**, then\n",
        "$f(y_1,\\ldots,y_n)$ is the joint pdf if it satisfies the following\n",
        "properties:\n",
        "\n",
        "1.  $f(y_1,\\ldots,y_n ) \\geq 0$,\n",
        "2.  $\\int_{y_1\\in\\mathcal{S}_1}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) dy_n \\cdots dy_1 = 1$,\n",
        "3.  $P((Y_1,\\ldots,Y_n)\\in A)=\\int \\cdots \\int_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n) dy_n\\ldots dy_1$.\n",
        "\n",
        "In this context,\n",
        "\n",
        "$$\n",
        "E(Y_1 \\cdots Y_n)=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} y_1 \\cdots y_n  f(y_1,\\ldots,y_n) dy_n \\ldots dy_1.\n",
        "$$\n",
        "\n",
        "In general,\n",
        "\n",
        "$$\n",
        "E(g(Y_1,\\ldots,Y_n))=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n) dy_n \\cdots dy_1,\n",
        "$$\n",
        "\n",
        "where $g$ is a function of the random variables.\n",
        "\n",
        "### Marginal distributions\n",
        "\n",
        "If the random variables are jointly discrete, then the marginal pmf of\n",
        "$Y_1$ is obtained by summing over the other variables $Y_2, ..., Y_n$:\n",
        "\n",
        "$$f_{Y_1}(y_1)=\\sum_{y_2\\in\\mathcal{S}_2}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n).$$\n",
        "\n",
        "Similarly, if the random variables are jointly continuous, then the\n",
        "marginal pdf of $Y_1$ is obtained by integrating over the other\n",
        "variables $Y_2, ..., Y_n$:\n",
        "\n",
        "$$f_{Y_1}(y_1)=\\int_{y_2\\in\\mathcal{S}_2}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n) dy_n \\cdots dy_2.\n",
        "$$\n",
        "\n",
        "### Independence of random variables\n",
        "\n",
        "Random variables $X$ and $Y$ are independent if\n",
        "$$F(x, y) = F_X(x) F_Y(y).$$\n",
        "\n",
        "Alternatively, $X$ and $Y$ are independent if\n",
        "$$f(x, y) = f_X(x)f_Y(y).$$\n",
        "\n",
        "### Conditional distributions\n",
        "\n",
        "Let $X$ and $Y$ be random variables. Then assuming $f_Y(y)>0$, the\n",
        "conditional distribution of $X$ given $Y = y$, denoted $X|Y=y$ comes\n",
        "from Bayes’ formula:\n",
        "$$f(x|y) = \\frac{f(x, y)}{f_{Y}(y)}, \\quad f_Y(y)>0.$$\n",
        "\n",
        "### Covariance\n",
        "\n",
        "The covariance between random variables $X$ and $Y$ is\n",
        "\n",
        "$$\\mathrm{cov}(X,Y)=E[(X-E(X))(Y-E(Y))]\\\\=E(XY)-E(X)E(Y).$$\n",
        "\n",
        "### Useful facts for transformations of multiple random variables\n",
        "\n",
        "Let $a$ and $b$ be scalar constants. Let $Y$ and $Z$ be random\n",
        "variables. Then:\n",
        "\n",
        "-   $E(aY+bZ)=aE(Y)+bE(Z)$.\n",
        "-   $\\mathrm{var}(Y+Z)=\\mathrm{var}(Y)+\\mathrm{var}(Z)+2\\mathrm{cov}(Y, Z)$.\n",
        "-   $\\mathrm{cov}(a,Y)=0$.\n",
        "-   $\\mathrm{cov}(Y,Y)=\\mathrm{var}(Y)$.\n",
        "-   $\\mathrm{cov}(aY, bZ)=ab\\mathrm{cov}(Y, Z)$.\n",
        "-   $\\mathrm{cov}(a + Y,b + Z)=\\mathrm{cov}(Y, Z)$.\n",
        "\n",
        "If $Y$ and $Z$ are also independent, then:\n",
        "\n",
        "-   $E(YZ)=E(Y)E(Z)$.\n",
        "-   $\\mathrm{cov}(Y, Z)=0$.\n",
        "\n",
        "In general, if $Y_1, Y_2, \\ldots, Y_n$ are a set of random variables,\n",
        "then:\n",
        "\n",
        "-   $E(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n E(Y_i)$, i.e., the expectation\n",
        "    of the sum of random variables is the sum of the expectation of the\n",
        "    random variables.\n",
        "-   $\\mathrm{var}(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n \\mathrm{var}(Y_i) + \\sum_{j=1}^n\\sum_{1\\leq i<j\\leq n}2\\mathrm{cov}(Y_i, Y_j)$,\n",
        "    i.e., the variance of the sum of random variables is the sum fo the\n",
        "    variables’ variances plus the sum of twice all possible pairwise\n",
        "    covariances.\n",
        "\n",
        "If in addition, $Y_1, Y_2, \\ldots, Y_n$ are all independent of each\n",
        "other, then:\n",
        "\n",
        "-   $\\mathrm{var}(\\sum_{i=1}^n Y_i) = \\sum_{i=1}^n \\mathrm{var}(Y_i)$\n",
        "    since all pairwise covariances are 0.\n",
        "\n",
        "### Example (Binomial distribution)\n",
        "\n",
        "A random variable $Y$ is said to have a Binomial distribution with $n$\n",
        "trials and probability of success $\\theta$, denoted\n",
        "$Y\\sim \\mathsf{Bin}(n,\\theta)$ when $\\mathcal{S}=\\{0,1,2,\\ldots,n\\}$ and\n",
        "the pmf is:\n",
        "\n",
        "**Binomial PMF:**\n",
        "\n",
        "$$f(y\\mid\\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{(n-y)}.$$\n",
        "\n",
        "An alternative explanation of a Binomial random variable is that it is\n",
        "the sum of $n$ independent and identically-distributed Bernoulli random\n",
        "variables. Alternatively, let\n",
        "$Y_1,Y_2,\\ldots,Y_n\\stackrel{i.i.d.}{\\sim} \\mathsf{Bernoulli}(\\theta)$,\n",
        "where i.i.d. stands for independent and identically distributed, i.e.,\n",
        "$Y_1, Y_2, \\ldots, Y_n$ are independent random variables with identical\n",
        "distributions. Then $Y=\\sum_{i=1}^n Y_i \\sim \\mathsf{Bin}(n,\\theta)$.\n",
        "\n",
        "A Binomial random variable with $\\theta = 0.5$ models the question: what\n",
        "is the probability of flipping $y$ heads in $n$ flips?\n",
        "\n",
        "Determine the mean and variance of $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "### Example (Continuous bivariate distribution)\n",
        "\n",
        "Hydration is important for health. Like many people, the author has a\n",
        "water bottle he uses to say hydrated through the day and drinks several\n",
        "liters of water per day. Let’s say the author refills his water bottle\n",
        "every 3 hours.\n",
        "\n",
        "-   Let $Y$ denote the proportion of the water bottle filled with water\n",
        "    at the beginning of the 3-hour window.\n",
        "-   Let $X$ denote the amount of water the author consumes in the 3-hour\n",
        "    window (measured in the the proportion of total water bottle\n",
        "    capacity).\n",
        "\n",
        "We know that $0\\leq X \\leq Y \\leq 1$. The joint density of the random\n",
        "variables is\n",
        "\n",
        "$$\n",
        "f(x,y)=4y^2,\\quad 0 \\leq x\\leq y\\leq 1,\n",
        "$$ and 0 otherwise.\n",
        "\n",
        "We answer a series of questions about this distribution.\n",
        "\n",
        "**Q1: Determine** $P(0.5\\leq X\\leq 1, 0.75\\leq Y)$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Q2: Determine the marginal distributions of** $X$ and $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Q3: Determine the means of** $X$ and $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Q4: Determine the variances of** $X$ and $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br> </br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Q5: Determine the mean of** $XY$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Q6: Determine the covariance of** $X$ and $Y$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Q7: Determine the mean and variance of** $Y-X$, i.e., the average\n",
        "amount of water remaining after a 3-hour window and the variability of\n",
        "that amount.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "## Random vectors\n",
        "\n",
        "### Definition\n",
        "\n",
        "A **random vector** is a vector of random variables. A random vector is\n",
        "assumed to be a column vector unless otherwise specified.\n",
        "\n",
        "Additionally, a **random matrix** is a matrix of random variables.\n",
        "\n",
        "### Mean, variance, and covariance\n",
        "\n",
        "Let $\\mathbf{y}=[Y_1,Y_2,\\dots,Y_n]$ be an $n\\times1$ random vector.\n",
        "\n",
        "The mean of a random vector is the vector containing the means of the\n",
        "random variables in the vector. More specifically, the mean of\n",
        "$\\mathbf{y}$ is defined as\n",
        "\n",
        "$$\n",
        "E(\\mathbf{y})=\\begin{bmatrix}E(Y_1)\\\\E(Y_2)\\\\\\vdots\\\\E(Y_n)\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "The variance of a random vector isn’t a number. Instead, it is the\n",
        "matrix of covariances of all pairs of random variables in the random\n",
        "vector. The variance of $\\mathbf{y}$ is\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{var}(\\mathbf{y}) &= E(\\mathbf{y}\\mathbf{y}^T )-E(\\mathbf{y})E(\\mathbf{y})^T\\\\\n",
        "&= \\begin{bmatrix}\\mathrm{var}(Y_1) & \\mathrm{cov}(Y_1,Y_2) &\\dots &\\mathrm{cov}(Y_1,Y_n)\\\\\\mathrm{cov}(Y_2,Y_1 )&\\mathrm{var}(Y_2)&\\dots&\\mathrm{cov}(Y_2,Y_n)\\\\\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
        "\\mathrm{cov}(Y_n,Y_1)&\\mathrm{cov}(Y_n,Y_2)&\\dots&\\mathrm{var}(Y_n)\\end{bmatrix}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Alternatively, the variance of $\\mathbf{y}$ is called the **covariance\n",
        "matrix** of $\\mathbf{y}$ or the **variance-covariance matrix** of\n",
        "$\\mathbf{y}$.\n",
        "\n",
        "*Note:* $\\mathrm{var}(\\mathbf{y})=\\mathrm{cov}(\\mathbf{y}, \\mathbf{y})$.\n",
        "\n",
        "Let $\\mathbf{x} = [X_1, X_2, \\ldots, X_n]$ be an $n\\times 1$ random\n",
        "vector.\n",
        "\n",
        "The covariance matrix between $\\mathbf{x}$ and $\\mathbf{y}$ is defined\n",
        "as\n",
        "\n",
        "$$\n",
        "\\mathrm{cov}(\\mathbf{x}, \\mathbf{y}) = E(\\mathbf{x}\\mathbf{y}^T) - E(\\mathbf{x}) E(\\mathbf{y})^T.\n",
        "$$\n",
        "\n",
        "### Properties of transformations of random vectors\n",
        "\n",
        "Define:\n",
        "\n",
        "-   $\\mathbf{a}$ to be an $n\\times 1$ vector of constants (not\n",
        "    necessarily the same constant).\n",
        "-   $\\mathbf{A}$ to be an $m\\times n$ matrix of constants (not\n",
        "    necessarily the same constant).\n",
        "-   $\\mathbf{x}=[X_1,X_2,\\ldots,X_n]$ to be an $n\\times 1$ random\n",
        "    vector.\n",
        "-   $\\mathbf{y}=[Y_1,Y_2,\\ldots,Y_n]$ to be an $n\\times 1$ random\n",
        "    vector.\n",
        "-   $\\mathbf{z}=[Z_1,Z_2,\\ldots,Z_n]$ to be an $n\\times 1$ random\n",
        "    vector.\n",
        "-   $0_{n\\times n}$ to be an $n\\times n$ matrix of zeros.\n",
        "\n",
        "Then:\n",
        "\n",
        "-   $E(\\mathbf{A}\\mathbf{y})=\\mathbf{A}E(\\mathbf{y})$.\n",
        "-   $E(\\mathbf{y}\\mathbf{A}^T )=E(\\mathbf{y}) \\mathbf{A}^T$.\n",
        "-   $E(\\mathbf{x}+\\mathbf{y})=E(\\mathbf{x})+E(\\mathbf{y})$.\n",
        "-   $\\mathrm{var}(\\mathbf{A}\\mathbf{y})=\\mathbf{A}\\mathrm{var}(\\mathbf{y}) \\mathbf{A}^T$.\n",
        "-   $\\mathrm{cov}(\\mathbf{x}+\\mathbf{y},\\mathbf{z})=\\mathrm{cov}(\\mathbf{x},\\mathbf{z})+\\mathrm{cov}(\\mathbf{y},\\mathbf{z})$.\n",
        "-   $\\mathrm{cov}(\\mathbf{x},\\mathbf{y}+\\mathbf{z})=\\mathrm{cov}(\\mathbf{x},\\mathbf{y})+\\mathrm{cov}(\\mathbf{x},\\mathbf{z})$.\n",
        "-   $\\mathrm{cov}(\\mathbf{A}\\mathbf{x},\\mathbf{y})=\\mathbf{A}\\ \\mathrm{cov}(\\mathbf{x},\\mathbf{y})$.\n",
        "-   $\\mathrm{cov}(\\mathbf{x},\\mathbf{A}\\mathbf{y})=\\mathrm{cov}(\\mathbf{x},\\mathbf{y}) \\mathbf{A}^T$.\n",
        "-   $\\mathrm{var}(\\mathbf{a})= 0_{n\\times n}$.\n",
        "-   $\\mathrm{cov}(\\mathbf{a},\\mathbf{y})=0_{n\\times n}$.\n",
        "-   $\\mathrm{var}(\\mathbf{a}+\\mathbf{y})=\\mathrm{var}(\\mathbf{y})$.\n",
        "\n",
        "### Example (Continuous bivariate distribution continued)\n",
        "\n",
        "Using the definitions we introduced, we want to answer **Q7** of the\n",
        "hydration example. Summarizing only the essential details, we have a\n",
        "random vector $\\mathbf{z}=[X, Y]$ with mean $E(\\mathbf{z})=[2/5, 4/5]$\n",
        "and covariance matrix\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\mathbf{z})=\n",
        "\\begin{bmatrix}\n",
        "14/225 & 1/75 \\\\\n",
        "1/75 & 2/75\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Determine $E(Y-X)$ and $\\mathrm{var}(Y-X)$.\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "## Multivariate normal (Gaussian) distribution\n",
        "\n",
        "### Definition\n",
        "\n",
        "The random vector $\\mathbf{y}=[Y_1,\\dots,Y_n]$ has a multivariate normal\n",
        "distribution with mean $E(\\mathbf{y})=\\boldsymbol{\\mu}$ (an $n\\times 1$\n",
        "vector) and covariance matrix\n",
        "$\\mathrm{var}(\\mathbf{y})=\\boldsymbol{\\Sigma}$ (an $n\\times n$ matrix)\n",
        "if its joint pdf is,\n",
        "\n",
        "$$\n",
        "f(\\mathbf{y})=\\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2} }  \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y}-\\boldsymbol{\\mu})\\right),\n",
        "$$\n",
        "\n",
        "where $|\\boldsymbol{\\Sigma}|$ is the determinant of\n",
        "$\\boldsymbol{\\Sigma}$. Note that $\\boldsymbol{\\Sigma}$ must be symmetric\n",
        "and positive definite.\n",
        "\n",
        "In this case, we would denote the distribution of $\\mathbf{y}$ as\n",
        "\n",
        "$$\\mathbf{y}\\sim \\mathsf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}).$$\n",
        "\n",
        "### Linear functions of a multivariate normal random vector\n",
        "\n",
        "A linear function of a multivariate normal random vector (i.e.,\n",
        "$\\mathbf{a}+\\mathbf{A}\\mathbf{y}$, where $\\mathbf{a}$ is an $m\\times 1$\n",
        "vector of constant values and $\\mathbf{A}$ is an $m\\times n$ matrix of\n",
        "constant values) is also multivariate normal (though it could collapse\n",
        "to a single random variable if $\\mathbf{A}$ is a $1\\times n$ vector).\n",
        "\n",
        "**Application**: Suppose that\n",
        "$\\mathbf{y}\\sim \\mathsf{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$. For\n",
        "an $m\\times n$ matrix of constants $\\mathbf{A}$,\n",
        "$\\mathbf{A}\\mathbf{y}\\sim \\mathsf{N}(\\mathbf{A}\\boldsymbol{\\mu},\\mathbf{A}\\boldsymbol{\\Sigma} \\mathbf{A}^T)$.\n",
        "\n",
        "More generally, the most common estimators used in linear regression are\n",
        "linear combinations of a (typically) multivariate normal random vector,\n",
        "meaning that many of the estimators also have a (multivariate) normal\n",
        "distribution.\n",
        "\n",
        "### Example (OLS matrix form)\n",
        "\n",
        "Ordinary least squares regression is a method for fitting a linear\n",
        "regression model to data. Suppose that we have observed variables\n",
        "$X_1, X_2, X_3, \\ldots, X_{p-1}, Y$ for each of $n$ subjects from some\n",
        "population, with $X_{i,j}$ denoting the value of $X_j$ for observation\n",
        "$i$ and $Y_i$ denoting the value of $Y$ for observation $i$. In general,\n",
        "we want to use $X_1, \\ldots, X_{p-1}$ to predict the value of $Y$. Let,\n",
        "$$\n",
        "\\mathbf{X} =\n",
        "\\begin{bmatrix}\n",
        "1 & X_{1,1} & X_{1,2} & \\cdots & X_{1,n} \\\\\n",
        "1 & X_{2,1} & X_{2,2} & \\cdots & X_{2,n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "1 & X_{n,1} & X_{n,2} & \\cdots & X_{n,n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "be a full-rank matrix of size $n\\times p$ and\n",
        "\n",
        "$$\n",
        "\\mathbf{y}=[Y_1, Y_2, \\ldots,Y_n],\n",
        "$$\n",
        "\n",
        "be an $n\\times 1$ vector of responses. It is common to assume that,\n",
        "$$ \\mathbf{y}\\sim \\mathsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_{n\\times n}).\\,\n",
        "$$ where $\\beta=[\\beta_0,\\beta_1,\\ldots,\\beta_{p-1}]$ is a\n",
        "$p$-dimensional vector of constants.\n",
        "\n",
        "The matrix\n",
        "$\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n",
        "projects $\\mathbf{y}$ into the space spanned by the vectors in\n",
        "$\\mathbf{X}$.\n",
        "\n",
        "Determine the distribution of $\\mathbf{Hy}.$\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>"
      ],
      "id": "df23695a-9e3f-43d1-988c-71e2e3546999"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}