{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 5 - Linear Model Theory\n",
        "\n",
        "Joshua French\n",
        "\n",
        "To open this information in an interactive Colab notebook, click the Open in Colab graphic below.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/jfrench/LinearRegression/blob/master/notebooks/05-linear-model-theory-notebook.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"> </a>\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# Basic theoretical results for linear models\n",
        "\n",
        "In this chapter we discuss many basic theoretical results for linear models.\n",
        "\n",
        "We assume the responses can be modeled as\n",
        "\n",
        "$$\n",
        "Y_i=\\beta_0+\\beta_1 x_{i,1} +\\ldots + \\beta_{p-1}x_{i,-1}+\\epsilon_i,\\quad i=1,2,\\ldots,n,\n",
        "$$\n",
        "\n",
        "or using matrix formulation, as\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}.\n",
        "$$\n",
        "\n",
        "# Standard assumptions\n",
        "\n",
        "We assume that the components of our linear model have the characteristics previously described in Chapter 3. We also need to make several specific assumptions about the errors.\n",
        "\n",
        "**Error Assumption 1**\n",
        "\n",
        "The mean of the errors is zero conditional on the value of the regressors.\n",
        "\n",
        "This means that\n",
        "\n",
        "$$E(\\epsilon_i \\mid \\mathbb{X} = \\mathbf{x}_i)=0, i=1,2,\\ldots,n,$$\n",
        "\n",
        "or using matrix notation,\n",
        "\n",
        "$$\n",
        "E(\\boldsymbol{\\epsilon}\\mid \\mathbf{X}) = 0_{n\\times 1}.\n",
        "$$\n",
        "\n",
        "where “$\\mid \\mathbf{X}$” is notation meaning “conditional on knowing the regressor values for all observations”.\n",
        "\n",
        "**Error Assumption 2**\n",
        "\n",
        "The errors have constant variances and are uncorrelated, conditional on knowing the regressors, i.e.,\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\epsilon_i\\mid \\mathbb{X}=\\mathbf{x}_i) = \\sigma^2, \\quad i=1,2,\\ldots,n.\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\mathrm{cov}(\\epsilon_i, \\epsilon_j\\mid \\mathbf{X}) = 0, \\quad i,j=1,2,\\ldots,n,\\quad i\\neq j.\n",
        "$$\n",
        "\n",
        "In matrix notation, this is stated as\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\boldsymbol{\\epsilon} \\mid {\\mathbf{X}})=\\sigma^2\\mathbf{I}_{n\\times n}.\n",
        "$$\n",
        "\n",
        "**Error Assumption 3**\n",
        "\n",
        "The errors are identically distributed. This may be written as\n",
        "\n",
        "$$\n",
        "\\epsilon_i \\sim F, i=1,2,\\ldots,n,\n",
        "$$\n",
        "\n",
        "where $F$ is some arbitrary distribution.\n",
        "\n",
        "**Error Assumption 4**\n",
        "\n",
        "In practice, it is common to assume the errors have a normal (Gaussian) distribution.\n",
        "\n",
        "**Assumptions 1-4 combined**\n",
        "\n",
        "Two uncorrelated normal random variables are also independent (but this is not generally true for other distributions).\n",
        "\n",
        "Putting assumptions 1-4 together, we have that\n",
        "\n",
        "$$\n",
        "\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n \\mid \\mathbf{X}\\stackrel{i.i.d.}{\\sim} \\mathsf{N}(0, \\sigma^2),\n",
        "$$\n",
        "\n",
        "or using matrix notation,\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\epsilon}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{0}_{n\\times 1},\\sigma^2 \\mathbf{I}_{n\\times n}).\n",
        "$$\n",
        "\n",
        "In summary, our error assumptions are:\n",
        "\n",
        "1.  $E(\\epsilon_i \\mid \\mathbb{X}=\\mathbf{x}_i)=0$ for $i=1,2,\\ldots,n$.\n",
        "2.  $\\mathrm{var}(\\epsilon_i\\mid \\mathbb{X}=\\mathbf{x}_i)=\\sigma^2$ for $i=1,2,\\ldots,n$.\n",
        "3.  $\\mathrm{cov}(\\epsilon_i,\\epsilon_j\\mid \\mathbf{X})=0$ for $i\\neq j$ with $i,j=1,2,\\ldots,n$.\n",
        "4.  $\\epsilon_i$ has a normal distribution for $i=1,2,\\ldots,n$.\n",
        "\n",
        "**Summary of results**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "Combining these results with our linear model, we have:\n",
        "\n",
        "1.  $\\mathbf{y}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_{n\\times n})$.\n",
        "2.  $\\hat{\\boldsymbol{\\beta}}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1})$.\n",
        "3.  $\\hat{\\boldsymbol{\\epsilon}}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{0}_{n\\times 1}, \\sigma^2 (\\mathbf{I}_{n\\times n} - \\mathbf{H}))$, where $\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$.\n",
        "4.  $\\hat{\\boldsymbol{\\beta}}$ has the minimum variance among all unbiased estimators of $\\boldsymbol{\\beta}$ with the additional assumptions that the model is correct and $\\mathbf{X}$ is full-rank.\n",
        "\n",
        "We prove these results in the sections below. To simplify the derivations below, we let $\\mathbf{I}=\\mathbf{I}_{n\\times n}$.\n",
        "\n",
        "**Results for $\\mathbf{y}$**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "For our given linear model and under the assumptions summarized previously, our response variable has mean\n",
        "\n",
        "$$\n",
        "E(\\mathbf{y}\\mid \\mathbf{X})=\\mathbf{X}\\boldsymbol{\\beta}.\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blank}$\n",
        "\n",
        "For the variance of the response:\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\mathbf{y}\\mid \\mathbf{X})=\\sigma^2 \\mathbf{I}.\n",
        "$$ *Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blank}$\n",
        "\n",
        "The response variable has the following distribution:\n",
        "\n",
        "$$\n",
        "\\mathbf{y}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}).\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blank}$\n",
        "\n",
        "**Results for $\\hat{\\boldsymbol{\\beta}}$**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The OLS estimator for $\\boldsymbol{\\beta}$ is\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^T\\mathbf{X}^T\\mathbf{y}.\n",
        "$$\n",
        "\n",
        "This is an unbiased estimator for $\\boldsymbol{\\beta}$, i.e.,\n",
        "\n",
        "$$\n",
        "E(\\hat{\\boldsymbol{\\beta}}\\mid \\mathbf{X})=\\boldsymbol{\\beta}.\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blah}$\n",
        "\n",
        "The OLS estimator $\\hat{\\boldsymbol{\\beta}}$ has variance\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\hat{\\boldsymbol{\\beta}}\\mid \\mathbf{X})=\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}.\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blah}$\n",
        "\n",
        "The OLS estimator $\\hat{\\boldsymbol{\\beta}}$ has the following distribution:\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\boldsymbol{\\beta}, \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}).\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "**Results for the residuals**\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "The residual vector can be expressed in various equivalent ways, such as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\boldsymbol{\\epsilon}} &= \\mathbf{y}-\\hat{\\mathbf{y}} \\\\\n",
        "&= \\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The **hat** matrix is denoted as:\n",
        "\n",
        "$$\n",
        "\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T.\n",
        "$$\n",
        "\n",
        "Thus, using the substitution $\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$ and the definition for $\\mathbf{H}$, we see that:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\boldsymbol{\\epsilon}} &= \\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\ \n",
        "&= \\mathbf{y} - \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
        "&= \\mathbf{y} - \\mathbf{H}\\mathbf{y} \\\\\n",
        "&= (\\mathbf{I}-\\mathbf{H})\\mathbf{y}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The hat matrix is an important theoretical matrix, as it projects $\\mathbf{y}$ into the space spanned by the vectors in $\\mathbf{X}$.\n",
        "\n",
        "The hat matrix $\\mathbf{H}$ is symmetric and idempotent.\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The matrix $\\mathbf{I} - \\mathbf{H}$ is symmetric and idempotent.\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "Under the assumptions we discussed previously, the residuals have mean\n",
        "\n",
        "$$\n",
        "E(\\hat{\\boldsymbol{\\epsilon}}\\mid \\mathbf{X})=\\mathbf{0}_{n\\times 1}.\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blank}$\n",
        "\n",
        "The residuals have variance\n",
        "\n",
        "$$\n",
        "\\mathrm{var}(\\hat{\\boldsymbol{\\epsilon}}\\mid \\mathbf{X})=\\sigma^2 (\\mathbf{I} - \\mathbf{H}).\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "The residuals have the following distribution:\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\epsilon}}\\mid \\mathbf{X}\\sim \\mathsf{N}(\\mathbf{0}_{n\\times 1}, \\sigma^2 (\\mathbf{I} - \\mathbf{H})).\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "$\\vphantom{blank}$\n",
        "\n",
        "The RSS can be represented as\n",
        "\n",
        "$$\n",
        "RSS=\\mathbf{y}^T(\\mathbf{I}-\\mathbf{H})\\mathbf{y}.\n",
        "$$\n",
        "\n",
        "*Proof:*\n",
        "\n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>  \n",
        "</br>\n",
        "\n",
        "# The Gauss-Markov Theorem\n",
        "\n",
        "Suppose we will fit the regression model:\n",
        "\n",
        "$$\n",
        "\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n",
        "$$\n",
        "\n",
        "Assume that\n",
        "\n",
        "1.  $E(\\boldsymbol{\\epsilon}\\mid \\mathbf{X}) = 0$.\n",
        "2.  $\\mathrm{var}(\\boldsymbol{\\epsilon}\\mid \\mathbf{X}) = \\sigma^2 \\mathbf{I}$, i.e., the errors have constant variance and are uncorrelated.\n",
        "3.  $E(\\mathbf{y}\\mid \\mathbf{X})=\\mathbf{X}\\boldsymbol{\\beta}$\n",
        "4.  $\\mathbf{X}$ is a full-rank matrix.\n",
        "\n",
        "Then the **Gauss-Markov** states that the OLS estimator of $\\boldsymbol{\\beta}$,\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^T\\mathbf{X}^T\\mathbf{y},\n",
        "$$\n",
        "\n",
        "has the minimum variance among all unbiased estimators of $\\boldsymbol{\\beta}$ and this estimator is unique.\n",
        "\n",
        "Some comments:\n",
        "\n",
        "-   Assumption 3 guarantees that we have hypothesized the correct model, i.e., that we have included exactly the correct regressors in our model. Not only are we fitting a linear model to the data, but our hypothesized model is actually correct.\n",
        "-   Assumption 4 ensures that the OLS estimator can be computed (otherwise, there is no unique solution).\n",
        "-   The Gauss-Markov theorem only applies to unbiased estimators of $\\boldsymbol{\\beta}$. Biased estimators could have a smaller variance.\n",
        "-   The Gauss-Markov theorem states that no unbiased estimator of $\\boldsymbol{\\beta}$ can have a smaller variance than $\\hat{\\boldsymbol{\\beta}}$.\n",
        "-   The OLS estimator uniquely has the minimum variance property, meaning that if an $\\tilde{\\boldsymbol{\\beta}}$ is another unbiased estimator of $\\boldsymbol{\\beta}$ and $\\mathrm{var}(\\tilde{\\boldsymbol{\\beta}}) = \\mathrm{var}(\\hat{\\boldsymbol{\\beta}})$, then in fact the two estimators are identical and $\\tilde{\\boldsymbol{\\beta}}=\\hat{\\boldsymbol{\\beta}}$.\n",
        "\n",
        "We do not prove this theorem."
      ],
      "id": "2d23a289-48f5-42b9-be39-b600bb4886f9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "display_name": "R",
      "language": "R"
    },
    "language_info": {
      "name": "R",
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "pygments_lexer": "r",
      "version": "4.2.2"
    }
  }
}